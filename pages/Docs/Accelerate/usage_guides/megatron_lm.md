Ù„Ù… ÙŠØªÙ… ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„ØªÙŠ Ø·Ù„Ø¨Øª Ø¹Ø¯Ù… ØªØ±Ø¬Ù…ØªÙ‡Ø§ØŒ Ù…Ø«Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ±Ù…ÙˆØ² HTML ÙˆCSS ÙˆØ§Ù„Ø´ÙØ±Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©.

# Megatron-LM

ÙŠØªÙŠØ­ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø© Ø¹Ù„Ù‰ Ù†Ø·Ø§Ù‚ ÙˆØ§Ø³Ø¹. ÙŠÙˆÙØ± Ù…ÙˆØ§Ø²Ø§Ø© ÙØ¹Ø§Ù„Ø© Ù„Ù„Ù†Ø³ÙŠØ¬ ÙˆØ®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ ÙˆØ§Ù„ØªØ³Ù„Ø³Ù„ Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø­ÙˆÙ„ Ù…Ø«Ù„ [GPT](https://arxiv.org/abs/2005.14165) (ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙ‚Ø·) Ùˆ [BERT](https://arxiv.org/pdf/1810.04805.pdf) (Ø§Ù„ØªØ±Ù…ÙŠØ² ÙÙ‚Ø·) Ùˆ [T5](https://arxiv.org/abs/1910.10683) (Ø§Ù„ØªØ±Ù…ÙŠØ² ÙˆÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±). Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© ÙˆÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø®Ù„Ù Ø§Ù„ÙƒÙˆØ§Ù„ÙŠØ³ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ [repo](https://github.com/NVIDIA/Megatron-LM) Ø¹Ù„Ù‰ GitHub.

## Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø¯Ù…Ø¬ØŸ

ÙŠØ¯Ù…Ø¬ Accelerate Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Megatron-LM Ù„ØªÙ…ÙƒÙŠÙ† Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨/Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¹Ù„Ù‰ Ù†Ø·Ø§Ù‚ ÙˆØ§Ø³Ø¹ Ù„Ù€ BERT (Encoder) Ø£Ùˆ GPT (Decoder) Ø£Ùˆ Ù†Ù…Ø§Ø°Ø¬ T5 (Encoder and Decoder):

Ø£. **Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ù†Ø³ÙŠØ¬ (TP)**: ØªÙ‚Ù„Ù„ Ù…Ù† Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¯ÙˆÙ† Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ Ø¹Ù„Ù‰ Ø±ØªØ¨ Ø§Ù„Ø¹Ù‚Ø¯ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù‚Ø¯Ø©. ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ… ÙƒÙ„ ØªÙ†Ø³ÙŠÙ‚ Ø¥Ù„Ù‰ Ø¹Ø¯Ø© Ù‚Ø·Ø¹ØŒ Ù…Ø¹ ÙˆØ¬ÙˆØ¯ ÙƒÙ„ Ø´Ø±ÙŠØ­Ø© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© GPU Ù…Ù†ÙØµÙ„Ø©. ÙÙŠ ÙƒÙ„ Ø®Ø·ÙˆØ©ØŒ ØªØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ÙØ³ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„ ÙˆØ¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù…Ù† Ù‚Ø¨Ù„ ÙƒÙ„ Ø´Ø±ÙŠØ­Ø©ØŒ ÙŠÙ„ÙŠÙ‡Ø§ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ ÙˆØ­Ø¯Ø§Øª GPU (`Ø¹Ù…Ù„ÙŠØ© all-reduce`). ÙÙŠ Ø·Ø¨Ù‚Ø© Ù…Ø­ÙˆÙ„ Ø¨Ø³ÙŠØ·Ø©ØŒ ÙŠØ¤Ø¯ÙŠ Ù‡Ø°Ø§ Ø¥Ù„Ù‰ `all-reduces` 2 ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ùˆ 2 ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«ÙŠØ© [Megatron-LM: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù„ØºØ© Ù…Ø¹Ù„Ù…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ù„ÙŠØ§Ø±Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬](https://arxiv.org/pdf/1909.08053.pdf) ÙˆÙ‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ù…Ù† Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© [Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ±Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism).

Ø¨. **Ù…ÙˆØ§Ø²Ø§Ø© Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ (PP)**: ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø³Ø¹ Ø§Ù„Ù†Ø·Ø§Ù‚ Ù…Ù† Ø®Ù„Ø§Ù„ Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù‚Ø¯Ø©. ØªÙ‚Ù„Ù„ ÙÙ‚Ø§Ø¹Ø© PP Ø§Ù„Ø³Ø§Ø°Ø¬Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø¬Ø¯ÙˆÙ„ PipeDream-Flush schedule/1F1B ÙˆØ¬Ø¯ÙˆÙ„ 1F1B Ù…ØªØ¯Ø§Ø®Ù„. ÙŠØªÙ… ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø¨Ø§Ù„ØªØ³Ø§ÙˆÙŠ Ø¹Ø¨Ø± Ù…Ø±Ø§Ø­Ù„ PP. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ "24" Ø·Ø¨Ù‚Ø© ÙˆÙ„Ø¯ÙŠÙ†Ø§ "4" ÙˆØ­Ø¯Ø§Øª GPU Ù„Ù…ÙˆØ§Ø²Ø§Ø© Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ØŒ ÙØ³ØªØ­ØªÙˆÙŠ ÙƒÙ„ ÙˆØ­Ø¯Ø© GPU Ø¹Ù„Ù‰ "6" Ø·Ø¨Ù‚Ø§Øª (24/4). Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ø®Ù…ÙˆÙ„ Ù„Ù€ PPØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«ÙŠØ© [ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù„ØºØ© ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ù†Ø·Ø§Ù‚ Ø¨ÙƒÙØ§Ø¡Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª GPU Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf) ÙˆÙ‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ù…Ù† Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© [Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ±Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism).

Ø¬. **ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù…ÙˆØ§Ø²Ø§Ø© (SP)**: ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¯ÙˆÙ† Ø£ÙŠ Ø§ØªØµØ§Ù„ Ø¥Ø¶Ø§ÙÙŠ. Ù„Ø§ ÙŠÙ†Ø·Ø¨Ù‚ Ø¥Ù„Ø§ Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… TP. Ø¥Ù†Ù‡ ÙŠÙ‚Ù„Ù„ Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ø£Ù†Ù‡ ÙŠÙ…Ù†Ø¹ ÙˆØ¬ÙˆØ¯ Ù†ÙØ³ Ø§Ù„Ù†Ø³Ø® Ø¹Ù„Ù‰ Ø±ØªØ¨ Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ù†Ø³ÙŠØ¬ Ø¨Ø¹Ø¯ `all-reduce` Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ø¹Ù…Ù„ÙŠØ© `reduce-scatter` ÙˆØ³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¹Ù…Ù„ÙŠØ© `no-op` Ø¨Ø¹Ù…Ù„ÙŠØ© `all-gather`. Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† `all-reduce = reduce-scatter + all-gather`ØŒ ÙØ¥Ù† Ù‡Ø°Ø§ ÙŠÙˆÙØ± Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ· Ø¯ÙˆÙ† Ø£ÙŠ ØªÙƒÙ„ÙØ© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø§ØªØµØ§Ù„. Ø¨Ø¨Ø³Ø§Ø·Ø©ØŒ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ´Ø·ÙŠØ± Ù…Ø®Ø±Ø¬Ø§Øª ÙƒÙ„ Ø·Ø¨Ù‚Ø© Ù…Ø­ÙˆÙ„ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø¨Ø¹Ø¯ Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ù‡Ùˆ "1024" ÙˆÙƒØ§Ù† Ø­Ø¬Ù… TP Ù‡Ùˆ "4"ØŒ ÙØ³ØªØ­ØªÙˆÙŠ ÙƒÙ„ ÙˆØ­Ø¯Ø© GPU Ø¹Ù„Ù‰ "256" Ø±Ù…Ø²Ù‹Ø§ (1024/4) Ù„ÙƒÙ„ Ø¹ÙŠÙ†Ø©. ÙˆÙ‡Ø°Ø§ ÙŠØ²ÙŠØ¯ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø¯Ø¹Ù…Ù‡Ø§ Ù„Ù„ØªØ¯Ø±ÙŠØ¨. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«ÙŠØ© [ØªÙ‚Ù„ÙŠÙ„ Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†Ø´ÙŠØ· ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©](https://arxiv.org/pdf/2205.05198.pdf).

Ø¯. **Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (DP)** Ø¹Ø¨Ø± Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ù…ÙÙ†ÙØ³ÙÙ‘Ù‚: ØªÙ‚Ù„Ù„ Ù…Ù† Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¬Ø²Ø¦Ø© Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙØ­ÙØ³ÙÙ‘Ù† ÙˆØ§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø¹Ø¨Ø± Ø±ØªØ¨ DP (Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ù„ØªÙƒØ±Ø§Ø± Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙØ­ÙØ³ÙÙ‘Ù† Ø¹Ø¨Ø± Ø±ØªØ¨ Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª). Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø³Ù† Adam Ù…Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ØŒ ÙŠØ­ØªÙˆÙŠ ÙƒÙ„ Ù…Ø¹Ù„Ù…Ø© Ø¹Ù„Ù‰ 12 Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©. ÙŠØªÙ… ØªÙˆØ²ÙŠØ¹ Ù‡Ø°Ø§ Ø¨Ø§Ù„ØªØ³Ø§ÙˆÙŠ Ø¹Ø¨Ø± ÙˆØ­Ø¯Ø§Øª GPUØŒ Ø£ÙŠ Ø£Ù† ÙƒÙ„ Ù…Ø¹Ù„Ù…Ø© Ø³ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 3 Ø¨Ø§ÙŠØª (12/4) Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙ†Ø§ 4 ÙˆØ­Ø¯Ø§Øª GPU. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«ÙŠØ© [ZeRO: ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù†Ø­Ùˆ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ±ÙŠÙ„ÙŠÙˆÙ†ÙŠØ©](https://arxiv.org/pdf/1910.02054.pdf) ÙˆØ§Ù„Ù‚Ø³Ù… Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† ğŸ¤— blog [Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ±Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed#zero-data-parallelism).

Ù‡Ù€. **Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†Ø´ÙŠØ· Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠ**: ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„ØªÙ†Ø´ÙŠØ· Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ³Ø¬ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ†Ø´ÙŠØ· Ø§Ù„Ø°ÙƒÙŠØ©. Ù„Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙ†Ø´ÙŠØ·Ø§Øª Ø§Ù„ØªÙŠ ØªØ´ØºÙ„ Ø°Ø§ÙƒØ±Ø© ÙƒØ¨ÙŠØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¨Ø³Ø±Ø¹Ø©ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ ØªØ­Ù‚ÙŠÙ‚ ØªÙˆØ§Ø²Ù† Ø±Ø§Ø¦Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø­Ø³Ø§Ø¨. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€ GPT-3ØŒ ÙŠØ¤Ø¯ÙŠ Ù‡Ø°Ø§ Ø¥Ù„Ù‰ ØªÙ‚Ù„ÙŠÙ„ Ø¨Ù†Ø³Ø¨Ø© 70% ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„ØªÙ†Ø´ÙŠØ· Ø¹Ù„Ù‰ Ø­Ø³Ø§Ø¨ 2.7% ÙÙ‚Ø· Ù…Ù† Ù†ÙÙ‚Ø§Øª FLOPs Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†Ø´ÙŠØ·Ø§Øª. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«ÙŠØ© [ØªÙ‚Ù„ÙŠÙ„ Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†Ø´ÙŠØ· ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©](https://arxiv.org/pdf/2205.05198.pdf).

Ùˆ. **Ø§Ù„Ù†ÙˆÙ‰ Ø§Ù„Ù…Ù†Ø¯Ù…Ø¬Ø©**: Softmax Ø§Ù„Ù…Ù†Ø¯Ù…Ø¬ØŒ ÙˆØ¯Ù‚Ø© Ù…Ø®ØªÙ„Ø·Ø© Ù…Ù†Ø¯Ù…Ø¬Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ØŒ ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ù†Ø¯Ù…Ø¬Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„ÙˆØ²Ù†ÙŠ Ù„Ø·Ø¨Ù‚Ø© Ø®Ø·ÙŠØ©. PyTorch JIT Ù‚Ø§Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ GeLU Ø§Ù„Ù…Ù†Ø¯Ù…Ø¬ ÙˆØ§Ù„Ø§Ù†Ø­ÙŠØ§Ø² Ø§Ù„Ù…Ù†Ø¯Ù…Ø¬ + Ø¥Ø³Ù‚Ø§Ø· + Ø¥Ø¶Ø§ÙØ© Ø¨Ù‚Ø§ÙŠØ§.

Ø². **Ø¯Ø¹Ù… Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ‡Ø±Ø³Ø©**: ØªÙ†Ø³ÙŠÙ‚ Ø«Ù†Ø§Ø¦ÙŠ ÙØ¹Ø§Ù„ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø³Ø¹ Ø§Ù„Ù†Ø·Ø§Ù‚. Ø¯Ø¹Ù… Ù„Ù€ `mmap`ØŒ ÙˆÙ…Ù„Ù ÙÙ‡Ø±Ø³ `cached`ØŒ ÙˆØªÙ†Ø³ÙŠÙ‚ Ù…Ø­Ù…Ù„ `lazy`.

Ø­. **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø´ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ ÙˆÙ‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙŠÙ†ÙŠ**: Ø£Ø¯Ø§Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ù†Ù‚Ø§Ø· ØªÙØªÙŠØ´ Megatron-LM Ø°Ø§Øª Ø£Ø­Ø¬Ø§Ù… Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…ØªØºÙŠØ±Ø© Ù„Ù„Ù†Ø³ÙŠØ¬ ÙˆØ®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· ØªÙØªÙŠØ´ Ù…Ø¬Ø²Ø£Ø© ğŸ¤— Transformers Ø§Ù„Ù…Ø­Ø¨ÙˆØ¨Ø© Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø¯Ø¹Ù…Ù‡Ø§ Ø§Ù„Ø±Ø§Ø¦Ø¹ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø¯ÙˆØ§Øª ÙˆÙÙŠØ±Ø© Ù…Ø«Ù„ ğŸ¤— Accelerate Big Model Inference Ùˆ Megatron-DeepSpeed InferenceØŒ Ø¥Ù„Ø®. ÙŠØªÙˆÙØ± Ø§Ù„Ø¯Ø¹Ù… Ø£ÙŠØ¶Ù‹Ø§ Ù„ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø§Ø· ØªÙØªÙŠØ´ ğŸ¤— Transformers Ø§Ù„Ù…Ø¬Ø²Ø£Ø© Ø¥Ù„Ù‰ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Megatron-LM Ø°Ø§Øª Ø£Ø­Ø¬Ø§Ù… Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…ØªØºÙŠØ±Ø© Ù„Ù„Ù†Ø³ÙŠØ¬ ÙˆØ®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø³Ø¹ Ø§Ù„Ù†Ø·Ø§Ù‚.

## Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ«Ø¨ÙŠØª Ø£Ø­Ø¯Ø« Ø¥ØµØ¯Ø§Ø±Ø§Øª PyTorch Ùˆcuda Ùˆnccl Ùˆ[APEX](https://github.com/NVIDIA/apex#quick-start) Ù…Ù† NVIDIA ÙˆÙ…ÙƒØªØ¨Ø© nltk. Ø±Ø§Ø¬Ø¹ [Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚](https://github.com/NVIDIA/Megatron-LM#setup) Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„.

Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© Ù‡ÙŠ Ø³Ø­Ø¨ Ø­Ø§ÙˆÙŠØ© PyTorch Ù…Ù† NVIDIA ØªØ£ØªÙŠ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ«Ø¨ÙŠØªØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ù† NGC.

ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© conda:

1. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©:

```
conda create --name ml
```

2. Ø§ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ø¢Ù„Ø© Ø¨Ù‡Ø§ CUDA 11.3 Ù…Ø«Ø¨ØªØŒ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª Ø¥ØµØ¯Ø§Ø± GPU Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ Ù„Ù€ PyTorch:

```
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
```

3. ØªØ«Ø¨ÙŠØª Nvidia APEX:

```
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
cd ..
```

4. ØªØ«Ø¨ÙŠØª Megatron-LM:

```
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM
git checkout core_r0.5.0
pip install --no-use-pep517 -e .
```
## ØªØ³Ø±ÙŠØ¹ Megatron-LM Plugin

ØªÙØ¯Ø¹Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ø¨Ø± Ø£Ù…Ø± `accelerate config`. ÙˆÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙŠØ²Ø§Øª Megatron-LM:

```bash
:~$ accelerate config --config_file "megatron_gpt_config.yaml"
ÙÙŠ Ø£ÙŠ Ø¨ÙŠØ¦Ø© Ø­ÙˆØ³Ø¨Ø© ØªØ¹Ù…Ù„ØŸ ([0] Ù‡Ø°Ù‡ Ø§Ù„Ø¢Ù„Ø©ØŒ [1] AWS (Amazon SageMaker)): 0
Ù…Ø§ Ù†ÙˆØ¹ Ø§Ù„Ø¢Ù„Ø© Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ØŸ ([0] Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªØ¯Ø±ÙŠØ¨ Ù…ÙˆØ²Ø¹ØŒ [1] Ù…ØªØ¹Ø¯Ø¯ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ©ØŒ [2] Ù…ØªØ¹Ø¯Ø¯ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§ØªØŒ [3] ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© ÙØ§Ø¦Ù‚Ø©): 2
ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¢Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø§Ù„ØªÙŠ Ø³ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ (Ø§Ø³ØªØ®Ø¯Ù… Ø£ÙƒØ«Ø± Ù…Ù† 1 Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø¯)ØŸ [1]:
Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… DeepSpeedØŸ [Ù†Ø¹Ù…/Ù„Ø§]:
Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… FullyShardedDataParallelØŸ [Ù†Ø¹Ù…/Ù„Ø§]:
Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Megatron-LMØŸ [Ù†Ø¹Ù…/Ù„Ø§]: Ù†Ø¹Ù…
Ù…Ø§ Ù‡ÙŠ Ø¯Ø±Ø¬Ø©/Ø­Ø¬Ù… Ø§Ù„ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ†Ø³ÙˆØ±ÙŠØ©ØŸ [1]:2
Ù‡Ù„ ØªØ±ÙŠØ¯ ØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØŸ [Ù†Ø¹Ù…/Ù„Ø§]:
Ù…Ø§ Ù‡ÙŠ Ø¯Ø±Ø¬Ø©/Ø­Ø¬Ù… Ø§Ù„ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ØŸ [1]:2
Ù…Ø§ Ù‡Ùˆ Ø¹Ø¯Ø¯ Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„ØµØºØ±Ù‰ØŸ [1]:2
Ù‡Ù„ ØªØ±ÙŠØ¯ ØªÙ…ÙƒÙŠÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†Ø´ÙŠØ· Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠØŸ [Ù†Ø¹Ù…/Ù„Ø§]:
Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù† Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø³Ù… Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù† ÙˆØ§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø¹Ø¨Ø± Ø±ØªØ¨ Ø§Ù„ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ [Ù†Ø¹Ù…/Ù„Ø§]:
Ù…Ø§ Ù‡ÙŠ Ù‚ÙŠÙ…Ø© ØªÙ‚Ù„ÙŠÙ… Ø§Ù„ØªØ¯Ø±Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹ÙŠØ§Ø± L2 Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ (0 Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ´ØºÙŠÙ„)ØŸ [1.0]:
ÙƒÙ… Ø¹Ø¯Ø¯ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª (GPU) Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ØŸ [1]:4
Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø§Ø¦Ù…Ø© FP16 Ø£Ùˆ BF16 (Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø©)ØŸ [Ù„Ø§/FP16/BF16]: BF16
```

ÙŠÙØ¸Ù‡Ø± Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ø§ØªØ¬ Ø£Ø¯Ù†Ø§Ù‡:

```
~$ cat megatron_gpt_config.yaml
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MEGATRON_LM
downcast_bf16: 'no'
fsdp_config: {}
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
megatron_lm_config:
megatron_lm_gradient_clipping: 1.0
megatron_lm_num_micro_batches: 2
megatron_lm_pp_degree: 2
megatronMultiplier_lm_recompute_activations: true
megatron_lm_sequence_parallelism: true
megatron_lm_tp_degree: 2
megatron_lm_use_distributed_optimizer: true
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
use_cpu: false
```

Ø³Ù†Ø£Ø®Ø° Ù…Ø«Ø§Ù„Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø³Ø¨Ù‚ Ù„Ù€ GPT. ÙˆÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ø¯Ù†ÙŠØ§ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙŠ `run_clm_no_trainer.py` Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Megatron-LM:

1. Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Megatron-LM ÙŠØ³ØªØ®Ø¯Ù… ØªÙ†ÙÙŠØ°Ù‡ Ø§Ù„Ø®Ø§Øµ Ù…Ù† Ø§Ù„Ù…Ø­Ø³Ù†ØŒ ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹Ù‡. ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ÙØ¥Ù† Ø§Ù„Ø¯Ø¹Ù… Ù…ØªØ§Ø­ ÙÙ‚Ø· Ù„Ø¬Ø¯ÙˆÙ„Ø© Megatron-LM. ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù†Ø´Ø§Ø¡ `accelerate.utils.MegatronLMDummyScheduler`. ÙˆÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø°Ù„Ùƒ:

```python
from accelerate.utils import MegatronLMDummyScheduler

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
lr_scheduler = MegatronLMDummyScheduler(
optimizer=optimizer,
total_num_steps=args.max_train_steps,
warmup_num_steps=args.num_warmup_steps,
)
else:
lr_scheduler = get_scheduler(
name=args.lr_scheduler_type,
optimizer=optimizer,
num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,
num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
)
```

2. ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø¢Ù† Ù…Ø¹Ø±ÙØ© Ø£Ø­Ø¬Ø§Ù… Ø§Ù„ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ†Ø³ÙˆØ±ÙŠØ© ÙˆØ§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨. ÙˆÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ¹Ø§Ù„Ø©:

```python
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size
else:
total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
```

3. Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Megatron-LMØŒ ÙŠØªÙ… Ø¨Ø§Ù„ÙØ¹Ù„ Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø³Ø§Ø¦Ø± Ø¹Ø¨Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

```python
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
losses.append(loss)
else:
losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
losses = torch.tensor(losses)
else:
losses = torch.cat(losses)
```

4. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ø¥Ù„Ù‰ Megatron-LMØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `accelerator.save_state`:

```python
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
accelerator.save_state(args.output_dir)
else:
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(
args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
)
```

Ù‡Ø°Ø§ ÙƒÙ„ Ø´ÙŠØ¡! Ù†Ø­Ù† Ù…Ø³ØªØ¹Ø¯ÙˆÙ† Ø§Ù„Ø¢Ù† Ù„Ù„Ø§Ù†Ø·Ù„Ø§Ù‚ ğŸš€. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ù„Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø± `accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py`.

Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ù†ÙØ° Ø°Ù„Ùƒ Ù„Ù†Ù…ÙˆØ°Ø¬ `gpt-large` Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… 4 ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³ÙˆÙ…ÙŠØ© A100-80GB.

```bash
accelerate launch --config_file megatron_gpt_config.yaml \
examples/by_feature/megatron_lm_gpt_pretraining.py \
--config_name "gpt2-large" \
--tokenizer_name "gpt2-large" \
--dataset_name wikitext \
--dataset_config_name wikitext-2-raw-v1 \
--block_size 1024 \
--learning_rate 5e-5 \
--per_device_train_batch_size 24 \
--per_device_eval_batch_size 24 \
--num_train_epochs 5 \
--with_tracking \
--report_to "wandb" \
--output_dir "awesome_model"
```

ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù† Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:

```bash
Loading extension module fused_dense_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 3.569 seconds
> padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
Building gpt model in the pre-training mode.
The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.
Preparing dataloader
Preparing dataloader
Preparing model
> number of parameters on (tensor, pipeline) model parallel rank (1, 0): 210753280
> number of parameters on (tensor, pipeline) model parallel rank (1, 1): 209445120
> number of parameters on (tensor, pipeline) model parallel rank (0, 0): 210753280
> number of parameters on (tensor, pipeline) model parallel rank (0, 1): 209445120
Preparing optimizer
Preparing scheduler
> learning rate decay style: linear
10/10/2022 22:57:22 - INFO - __main__ - ***** Running training *****
10/10/2022 22:57:22 - INFO - __main__ -   Num examples = 2318
10/10/2022 22:57:22 - INFO - __main__ -   Num Epochs = 5
10/10/2022 22:57:22 - INFO - __main__ -   Instantaneous batch size per device = 24
10/10/2022 22:57:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 48
10/10/2022 22:57:22 - INFO - __main__ -   Gradient Accumulation steps = 1
10/10/2022 22:57:22 - INFO - __main__ -   Total optimization steps = 245
20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                 | 49/245 [01:04<04:09,  1.27s/it]
10/10/2022 22:58:29 - INFO - __main__ - epoch 0: perplexity: 1222.1594275215962 eval_loss: 7.10837459564209
40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 98/245 [02:10<03:07,  1.28s/it]
10/10/2022 22:59:35 - INFO - __main__ - epoch 1: perplexity: 894.5236583794557 eval_loss: 6.796291351318359
60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 147/245 [03:16<02:05,  1.28s/it]
10/10/2022 23:00:40 - INFO - __main__ - epoch 2: perplexity: 702.8458788508042 eval_loss: 6.555137634277344
80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 196/245 [04:22<01:02,  1.28s/it]
10/10/2022 23:01:46 - INFO - __main__ - epoch 3: perplexity: 600.3220028695281 eval_loss: 6.39746618270874
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245/245 [05:27<00:00,  1.28s/it]
```

Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª/Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØªØ¹ÙŠÙŠÙ†Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `accelerate.utils.MegatronLMPlugin`.

## Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ÙƒØªØ§Ø¨Ø© Ø®Ø·ÙˆØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø®ØµØµØ© ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Megatron-LM Ø§Ù„Ù…ÙÙ‡Ø±Ø³Ø©

Ù„Ø§Ø³ØªØºÙ„Ø§Ù„ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø£Ø¯Ù†Ø§Ù‡.

1. ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ®ØµÙŠØµ Ø®Ø·ÙˆØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Megatron-LM. Ø³ØªÙ‚ÙˆÙ… Ø¨ØªÙ†ÙÙŠØ° `accelerate.utils.AbstractTrainStep` Ø£Ùˆ ÙˆØ±Ø§Ø«Ø© Ø£Ø­Ø¯ Ø£Ø·ÙØ§Ù„Ù‡Ù…Ø§ `accelerate.utils.GPTTrainStep`ØŒ `accelerate.utils.BertTrainStep` Ø£Ùˆ `accelerate.utils.T5TrainStep`.

```python
from accelerate.utils import MegatronLMDummyScheduler, GPTTrainStep, avg_losses_across_data_parallel_group

# Custom loss function for the Megatron model
class GPTTrainStepWithCustomLoss(GPTTrainStep):
def __init__(self, megatron_args, **kwargs):
super().__init__(megatron_args)
self.kwargs = kwargs

def get_loss_func(self):
def loss_func(inputs, loss_mask, output_tensor):
batch_size, seq_length = output_tensor.shape
losses = output_tensor.float()
loss_mask = loss_mask.view(-1).float()
loss = losses.view(-1) * loss_mask

# Resize and average loss per sample
loss_per_sample = loss.view(batch_size, seq_length).sum(axis=1)
loss_mask_per_sample = loss_mask.view(batch_size, seq_length).sum(axis=1)
loss_per_sample = loss_per_sample / loss_mask_per_sample

# Calculate and scale weighting
weights = torch.stack([(inputs == kt).float() for kt in self.kwargs["keytoken_ids"]]).sum(axis=[0, 2])
weights = 1.0 + self.kwargs["alpha"] * weights
# Calculate weighted average
weighted_loss = (loss_per_sample * weights).mean()

# Reduce loss across data parallel groups
averaged_loss = avg_losses_across_data_parallel_group([weighted_loss])

return weighted_loss, {"lm loss": averaged_loss[0]}

return loss_func

def get_forward_step_func(self):
def forward_step(data_iterator, model):
"""Forward step."""
# Get the batch.
tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)
output_tensor = model(tokens, position_ids, attention_mask, labels=labels)

return output_tensor, partial(self.loss_func, tokens, loss_mask)

return forward_step


def main():
# Custom loss function for the Megatron model
keytoken_ids = []
keywords = ["plt", "pd", "sk", "fit", "predict", " plt", " pd", " sk", " fit", " predict"]
for keyword in keywords:
ids = tokenizer([keyword]).input_ids[0]
if len(ids) == 1:
keytoken_ids.append(ids[0])
accelerator.print(f"Keytoken ids: {keytoken_ids}")
accelerator.state.megatron_lm_plugin.custom_train_step_class = GPTTrainStepWithCustomLoss
accelerator.state.megatron_lm_plugin.custom_train_step_kwargs = {
"keytoken_ids": keytoken_ids,
"alpha": 0.25,
}
```

2. Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Megatron-LMØŒ Ù‡Ù†Ø§Ùƒ Ø¨Ø¹Ø¶ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©. Ù…ØªØ§Ø­ Ø¨Ø±Ø§Ù…Ø¬ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø±ØªØ¨Ø© 0 Ù…Ù† ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© ØªÙˆØ§Ø²ÙŠ ØªÙ†Ø³ÙˆØ±ÙŠØ©. ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ Ù‡Ù†Ø§Ùƒ Ø±ØªØ¨ Ø­ÙŠØ« Ù„Ù† ÙŠÙƒÙˆÙ† Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ù‹Ø§ØŒ ÙˆÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ Ø¥Ø¬Ø±Ø§Ø¡ ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø¹Ù„Ù‰ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ø¥Ù† Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ ØªÙØ¸Ù‡Ø± Ù…Ø¯Ù‰ Ù…Ø±ÙˆÙ†Ø© ÙˆÙ‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù…ØªØ¯Ø§Ø¯ Ù…ÙƒØªØ¨Ø© ğŸ¤— Accelerate. ÙˆÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:

   - Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Megatron-LM Ø§Ù„Ù…ÙÙ‡Ø±Ø³Ø©ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… `MegatronLMDummyDataLoader` ÙˆØªÙ…Ø±ÙŠØ± ÙˆØ³Ø§Ø¦Ø· Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¥Ù„ÙŠÙ‡ Ù…Ø«Ù„ `data_path`ØŒ `seq_length`ØŒ ÙˆÙ…Ø§ Ø¥Ù„Ù‰ Ø°Ù„Ùƒ. Ø§Ù†Ø¸Ø± [Ù‡Ù†Ø§](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py#L804) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ§Ø­Ø©.

   ```python
   from accelerate.utils import MegatronLMDummyDataLoader

   megatron_dataloader_config = {
   "data_path": args.data_path,
   "splits_string": args.splits_string,
   "seq_length": args.block_size,
   "micro_batch_size": args.per_device_train_batch_size,
   }
   megatron_dataloader = MegatronLMDummyDataLoader(**megatron_dataloader_config)
   accelerator.state.megatron_lm_plugin.megatron_dataset_flag = True
   ```

   - ÙŠØªÙ… ØªÙƒØ±Ø§Ø± `megatron_dataloader` 3 Ù…Ø±Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨Ø±Ø§Ù…Ø¬ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆÙÙ‚Ù‹Ø§ Ù„Ù†Ø³Ø¨ `args.splits_string`

   ```python
   model, optimizer, lr_scheduler, train_dataloader, eval_dataloader, _ = accelerator.prepare(
   model, optimizer, lr
## Ø£Ø¯Ø§Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ ÙˆØ§Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠ 
1. ØªØªÙˆÙØ± Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© ÙÙŠ Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers ØªØ­Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©.
ÙˆÙ‡ÙŠ Ù…ØªÙˆÙØ±Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ Ù„Ù†Ù…ÙˆØ°Ø¬ GPT [checkpoint_reshaping_and_interoperability.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py) 
2. ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù…Ù† Megatron-LM Ø¥Ù„Ù‰ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù…Ø¬Ø²Ø£Ø© Ø¹Ø§Ù„Ù…ÙŠØ© ÙÙŠ ğŸ¤— Transformers.
```bash
python checkpoint_reshaping_and_interoperability.py \
--convert_checkpoint_from_megatron_to_transformers \
--load_path "gpt/iter_0005000" \
--save_path "gpt/trfs_checkpoint" \
--max_shard_size "200MB" \
--tokenizer_name "gpt2" \
--print-checkpoint-structure
``` 
3. ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ù…Ù† Transformers Ø¥Ù„Ù‰ Megatron Ù…Ø¹ `tp_size=2`ØŒ `pp_size=2` Ùˆ `dp_size=2`.
```bash
python checkpoint_utils/megatgron_gpt2/checkpoint_reshaping_and_interoperability.py \
--load_path "gpt/trfs_checkpoint" \
--save_path "gpt/megatron_lm_checkpoint" \
--target_tensor_model_parallel_size 2 \
--target_pipeline_model_parallel_size 2 \
--target_data_parallel_size 2 \
--target_params_dtype "bf16" \
--make_vocab_size_divisible_by 128 \
--use_distributed_optimizer \
--print-checkpoint-structure
```

## Ø¯Ø¹Ù… Ù†Ù…Ø§Ø°Ø¬ Megatron-LM GPT Ù„Ø¥Ø±Ø¬Ø§Ø¹ logits ÙˆØ¯Ø§Ù„Ø© `megatron_generate` Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ 
1. ÙŠØªØ·Ù„Ø¨ Ø¥Ø±Ø¬Ø§Ø¹ logits ØªØ¹ÙŠÙŠÙ† `require_logits=True` ÙÙŠ MegatronLMPlugin ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡.
Ø³ØªÙƒÙˆÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ… Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨.
```python
megatron_lm_plugin = MegatronLMPlugin(return_logits=True)
``` 
2. Ø·Ø±ÙŠÙ‚Ø© `megatron_generate` Ù„Ù†Ù…ÙˆØ°Ø¬ Megatron-LM GPT: Ø³ØªØ³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„ØªÙˆØªØ±ÙŠ ÙˆØ£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¬Ø´Ø¹ Ù…Ø¹/Ø¨Ø¯ÙˆÙ† Ø¹ÙŠÙ†Ø§Øª top_k/top_p ÙˆÙ„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ù„ Ø§Ù„ÙØ±Ø¯ÙŠØ© Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¹Ø§Ø¹ÙŠ.
ÙŠØªÙ… Ø¯Ø¹Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© ÙÙ‚Ø· Ù…Ù† Ù…ÙŠØ²Ø§Øª generate transformers. Ø³ÙŠØ³Ø§Ø¹Ø¯ Ù‡Ø°Ø§ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ø¨Ø± Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„ØªÙˆØªØ±ÙŠ ÙˆØ£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„ØªÙˆÙ„ÙŠØ¯ (ÙŠØªÙ… Ø¨Ø§Ù„ÙØ¹Ù„ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ÙˆØ§Ø© Ø§Ù„Ù…Ù†Ø¯Ù…Ø¬Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ).
ÙŠØªØ·Ù„Ø¨ Ù‡Ø°Ø§ Ø£Ù† ÙŠÙƒÙˆÙ† Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø²ÙŠØ© 1ØŒ ÙˆØ£Ù† ÙŠØªÙ… ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠ ÙˆÙ†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ø§Ù„ØªÙ†Ø´ÙŠØ·.
ÙƒÙ…Ø§ ÙŠØªØ·Ù„Ø¨ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆÙ…Ù„Ù Ø§Ù„Ø§Ù†Ø¯Ù…Ø§Ø¬ ÙÙŠ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ.
ÙŠÙˆØ¶Ø­ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙƒÙŠÙÙŠØ© ØªÙƒÙˆÙŠÙ† ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© `megatron_generate` Ù„Ù†Ù…ÙˆØ°Ø¬ Megatron-LM GPT.
```python
# ØªØ­Ø¯ÙŠØ¯ Ù…Ù„Ù Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆÙ…Ù„Ù Ø§Ù„Ø§Ù†Ø¯Ù…Ø§Ø¬ ÙÙŠ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ
vocab_file = os.path.join(args.resume_from_checkpoint, "vocab.json")
merge_file = os.path.join(args.resume_from_checkpoint, "merges.txt")
other_megatron_args = {"vocab_file": vocab_file, "merge_file": merge_file}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)

# Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `megatron_generate`
tokenizer.pad_token = tokenizer.eos_token
max_new_tokens = 64
batch_texts = [
"Are you human?",
"The purpose of life is",
"The arsenal was constructed at the request of",
"How are you doing these days?",
]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)

# Ø¹ÙŠÙ†Ø© top-p
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"],
batch_encodings["attention_mask"],
max_new_tokens=max_new_tokens,
top_p=0.8,
top_p_decay=0.5,
temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# Ø¹ÙŠÙ†Ø© top-k
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"],
batch_encodings["attention_mask"],
max_new_tokens=max_new_tokens,
top_k=50,
temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² `bos` ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"], batch_encodings["attention_mask"], max_new_tokens=max_new_tokens, add_BOS=True
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# Ø¨Ø­Ø« Ø´Ø¹Ø§Ø¹ÙŠ => ÙŠØ£Ø®Ø° Ù…ÙˆØ¬Ù‡ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
batch_texts = ["The purpose of life is"]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"],
batch_encodings["attention_mask"],
max_new_tokens=max_new_tokens,
num_beams=20,
length_penalty=1.5,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)
``` 
3. ÙŠØªÙˆÙØ± Ù…Ø«Ø§Ù„ Ø´Ø§Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© `megatron_generate` Ù„Ù†Ù…ÙˆØ°Ø¬ Megatron-LM GPT ÙÙŠ
[megatron_gpt2_generation.py](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/inference/megatron_gpt2_generation.py) Ù…Ø¹
Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† [megatron_lm_gpt_generate_config.yaml](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/Configs/megatron_lm_gpt_generate_config.yaml).
ÙŠØªÙˆÙØ± Ù†Øµ Bash Ù…Ø¹ Ø£Ù…Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ ÙÙŠ [megatron_lm_gpt_generate.sh](https://github.com/pacman100/accelerate-megatron-test/blob/main/megatron_lm_gpt_generate.sh).
ØªØªÙˆÙØ± Ø³Ø¬Ù„Ø§Øª Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙÙŠ [megatron_lm_gpt_generate.log](https://github.com/pacman100/accelerate-megatron-test/blob/main/output_logs/megatron_lm_gpt_generate.log).

## Ø¯Ø¹Ù… ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…ÙˆØ¶Ø¹ ROPE ÙˆALiBi ÙˆMulti-Query Attention 
1. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø§Ù‡ØªÙ…Ø§Ù… ROPE/ALiBiØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± `position_embedding_type` Ù…Ø¹ `("absolute" | "rotary" | "alibi")` Ø¥Ù„Ù‰ `MegatronLMPlugin` ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡.
```python
other_megatron_args = {"position_embedding_type": "alibi"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
``` 
2. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø§Ù‡ØªÙ…Ø§Ù… Multi-QueryØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± `attention_head_type` Ù…Ø¹ `("multihead" | "multiquery")` Ø¥Ù„Ù‰ `MegatronLMPlugin` ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡.
```python
other_megatron_args = {"attention_head_type": "multiquery"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
```

## Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª 
1. ÙŠØ¯Ø¹Ù… Ù†Ù…Ø§Ø°Ø¬ Transformers GPT2 ÙˆMegatron-BERT ÙˆT5.
ÙŠØºØ·ÙŠ Ù‡Ø°Ø§ ÙØ¦Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙ‚Ø· ÙˆØ§Ù„ØªØ±Ù…ÙŠØ² ÙÙ‚Ø· ÙˆØ§Ù„ØªØ±Ù…ÙŠØ² ÙˆÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±. 
2. ÙŠØªÙ… Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙÙ‚Ø· Ù…Ù† ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù…Ø§Ù…
Ù†Ø¸Ø±Ù‹Ø§ Ù„ÙˆØ¬ÙˆØ¯ ØªÙØ§Ø¹Ù„ Ù…Ø¹Ù‚Ø¯ Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ ÙˆØ§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„ØªÙˆØªØ±ÙŠ ÙˆØ§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ù„Ù Ø§Ù„ÙƒÙˆØ§Ù„ÙŠØ³.
ØªÙØ±Ø¬Ø¹ Ù…ÙƒØ§Ù„Ù…Ø© `model(**batch_data)` Ø§Ù„Ø®Ø³Ø§Ø¦Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø·Ù‡Ø§ Ø¹Ø¨Ø± Ù…Ø±Ø§ØªØ¨ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
Ù‡Ø°Ø§ Ø¬ÙŠØ¯ Ù„Ù…Ø¹Ø¸Ù… Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… ÙÙŠÙ‡Ø§ ØªØ´ØºÙŠÙ„ ÙˆØ¸Ø§Ø¦Ù Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙŠØ²Ø§Øª Megatron-LM
ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø­Ø³Ø§Ø¨ "perplexity" Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø³Ø§Ø±Ø©.
Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ù…ÙˆØ°Ø¬ GPTØŒ ÙŠØªÙ… Ø¯Ø¹Ù… Ø¥Ø±Ø¬Ø§Ø¹ logits Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø³Ø§Ø¦Ø±.
Ù„Ø§ ÙŠØªÙ… Ø¬Ù…Ø¹ Ù‡Ø°Ù‡ logits Ø¹Ø¨Ø± Ù…Ø±Ø§ØªØ¨ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø§Ø³ØªØ®Ø¯Ù… `accelerator.utils.gather_across_data_parallel_groups`
Ù„Ø¬Ù…Ø¹ logits Ø¹Ø¨Ø± Ù…Ø±Ø§ØªØ¨ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØµØ­ÙŠØ­Ø© Ù…Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©. 
3. Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‡ÙŠ Ø§Ù„Ù…Ø±ØªØ¨Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø­ÙŠØ« ØªØªÙˆÙØ± Ø§Ù„Ø®Ø³Ø§Ø¦Ø±/logits ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨.
`accelerator.is_main_process` Ùˆ `accelerator.is_local_main_process` Ø¥Ø±Ø¬Ø§Ø¹ `True` Ù„Ù„Ù…Ø±ØªØ¨Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…
Ø¯Ù…Ø¬ Megatron-LM. 
4. ÙÙŠ Ù…ÙƒØ§Ù„Ù…Ø© `accelerator.prepare`ØŒ ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Megatron-LM Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ Ù„Ù†Ù…ÙˆØ°Ø¬ Transformers Ù…Ø¹ÙŠÙ†
Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… `accelerator.load_state` Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Megatron-LM Ù…Ø¹ Ø£Ù‚Ø³Ø§Ù… TP ÙˆPP ÙˆDP Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©. 
5. Ø­Ø§Ù„ÙŠÙ‹Ø§ØŒ ÙŠØªÙˆÙØ± Ø¯Ø¹Ù… ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ ÙˆØ§Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠ Ù„Ù†Ù…ÙˆØ°Ø¬ GPT ÙÙ‚Ø·.
Ø³ÙŠØªÙ… ØªÙˆØ³ÙŠØ¹Ù‡ Ù‚Ø±ÙŠØ¨Ù‹Ø§ Ù„ÙŠØ´Ù…Ù„ BERT ÙˆT5. 
6. ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† `gradient_accumulation_steps` 1. Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Megatron-LMØŒ ÙØ¥Ù† Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø© ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ÙŠ
Ù…Ø±Ø§Ø¯Ù Ù„ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª. 
7. Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Megatron-LMØŒ Ø§Ø³ØªØ®Ø¯Ù… `accelerator.save_state` Ùˆ `accelerator.load_state` Ù„Ø­ÙØ¸ ÙˆØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙØªÙŠØ´. 
8. ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø®Ø±ÙŠØ·Ø© Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª Ù†Ù…ÙˆØ°Ø¬ Megatron-LM Ø¥Ù„Ù‰ Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— transformers Ø§Ù„Ù…ÙƒØ§ÙØ¦Ø©.
ÙŠØªÙ… Ø¯Ø¹Ù… Ù†Ù…Ø§Ø°Ø¬ ğŸ¤— transformers Ù‡Ø°Ù‡ ÙÙ‚Ø·. 
Ø£. Megatron-LM [BertModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/bert_model.py) :
ğŸ¤— Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ù…Ø¹ `megatron-bert` ÙÙŠ Ù†ÙˆØ¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙƒÙˆÙŠÙ†ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ
[MegatronBERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert) 
Ø¨. Megatron-LM [GPTModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py) :
ğŸ¤— Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ù…Ø¹ `gpt2` ÙÙŠ Ù†ÙˆØ¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙƒÙˆÙŠÙ†ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ
[OpenAI GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) 
Ø¬. Megatron-LM [T5Model](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py) :
ğŸ¤— Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ù…Ø¹ `t5` ÙÙŠ Ù†ÙˆØ¹ Ø§Ù„ØªÙƒÙˆÙŠÙ†ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ
[T5](https://huggingface.co/docs/transformers/model_doc/t5) Ùˆ
[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)