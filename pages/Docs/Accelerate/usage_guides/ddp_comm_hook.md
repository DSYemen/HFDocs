# Ø®Ø·Ø§ÙØ§Øª Ø§ØªØµØ§Ù„ DDP

ØªÙˆÙØ± Ø®Ø·Ø§ÙØ§Øª Ø§ØªØµØ§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ²Ø¹Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© (DDP) ÙˆØ§Ø¬Ù‡Ø© Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ ÙƒÙŠÙÙŠØ© Ù†Ù‚Ù„ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø¹Ù…Ø§Ù„ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¬Ø§ÙˆØ² allreduce Ø§Ù„ÙØ§Ù†ÙŠÙ„ÙŠØ§ ÙÙŠ `DistributedDataParallel`. ÙŠØªÙ… ØªÙˆÙÙŠØ± Ø¨Ø¹Ø¶ Ø®Ø·Ø§ÙØ§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©ØŒ ÙˆÙŠÙ…ÙƒÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ØªØ·Ø¨ÙŠÙ‚ Ø£ÙŠ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·Ø§ÙØ§Øª Ø¨Ø³Ù‡ÙˆÙ„Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§ØªØµØ§Ù„.

- **Ø®Ø·Ø§Ù Ø¶ØºØ· FP16**: ÙŠÙ‚ÙˆÙ… Ø¨Ø¶ØºØ· Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø¹Ø§Ø¦Ù…Ø© Ø°Ø§Øª Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù†ØµÙÙŠØ© (`torch.float16`)ØŒ Ù…Ù…Ø§ ÙŠÙ‚Ù„Ù„ Ù…Ù† Ø§Ù„Ù†ÙÙ‚Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø§ØªØµØ§Ù„.

- **Ø®Ø·Ø§Ù Ø¶ØºØ· BF16**: Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù€ FP16ØŒ ÙˆÙ„ÙƒÙ†Ù‡ ÙŠØ³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø¹Ø§Ø¦Ù…Ø© Ø§Ù„Ø¯Ù…Ø§ØºÙŠØ© (`torch.bfloat16`)ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø¹Ù„Ù‰ Ø£Ø¬Ù‡Ø²Ø© Ù…Ø¹ÙŠÙ†Ø©.

- **Ø®Ø·Ø§Ù PowerSGD**: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø¶ØºØ· ØªØ¯Ø±Ø¬ Ù…ØªÙ‚Ø¯Ù…Ø© ØªÙˆÙØ± Ù…Ø¹Ø¯Ù„Ø§Øª Ø¶ØºØ· Ø¹Ø§Ù„ÙŠØ© ÙˆÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³Ø±Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ±Ø¯Ø¯ÙŠ.

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØŒ Ø³ØªØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø·Ø§ÙØ§Øª Ø§ØªØµØ§Ù„ DDP Ø¨Ø³Ø±Ø¹Ø© ÙˆØ¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙÙŠ ğŸ¤— AccelerateØŒ ÙˆØ§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ø³ÙŠØ·Ø© Ù…Ø«Ù„ Ø¥Ø¶Ø§ÙØ© Ø³Ø·Ø± Ø±Ù…Ø² ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·! ÙˆÙ‡Ø°Ø§ ÙŠÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø·Ø§ÙØ§Øª Ø§ØªØµØ§Ù„ DDP Ù„ØªØ­Ø³ÙŠÙ† Ø§ØªØµØ§Ù„ Ø§Ù„ØªØ¯Ø±Ø¬ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— Accelerate.

## Ø®Ø·Ø§Ù Ø¶ØºØ· FP16

<hfoptions id="fp16">

<hfoption id="PyTorch">

```python
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed.algorithms.ddp_comm_hooks import default_hooks

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

model = MyModel()
model = DDP(model, device_ids=[torch.cuda.current_device()])
model.register_comm_hook(state=None, hook=default_hooks.fp16_compress_hook)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

</hfoption>

<hfoption id="Accelerate">

```python
from accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs
import torch

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù Ø§ØªØµØ§Ù„ DDP
ddp_kwargs = DistributedDataParallelKwargs(comm_hook=DDPCommunicationHookType.FP16)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])

model = MyModel()
optimizer = torch.optim.Adam(model.parameters())
data_loader = DataLoader(dataset, batch_size=16)

model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
```

</hfoption>

</hfoptions>

### Ø®Ø·Ø§Ù Ø¶ØºØ· BF16

<Tip warning={true}>

ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø®Ø·Ø§Ù Ø¶ØºØ· BF16 ØªØ¬Ø±ÙŠØ¨ÙŠØ©ØŒ ÙˆÙ‡ÙŠ ØªØªØ·Ù„Ø¨ Ø¥ØµØ¯Ø§Ø± NCCL Ø£Ø­Ø¯Ø« Ù…Ù† 2.9.6.

</Tip>

<hfoptions id="bf16">

<hfoption id="PyTorch">

```python
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed.algorithms.ddp_comm_hooks import default_hooks

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

model = MyModel()
model = DDP(model, device_ids=[torch.cuda.current_device()])
model.register_comm_hook(state=None, hook=default_hooks.bf16_compress_hook)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

</hfoption>

<hfoption id="Accelerate">

```python
from accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs
import torch

class Myamodel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù Ø§ØªØµØ§Ù„ DDP
ddp_kwargs = DistributedDataParallelKwargs(comm_hook=DDPCommunicationHookType.BF16)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])

model = MyModel()
optimizer = torch.optim.Adam(model.parameters())
data_loader = DataLoader(dataset, batch_size=16)

model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
```

</hfoption>

</hfoptions>

### Ø®Ø·Ø§Ù PowerSGD

<Tip warning={true}>

Ø¹Ø§Ø¯Ø©Ù‹ Ù…Ø§ ÙŠØªØ·Ù„Ø¨ PowerSGD Ø°Ø§ÙƒØ±Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ù†ÙØ³ Ø­Ø¬Ù… ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¹ÙˆØ¶ Ø¹Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø¶ØºÙˆØ· Ø§Ù„Ù…ØªØ­ÙŠØ² ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©.

</Tip>

<hfoptions id="powerSGD">

<hfoption id="PyTorch">

```python
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

model = MyModel()
model = DDP(modelØŒ device_ids = [torch.cuda.current_device()])
state = powerSGD_hook.PowerSGDState(process_group=None)
model.register_comm_hook(state=state, hook=powerSGD_hook.powerSGD_hook)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

</hfoption>

<hfoption id="Accelerate">

```python
from accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs
import torch

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù Ø§ØªØµØ§Ù„ DDP
ddp_kwargs = DistributedDataParallelKwargs(comm_hook=DDPCommunicationHookType.POWER_SGD)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])

model = MyModel()
optimizer = torch.optim.Adam(model.parameters())
data_loader = DataLoader(dataset, batch_size=16)

model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
```

</hfoption>

</hfoptions>

## Ø§Ù„Ù…Ø±Ø§ÙÙ‚ Ø®Ø·Ø§ÙØ§Øª Ø§ØªØµØ§Ù„ DDP

Ù‡Ù†Ø§Ùƒ Ø£Ø¯Ø§ØªØ§Ù† Ø¥Ø¶Ø§ÙÙŠØªØ§Ù† Ù„Ø¯Ø¹Ù… Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© Ù…Ø¹ Ø®Ø·Ø§ÙØ§Øª Ø§Ù„Ø§ØªØµØ§Ù„.

### comm_wrapper

`comm_wrapper` Ù‡Ùˆ Ø®ÙŠØ§Ø± Ù„ØªØºÙ„ÙŠÙ Ø®Ø·Ø§Ù Ø§ØªØµØ§Ù„ Ø¨ÙˆØ¸Ø§Ø¦Ù Ø¥Ø¶Ø§ÙÙŠØ©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¯Ù…Ø¬ Ø¶ØºØ· FP16 Ù…Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø£Ø®Ø±Ù‰. Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªØºÙ„ÙŠÙ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ Ù‡ÙŠ `no` Ùˆ`fp16` Ùˆ`bf16`.

```python
from accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs
import torch

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù Ø§ØªØµØ§Ù„ DDP
ddp_kwargs = DistributedDataParallelKwargs(
comm_hook=DDPCommunicationHookType.POWER_SGDØŒ
comm_wrapper=DDPCommunicationHookType.FP16
)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])

model = MyModel()
optimizer = torch.optim.Adam(model.parameters())
data_loader = DataLoader(dataset, batch_size=16)

model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
```

### comm_state_option

ÙŠØ³Ù…Ø­ Ù„Ùƒ `comm_state_option` Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø§Ù„Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨ÙˆØ§Ø³Ø·Ø© Ø®Ø·Ø§ÙØ§Øª Ø§ØªØµØ§Ù„ Ù…Ø¹ÙŠÙ†Ø©. Ù‡Ø°Ø§ Ù…ÙÙŠØ¯ Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ Ù„Ù„Ø®Ø·Ø§ÙØ§Øª Ø°Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø© Ù…Ø«Ù„ `PowerSGD`ØŒ ÙˆØ§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© ÙˆØ§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø¹Ø¨Ø± Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨. ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ ÙŠÙˆØ¶Ø­ Ø§Ø³ØªØ®Ø¯Ø§Ù… `comm_state_option` Ù…Ø¹ Ø®Ø·Ø§Ù `PowerSGD`.

```python
from accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs
import torch

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù Ø§ØªØµØ§Ù„ DDP
ddp_kwargs = DistributedDataParallelKwargs(
comm_hook=DDPCommunicationHookType.POWER_SGDØŒ
comm_state_option={"matrix_approximation_rank": 2}
)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])

model = MyModel()
optimizer = torch.optim.Adam(model.parameters())
data_loader = DataLoader(dataset, batch_size=16)

model, optimizer, data_loader = accelerator.prepare(model, optimizerØŒ data_loader)

# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
for data, targets in data_loader:
    outputs = model(data)
    loss = criterion(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
```

Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙƒØ«Ø± ØªÙ‚Ø¯Ù…Ù‹Ø§ ÙˆØ®Ø·Ø§ÙØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©ØŒ Ø±Ø§Ø¬Ø¹ ÙˆØ«Ø§Ø¦Ù‚ [Ø®Ø·Ø§ÙØ§Øª Ø§ØªØµØ§Ù„ PyTorch DDP](https://pytorch.org/docs/stable/ddp_comm_hooks.html).