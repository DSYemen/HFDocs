# Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø£Ù…Ø«Ù„ Ù…Ø¹ ONNX Runtime
Optimum Ù‡ÙŠ Ø­Ø²Ù…Ø© Ø¨Ø±Ø§Ù…Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¨Ù†Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…Ø¹ ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¹Ø¬Ù„ Ù…Ø«Ù„ ONNX Runtime.
ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Optimum Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ù† [Hugging Face Hub](hf.co/models) ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø®Ø·ÙˆØ· Ø£Ù†Ø§Ø¨ÙŠØ¨
Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ø¹Ø¬Ù„ Ø¯ÙˆÙ† Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.

## Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„
ÙØ¦Ø§Øª Ù†Ù…ÙˆØ°Ø¬ `optimum.onnxruntime.ORTModelForXXX` Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù†Ù…Ø§Ø°Ø¬ Hugging Face Transformers. ÙˆÙ‡Ø°Ø§
ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ ÙÙ‚Ø· Ø§Ø³ØªØ¨Ø¯Ø§Ù„ ÙØ¦Ø© `AutoModelForXXX` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© `ORTModelForXXX` ÙÙŠ `optimum.onnxruntime`.

Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙƒÙŠÙŠÙ Ø±Ù…Ø²Ùƒ Ù„Ø¬Ø¹Ù„Ù‡ ÙŠØ¹Ù…Ù„ Ù…Ø¹ ÙØ¦Ø§Øª `ORTModelForXXX`:

```diff
from transformers import AutoTokenizer, pipeline
-from transformers import AutoModelForQuestionAnswering
+from optimum.onnxruntime import ORTModelForQuestionAnswering

-model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2") # PyTorch checkpoint
+model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2") # ONNX checkpoint
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

onnx_qa = pipeline("question-answering",model=model,tokenizer=tokenizer)

question = "What's my name?"
context = "My name is Philipp and I live in Nuremberg."
pred = onnx_qa(question, context)
```

### ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Transformers Ø¹Ø§Ø¯ÙŠ
Ù„Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹Ù‡ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ù…Ø­ÙˆÙ„Ù‹Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¥Ù„Ù‰ ONNXØŒ ÙŠØªØ¶Ù…Ù† [`~optimum.onnxruntime.ORTModel`]
Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Transformers Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX. Ù…Ø§ Ø¹Ù„ÙŠÙƒ Ø³ÙˆÙ‰ ØªÙ…Ø±ÙŠØ± `export=True` Ø¥Ù„Ù‰
Ø·Ø±ÙŠÙ‚Ø© [`~optimum.onnxruntime.ORTModel.from_pretrained`]`~optimum.onnxruntime.ORTModel.from_pretrained`ØŒ ÙˆØ³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ ONNX Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„:

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification

>>> # Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ø±ÙƒØ² ÙˆØªØµØ¯ÙŠØ±Ù‡ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX
>>> model = ORTModelForSequenceClassification.from_pretrained(
...     "distilbert-base-uncased-finetuned-sst-2-english", export=True
... )
```

### Ø¯ÙØ¹ Ù†Ù…Ø§Ø°Ø¬ ONNX Ø¥Ù„Ù‰ Hugging Face Hub
Ù…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ØŒ ÙƒÙ…Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ø§Ù„ Ù…Ø¹ [`~transformers.PreTrainedModel`]s Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©ØŒ Ø¯ÙØ¹ `ORTModelForXXX` Ø¥Ù„Ù‰
[Hugging Face Model Hub](https://hf.co/models):

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification

>>> # Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ø±ÙƒØ² ÙˆØªØµØ¯ÙŠØ±Ù‡ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX
>>> model = ORTModelForSequenceClassification.from_pretrained(
...     "distilbert-base-uncased-finetuned-sst-2-english", export=True
... )

>>> # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„
>>> model.save_pretrained("a_local_path_for_convert_onnx_model")

# Ø§Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬ onnx Ø¥Ù„Ù‰ HF Hub
>>> model.push_to_hub(  # doctest: +SKIP
...   "a_local_path_for_convert_onnx_model", repository_id="my-onnx-repo", use_auth_token=True
... )
```

## Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„
ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ (Seq2Seq) Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime. Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… ØªØµØ¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Seq2Seq
Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNXØŒ ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø«Ù„Ø§Ø«Ø© Ø£Ø¬Ø²Ø§Ø¡ ÙŠØªÙ… Ø¯Ù…Ø¬Ù‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„:

- Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø´ÙØ± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
- Ø§Ù„Ø¬Ø²Ø¡ ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ + Ø±Ø£Ø³ Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©
- Ù†ÙØ³ Ø§Ù„Ø¬Ø²Ø¡ ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ + Ø±Ø£Ø³ Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© ÙˆÙ„ÙƒÙ† Ù…Ø¹ Ø¥Ø¯Ø®Ø§Ù„ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© / Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙƒØ¥Ø¯Ø®Ø§Ù„Ø§Øª ÙˆÙ…Ø®Ø±Ø¬Ø§Øª. ÙŠØ¬Ø¹Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø£Ø³Ø±Ø¹.

ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ T5 Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø©:

```python
>>> from transformers import AutoTokenizer, pipeline
>>> from optimum.onnxruntime import ORTModelForSeq2SeqLM

# Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ø±ÙƒØ² ÙˆØªØµØ¯ÙŠØ±Ù‡ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX
>>> model_name = "t5-small"
>>> model = ORTModelForSeq2SeqLM.from_pretrained(model_name, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨
>>> onnx_translation = pipeline("translation_en_to_fr"ØŒ model=modelØŒ tokenizer=tokenizer)
>>> text = "He never went out without a book under his arm, and he often came back with two."
>>> result = onnx_translation(text)
>>> # [{'translation_text': "Il n'est jamais sorti sans un livre sous son bras, et il est souvent revenu avec deux."}]
```

## Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø±
ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Stable Diffusion Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime. Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… ØªØµØ¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Stable Diffusion
Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNXØŒ ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹Ø© Ù…ÙƒÙˆÙ†Ø§Øª ÙŠØªÙ… Ø¯Ù…Ø¬Ù‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„:

- Ù…Ø´ÙØ± Ø§Ù„Ù†Øµ
- U-NET
- ÙÙƒ ØªØ´ÙÙŠØ± VAE
- ÙÙƒ ØªØ´ÙÙŠØ± VAE

ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª ğŸ¤— Diffusers.

Ù„ØªØ«Ø¨ÙŠØª `diffusers`:

```bash
pip install diffusers
```

### Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©
ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ONNX Stable Diffusion ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime:

```python
from optimum.onnxruntime import ORTStableDiffusionPipeline

model_id = "runwayml/stable-diffusion-v1-5"
pipeline = ORTStableDiffusionPipeline.from_pretrained(model_id, revision="onnx")
prompt = "sailing ship in storm by Leonardo da Vinci"
image = pipeline(prompt).images[0]
```

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ PyTorch Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ ONNX Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† `export=True`.

```python
pipeline = ORTStableDiffusionPipeline.from_pretrained(model_id, export=True)

# Ù„Ø§ ØªÙ†Ø³ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ONNX
save_directory = "a_local_path"
pipeline.save_pretrained(save_directory)
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/stable_diffusion_v1_5_ort_sail_boat.png">
</div>

### ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø©

```python
import requests
import torch
from PIL import Image
from io import BytesIO
from optimum.onnxruntime import ORTStableDiffusionImg2ImgPipeline

model_id = "runwayml/stable-diffusion-v1-5"
pipeline = ORTStableDiffusionImg2ImgPipeline.from_pretrained(model_id, revision="onnx")

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

response = requests.get(url)
init_image = Image.open(BytesIO(response.content)).convert("RGB")
init_image = init_image.resize((768, 512))

prompt = "A fantasy landscape, trending on artstation"

image = pipeline(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images[0]
image.save("fantasy_landscape.png")
```

### Ø¥ØµÙ„Ø§Ø­

```python
import PIL
import requests
import torch
from io import BytesIO
from optimum.onnxruntime import ORTStableDiffusionInpaintPipeline

model_id = "runwayml/stable-diffusion-inpainting"
pipeline = ORTStableDiffusionInpaintPipeline.from_pretrained(model_id, revision="onnx")

def download_image(url):
response = requests.get(url)
return PIL.Image.open(BytesIO(response.content)).convert("RGB")

img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

init_image = download_image(img_url).resize((512, 512))
mask_image = download_image(mask_url).resize((512, 512))

prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]
```

## Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø± XL
Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… `ORTStableDiffusionXLPipeline`ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª `diffusers` Ùˆ`invisible_watermark`. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„ØªØ§Ù„ÙŠ:

```bash
pip install diffusers
pip install invisible-watermark>=0.2.0
```

### Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©
ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ SDXL ONNX Ù…Ù† [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime:

```python
from optimum.onnxruntime import ORTStableDiffusionXLPipeline

model_id = "stabilityai/stable-diffusion-xl-base-1.0"
base = ORTStableDiffusionXLPipeline.from_pretrained(model_id)
prompt = "sailing ship in storm by Leonardo da Vinci"
image = base(prompt).images[0]

# Ù„Ø§ ØªÙ†Ø³ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ONNX
save_directory = "sd_xl_base"
base.save_pretrained(save_directory)
```

### ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø©
ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ PyTorch SDXLØŒ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ ONNX Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„ØŒ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime Ù„Ù€ *image-to-image*:

```python
from optimum.onnxruntime import ORTStableDiffusionXLImg2ImgPipeline
from diffusers.utils import load_image

model_id = "stabilityai/stable-diffusion-xl-refiner-1.0"
pipeline = ORTStableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, export=True)

url = "https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/sd_xl/castle_friedrich.png"
image = load_image(url).convert("RGB")
prompt = "medieval castle by Caspar David Friedrich"
image = pipeline(prompt, image=image).images[0]
image.save("medieval_castle.png")
```

### ØªØ­Ø³ÙŠÙ† Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©
ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ø«Ù„ [stabilityai/stable-diffusion-xl-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0). ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ØªØ­ØªØ§Ø¬ ÙÙ‚Ø· Ø¥Ù„Ù‰ Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø®ÙÙˆÙ†Ø§Øª Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ.

```python
from optimum.onnxruntime import ORTStableDiffusionXLImg2ImgPipeline

model_id = "stabilityai/stable-diffusion-xl-refiner-1.0"
refiner = ORTStableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, export=True)

image = base(prompt=prompt, output_type="latent").images[0]
image = refiner(prompt=prompt, image=image[None, :]).images[0]
image.save("sailing_ship.png")
```

## Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§ØªØ³Ø§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù†Ø©

### Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©
ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§ØªØ³Ø§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù†Ø© (LCMs) Ù…Ù† [SimianLuo/LCM_Dreamshaper_v7](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7) ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime:

```python
from optimum.onnxruntime import ORTLatentConsistencyModelPipeline

model_id = "SimianLuo/LCM_Dreamshaper_v7"
pipeline = ORTLatentConsistencyModelPipeline.from_pretrained(model_id, export=True)
prompt = "sailing ship in storm by Leonardo da Vinci"
images = pipeline(promptØŒ num_inference_steps=4ØŒ guidance_scale=8.0).images
```