# Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ø²Ø£Ø©

ØªÙ… ØªØ·ÙˆÙŠØ± [Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ø²Ø£Ø©](https://pytorch.org/docs/stable/fsdp.html) (FSDP) Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„ØªÙŠ ÙŠØµÙ„ Ø­Ø¬Ù…Ù‡Ø§ Ø¥Ù„Ù‰ 1 ØªÙŠØ±Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª. ÙˆÙŠØ­Ù‚Ù‚ FSDP Ø°Ù„Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¬Ø²Ø¦Ø© Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªØ¯Ø±Ø¬Ø§Øª ÙˆØ­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù† Ø¹Ø¨Ø± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙŠÙ…ÙƒÙ†Ù‡ Ø£ÙŠØ¶Ù‹Ø§ Ù†Ù‚Ù„ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ø²Ø£Ø© Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU). ØªØ³Ù…Ø­ ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙŠ ÙŠÙˆÙØ±Ù‡Ø§ FSDP Ø¨Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Ø§Ù„Ø¯ÙÙØ¹Ø© Ø£Ùˆ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.

ÙŠØªÙ… Ø¯Ø¹Ù… ÙƒÙ„Ø§ Ø§Ù„Ù…ÙŠØ²ØªÙŠÙ† ÙÙŠ ğŸ¤— AccelerateØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ù…Ø§ Ù…Ø¹ ğŸ¤— PEFT.

# Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT ÙˆFSDP

Ø³ÙŠØ³Ø§Ø¹Ø¯Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ù…Ù† Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¹Ù„Ù‰ ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ØµÙ†Ø§ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ [Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†ØµÙŠ](https://github.com/huggingface/peft/blob/main/examples/sft/train.py) DeepSpeed Ù„Ø£Ø¯Ø§Ø¡ SFT. Ø³ØªÙ‚ÙˆÙ… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ù€ SFT (Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù) Ù„Ù†Ù…ÙˆØ°Ø¬ Llama-70B Ù…Ø¹ LoRA ÙˆFSDP Ø¹Ù„Ù‰ 8xH100 Ù…Ù† ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª (GPU) Ø¨Ø³Ø¹Ø© 80 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² ÙˆØ§Ø­Ø¯. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙƒÙˆÙŠÙ†Ù‡ Ù„ÙŠØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø¹Ø¯Ø© Ø¢Ù„Ø§Øª Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØºÙŠÙŠØ± ØªÙƒÙˆÙŠÙ† Ø¨Ø±Ù†Ø§Ù…Ø¬ Accelerate.

## Ø§Ù„ØªÙ‡ÙŠØ¦Ø©

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙ‡ÙŠØ¦Ø© FSDP Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Accelerate. ÙŠØ³Ù…Ø­ Ø¹Ù„Ù… `--config_file` Ø¨Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙÙŠ Ù…ÙˆÙ‚Ø¹ Ù…Ø­Ø¯Ø¯ØŒ ÙˆØ¥Ù„Ø§ ÙØ³ÙŠØªÙ… Ø­ÙØ¸Ù‡ ÙƒÙ…Ù„Ù `default_config.yaml` ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ ğŸ¤— Accelerate.

ÙŠÙØ³ØªØ®Ø¯Ù… Ù…Ù„Ù Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ù„Ø¶Ø¨Ø· Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ù†Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

```bash
accelerate config --config_file fsdp_config.yaml
```

Ø³ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ø¥Ø¹Ø¯Ø§Ø¯ÙƒØŒ ÙˆØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„ØªØ§Ù„ÙŠØ©. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³ØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¨ÙŠØ§Ù† ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø£Ø¯Ù†Ø§Ù‡.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/fsdp-peft-config.png"/>
</div>

<small>Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‡ÙŠØ¦Ø© Accelerate Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… FSDP</small>

Ø¨Ù…Ø¬Ø±Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø°Ù„ÙƒØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ùˆ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡ØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙÙŠ [fsdp_config.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/fsdp_config.yaml):

```yml
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
fsdp_backward_prefetch: BACKWARD_PRE
fsdp_cpu_ram_efficient_loading: true
fsdp_forward_prefetch: false
fsdp_offload_params: false
fsdp_sharding_strategy: FULL_SHARD
fsdp_state_dict_type: SHARDED_STATE_DICT
fsdp_sync_module_states: true
fsdp_use_orig_params: false
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

## Ø£Ù…Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚

Ø£Ù…Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ù…ØªØ§Ø­ ÙÙŠ [run_peft_fsdp.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_fsdp.sh) ÙˆÙ‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡ Ø£ÙŠØ¶Ù‹Ø§:

```bash
accelerate launch --config_file "configs/fsdp_config.yaml"  train.py \
--seed 100 \
--model_name_or_path "meta-llama/Llama-2-70b-hf" \
--dataset_name "smangrul/ultrachat-10k-chatml" \
--chat_template_format "chatml" \
--add_special_tokens False \
--append_concat_token False \
--splits "train,test" \
--max_seq_len 2048 \
--num_train_epochs 1 \
--logging_steps 5 \
--log_level "info" \
--logging_strategy "steps" \
--evaluation_strategy "epoch" \
--save_strategy "epoch" \
--push_to_hub \
--hub_private_repo True \
--hub_strategy "every_save" \
--bf16 True \
--packing True \
--learning_rate 1e-4 \
--lr_scheduler_type "cosine" \
--weight_decay 1e-4 \
--warmup_ratio 0.0 \
--max_grad_norm 1.0 \
--output_dir "llama-sft-lora-fsdp" \
--per_device_train_batch_size 8 \
--per_device_eval_batch_size 8 \
--gradient_accumulation_steps 4 \
--gradient_checkpointing True \
--use_reentrant False \
--dataset_text_field "content" \
--use_flash_attn True \
--use_peft_lora True \
--lora_r 8 \
--lora_alpha 16 \
--lora_dropout 0.1 \
--lora_target_modules "all-linear" \
--use_4bit_quantization False
```

Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… LoRA Ù…Ø¹ Ø§Ù„Ø±ØªØ¨Ø© 8ØŒ Ùˆ alpha=16ØŒ ÙˆÙ†Ø³ØªÙ‡Ø¯Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©. Ù†Ù…Ø±Ø± Ù…Ù„Ù ØªÙ‡ÙŠØ¦Ø© FSDP ÙˆÙ†Ø¶Ø¨Ø· Ø¯Ù‚Ø© Ù†Ù…ÙˆØ°Ø¬ Llama Ø¨Ø­Ø¬Ù… 70 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ø¹Ù„Ù‰ Ø¬Ø²Ø¡ ÙØ±Ø¹ÙŠ Ù…Ù† [Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ultrachat](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k).

## Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø©

Ø¯Ø¹ÙˆÙ†Ø§ Ù†ØªØ¹Ù…Ù‚ Ù‚Ù„ÙŠÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ø±Ø¤ÙŠØ© Ù…Ø§ ÙŠØ­Ø¯Ø«ØŒ ÙˆÙÙ‡Ù… ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„Ù‡.

Ø£ÙˆÙ„ Ø´ÙŠØ¡ ÙŠØ¬Ø¨ Ù…Ø¹Ø±ÙØªÙ‡ Ù‡Ùˆ Ø£Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ ÙŠØ³ØªØ®Ø¯Ù… FSDP Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ Ù†Ø¸Ø±Ù‹Ø§ Ù„Ù…Ø±ÙˆØ± ØªÙ‡ÙŠØ¦Ø© FSDP. ØªØªÙˆÙ„Ù‰ ÙØ¦Ø© `SFTTrainer` Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø«Ù‚ÙŠÙ„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ PEFT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‡ÙŠØ¦Ø© PEFT Ø§Ù„ØªÙŠ ØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ù†Ø¯Ù…Ø§ ØªØ³ØªØ¯Ø¹ÙŠ `trainer.train()`ØŒ ÙŠØ³ØªØ®Ø¯Ù… Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§ ğŸ¤— Accelerate Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­Ø³Ù† ÙˆØ¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‡ÙŠØ¦Ø© FSDP Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù„ÙÙˆÙ Ø¨Ù€ FSDP ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ. Ù…Ù‚ØªØ·Ù Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù‡Ùˆ Ø£Ø¯Ù†Ø§Ù‡:

```python
# trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
args=training_args,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
peft_config=peft_config,
packing=data_args.packing,
dataset_kwargs={
"append_concat_token": data_args.append_concat_token,
"add_special_tokens": data_args.add_special_tokens,
},
dataset_text_field=data_args.dataset_text_field,
max_seq_length=data_args.max_seq_length,
)
trainer.accelerator.print(f"{trainer.model}")
if model_args.use_peft_lora:
# handle PEFT+FSDP case
trainer.model.print_trainable_parameters()
if getattr(trainer.accelerator.state, "fsdp_plugin", None):
from peft.utils.other import fsdp_auto_wrap_policy

fsdp_plugin = trainer.accelerator.state.fsdp_plugin
fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)

# train
checkpoint = None
if training_args.resume_from_checkpoint is not None:
checkpoint = training_args.resume_from_checkpoint
trainer.train(resume_from_checkpoint=checkpoint)

# saving final model
if trainer.is_fsdp_enabled:
trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")
trainer.save_model()
```

Ù‡Ù†Ø§Ùƒ Ø´ÙŠØ¡ Ø±Ø¦ÙŠØ³ÙŠ ÙŠØ¬Ø¨ Ù…Ù„Ø§Ø­Ø¸ØªÙ‡ Ù‡Ù†Ø§ Ù‡Ùˆ Ø£Ù†Ù‡ Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… FSDP Ù…Ø¹ PEFTØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† `use_orig_params` `False` Ù„ØªØ­Ù‚ÙŠÙ‚ ÙˆÙÙˆØ±Ø§Øª ÙÙŠ Ø°Ø§ÙƒØ±Ø© GPU. ÙˆØ¨Ø³Ø¨Ø¨ `use_orig_params=False`ØŒ ÙŠØ¬Ø¨ ØªØºÙŠÙŠØ± Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØªØºÙ„ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù€ FSDP Ø¨Ø­ÙŠØ« ÙŠØªÙ… ØªØºÙ„ÙŠÙ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØºÙŠØ± Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„. ÙŠØªÙ… Ø°Ù„Ùƒ Ø¨ÙˆØ§Ø³Ø·Ø© Ù…Ù‚ØªØ·Ù Ø§Ù„ÙƒÙˆØ¯ Ø£Ø¯Ù†Ø§Ù‡ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© `fsdp_auto_wrap_policy` Ù…Ù† PEFT:

```
if getattr(trainer.accelerator.state, "fsdp_plugin", None):
from peft.utils.other import fsdp_auto_wrap_policy

fsdp_plugin = trainer.accelerator.state.fsdp_plugin
fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)
```

## Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©

ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ ØªØ¨Ù„Øº Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙŠ ØªØ³ØªÙ‡Ù„ÙƒÙ‡Ø§ ÙƒÙ„ ÙˆØ­Ø¯Ø© GPU 72-80 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª (90-98Ùª) ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ ÙÙŠ Ù„Ù‚Ø·Ø© Ø§Ù„Ø´Ø§Ø´Ø© Ø£Ø¯Ù†Ø§Ù‡. ØªØ­Ø¯Ø« Ø§Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø·ÙÙŠÙØ© ÙÙŠ Ø°Ø§ÙƒØ±Ø© GPU ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø¹Ù†Ø¯ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙˆØ¹ `FULL_STATE_DICT` Ù…Ù† Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† `SHARDED_STATE_DICT` Ø­ØªÙ‰ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ù…Ø­ÙˆÙ„ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© `from_pretrained` Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_fsdp_mem_usage.png"/>
</div>

<small>Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© GPU Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨</small>

# Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT QLoRA ÙˆFSDP Ù„Ø¶Ø¨Ø· Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª (GPU) Ù…ØªØ¹Ø¯Ø¯Ø©

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…ØŒ Ø³Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… QLoRA ÙˆFSDP Ù„Ø¶Ø¨Ø· Ø¯Ù‚Ø© Ù†Ù…ÙˆØ°Ø¬ Llama Ø¨Ø­Ø¬Ù… 70 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ø¹Ù„Ù‰ ÙˆØ­Ø¯ØªÙŠ GPU Ø¨Ø³Ø¹Ø© 24 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª. Ù‚Ø§Ù…Øª Ø´Ø±ÙƒØ© [Answer.AI](https://www.answer.ai/) Ø¨Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ bitsandbytes ÙˆHugging Face ğŸ¤— Ø¨Ø¥ØªØ§Ø­Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ‘Ù† Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… FSDP+QLoRA ÙˆØ´Ø±Ø­Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ ÙÙŠ Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© Ø§Ù„Ù…ÙÙŠØ¯ [ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºØ© Ø¨Ø­Ø¬Ù… 70 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª ÙÙŠ Ø§Ù„Ù…Ù†Ø²Ù„](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html). ØªÙ… Ø¯Ù…Ø¬ Ù‡Ø°Ø§ Ø§Ù„Ø¢Ù† ÙÙŠ Ù†Ø¸Ø§Ù… Hugging Face Ø§Ù„Ø¨ÙŠØ¦ÙŠ.

Ù„Ù‡Ø°Ø§ØŒ Ù†Ø­ØªØ§Ø¬ Ø£ÙˆÙ„Ø§Ù‹ Ø¥Ù„Ù‰ `bitsandbytes>=0.43.0`ØŒ Ùˆ`accelerate>=0.28.0`ØŒ Ùˆ`transformers>4.38.2`ØŒ Ùˆ`trl>0.7.11`ØŒ Ùˆ`peft>0.9.0`. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¹ÙŠÙŠÙ† `fsdp_cpu_ram_efficient_loading=true`ØŒ Ùˆ`fsdp_use_orig_params=false`ØŒ Ùˆ`fsdp_offload_params=true` (Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨) Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‡ÙŠØ¦Ø© Ø¨Ø±Ù†Ø§Ù…Ø¬ Accelerate. Ø¹Ù†Ø¯ Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ AccelerateØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© `export FSDP_CPU_RAM_EFFICIENT_LOADING=true`. Ù‡Ù†Ø§ØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… ØªÙ‡ÙŠØ¦Ø© Ø¨Ø±Ù†Ø§Ù…Ø¬ Accelerate ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø© Ø£Ø¯Ù†Ø§Ù‡ Ù…ØªÙˆÙØ±Ø© ÙÙŠ [fsdp_config_qlora.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/fsdp_config_qlora.yaml):

```yml
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
fsdp_backward_prefetch: BACKWARD_PRE
fsdp_cpu_ram_efficient_loading: true
fsdp_forward_prefetch: false
fsdp_offload_params: true
fsdp_sharding_strategy: FULL_SHARD
fsdp_state_dict_type: SHARDED_STATE_DICT
fsdp_sync_module_states: true
fsdp_use_orig_params: false
machine_rank: 0
main_training_function: main
mixed_precision: 'no'
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

Ø£Ù…Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡ ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­ ÙÙŠ [run_peft_qlora_fsdp.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_qlora_fsdp.sh):

```
accelerate launch --config_file "configs/fsdp_config_qlora.yaml"  train.py \
--seed 100 \
--model_name_or_path "meta-llama/Llama-2-70b-hf" \
--dataset_name "smangrul/ultrachat-10k-chatml" \
--chat_template_format "chatml" \
--add_special_tokens False \
--append_concat_token False \
--splits "train,test" \
--max_seq_len 2048 \
--num_train_epochs 1 \
--logging_steps 5 \
--log_level "info" \
--logging_strategy "steps" \
--evaluation_strategy "epoch" \
--save_strategy "epoch" \
--push_to_hub \
--hub_private_repo True \
--hub_strategy "every_save" \
--bf16 True \
--packing True \
--learning_rate 1e-4 \
--lr_scheduler_type "cosine" \
--weight_decay 1e-4 \
--warmup_ratio 0.0 \
--max_grad_norm 1.0 \
--output_dir "llama-sft-qlora-fsdp" \
--per_device_train_batch_size 2 \
--per_device_eval_batch_size 2 \
--gradient_accumulation_steps 2 \
--gradient_checkpointing True \
--use_reentrant True \
--dataset_text_field "content" \
--use_flash_attn True \
--use_peft_lora True \
--lora_r 8 \
--lora_alpha 16 \
--lora_dropout 0.1 \
--lora_target_modules "all-linear" \
--use_4bit_quantization True \
--use_nested_quant True \
--bnb_4bit_compute_dtype "bfloat16" \
--bnb_4bit_quant_storage_dtype "bfloat16"
```

Ù„Ø§Ø­Ø¸ Ø§Ù„Ø­Ø¬Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§ØŒ `bnb_4bit_quant_storage_dtype`ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØºÙ„ÙŠÙ Ù…Ø¹Ù„Ù…Ø§Øª 4 Ø¨Øª. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡ Ø¹Ù„Ù‰ `bfloat16`ØŒ ÙŠØªÙ… ØªØºÙ„ÙŠÙ **16/4 = 4** Ù…Ù† Ù…Ø¹Ù„Ù…Ø§Øª 4 Ø¨Øª Ù…Ø¹Ù‹Ø§ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙƒÙ…ÙŠÙ…. Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø© Ù…Ø¹ `bfloat16`ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† `bnb_4bit_quant_storage_dtype` Ø¥Ù…Ø§ `bfloat16` Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ `bfloat16` Ø§Ù„Ù†Ù‚ÙŠØŒ Ø£Ùˆ `float32` Ù„Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø© (ÙŠØ³ØªÙ‡Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø°Ø§ÙƒØ±Ø© GPU). Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø© Ù…Ø¹ `float16`ØŒ ÙŠØ¬Ø¨ ØªØ¹ÙŠÙŠÙ† `bnb_4bit_quant_storage_dtype` Ø¹Ù„Ù‰ `float32` Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø®ØªÙ„Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙˆØ§Ù„Ù…Ø³ØªÙ‚Ø±.

Ù…Ù† Ø­ÙŠØ« ØªØºÙŠÙŠØ±Ø§Øª ÙƒÙˆØ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ØªØªÙ…Ø«Ù„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ù…Ø§ ÙŠÙ„ÙŠ:

```diff
...

bnb_config = BitsAndBytesConfig(
load