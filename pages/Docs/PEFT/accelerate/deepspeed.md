# DeepSpeed

[DeepSpeed](https://www.deepspeed.ai/) Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© Ù…ØµÙ…Ù…Ø© Ù„Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„ØªÙˆØ³Ø¹ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„ÙŠØ§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª. ÙˆÙÙŠ Ø¬ÙˆÙ‡Ø±Ù‡Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­Ø³Ù† Zero Redundancy Optimizer (ZeRO) Ø§Ù„Ø°ÙŠ ØªÙ‚Ø³Ù… Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù† (ZeRO-1) ÙˆØ§Ù„Ù…Ø¯Ø±Ø¬Ø§Øª (ZeRO-2) ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª (ZeRO-3) Ø¹Ø¨Ø± Ø¹Ù…Ù„ÙŠØ§Øª Ù…ÙˆØ§Ø²ÙŠØ© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙˆÙ‡Ø°Ø§ ÙŠÙ‚Ù„Ù„ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ Ù…Ù…Ø§ ÙŠØªÙŠØ­ Ù„Ùƒ ØªÙˆØ³ÙŠØ¹ Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¥Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ù„ÙŠØ§Ø±. ÙˆÙ„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ØªÙ‚Ù„Ù„ ZeRO-Offload Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø­ÙˆØ³Ø¨Ø© ÙˆØ°Ø§ÙƒØ±Ø© GPU Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù…ÙˆØ§Ø±Ø¯ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ†.

ÙŠØªÙ… Ø¯Ø¹Ù… ÙƒÙ„Ø§ Ø§Ù„Ù…ÙŠØ²ØªÙŠÙ† ÙÙŠ ğŸ¤— AccelerateØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ù…Ø§ Ù…Ø¹ ğŸ¤— PEFT.

## Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ `bitsandbytes` Ø§Ù„ÙƒÙ… + LoRA

ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø¬Ø¯ÙˆÙ„ ÙŠÙ„Ø®Øµ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ø¨ÙŠÙ† PEFT's LoRAØŒ ÙˆÙ…ÙƒØªØ¨Ø© [`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes) Ùˆ DeepSpeed Zero stages ÙÙŠÙ…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚. Ù„Ù† ÙŠÙƒÙˆÙ† Ù„Ù€ DeepSpeed Zero-1 Ùˆ 2 Ø£ÙŠ ØªØ£Ø«ÙŠØ± Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø­ÙŠØ« ØªÙ‚ÙˆÙ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1 Ø¨ØªÙ‚Ø³ÙŠÙ… Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù† ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù† ÙˆØ§Ù„Ù…Ø¯Ø±Ø¬Ø§Øª:

| Ù…Ø±Ø­Ù„Ø© DeepSpeed | Ù‡Ù„ Ù‡Ùˆ Ù…ØªÙˆØ§ÙÙ‚ØŸ |
|---|---|
| Zero-1 |  ğŸŸ¢ |
| Zero-2 |  ğŸŸ¢ |
| Zero-3 |  ğŸŸ¢ |

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€ DeepSpeed Stage 3 + QLoRAØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø³Ù… [Ø§Ø³ØªØ®Ø¯Ù… PEFT QLoRA Ùˆ DeepSpeed Ù…Ø¹ ZeRO3 Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª GPU Ù…ØªØ¹Ø¯Ø¯Ø©](#use-peft-qlora-and-deepspeed-with-zero3-for-finetuning-large-models-on-multiple-gpus) Ø£Ø¯Ù†Ø§Ù‡.

Ù„ØªØ£ÙƒÙŠØ¯ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§ØªØŒ Ù‚Ù…Ù†Ø§ Ø¨ØªØ´ØºÙŠÙ„ SFT (Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù) [Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ø±Ø³Ù…ÙŠØ©](https://github.com/huggingface/trl/tree/main/examples) Ù…Ù† Ù…ÙƒØªØ¨Ø© Transformers Reinforcement Learning (TRL) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… QLoRA + PEFT ÙˆØªÙ‡ÙŠØ¦Ø© Accelerate Ø§Ù„Ù…ØªÙˆÙØ±Ø© [Ù‡Ù†Ø§](https://github.com/huggingface/trl/tree/main/examples/accelerate_configs). ÙˆÙ‚Ø¯ Ø£Ø¬Ø±ÙŠÙ†Ø§ Ù‡Ø°Ù‡ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø¹Ù„Ù‰ 2x NVIDIA T4 GPU.

# Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT Ùˆ DeepSpeed Ù…Ø¹ ZeRO3 Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ Ø£Ø¬Ù‡Ø²Ø© Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆØ¹Ù‚Ø¯ Ù…ØªØ¹Ø¯Ø¯Ø©

Ø³ÙŠØ³Ø§Ø¹Ø¯Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ù…Ù† Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ØµÙ†Ø§ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/sft/train.py) Ù„Ø£Ø¯Ø§Ø¡ SFT. Ø³ØªÙ‚ÙˆÙ… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ù€ SFT (Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù) Ù„Ù†Ù…ÙˆØ°Ø¬ Llama-70B Ù…Ø¹ LoRA Ùˆ ZeRO-3 Ø¹Ù„Ù‰ 8xH100 80GB GPUs Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² ÙˆØ§Ø­Ø¯. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‡ÙŠØ¦ØªÙ‡ Ù„Ù„ØªÙˆØ³Ø¹ Ø¥Ù„Ù‰ Ø£Ø¬Ù‡Ø²Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØºÙŠÙŠØ± ØªÙ‡ÙŠØ¦Ø© Accelerate.

## Ø§Ù„ØªÙ‡ÙŠØ¦Ø©

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙ‡ÙŠØ¦Ø© DeepSpeed Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Accelerate. ÙŠØ³Ù…Ø­ Ø¹Ù„Ù… `--config_file` Ù„Ùƒ Ø¨Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙÙŠ Ù…ÙˆÙ‚Ø¹ Ù…Ø­Ø¯Ø¯ØŒ ÙˆØ¥Ù„Ø§ ÙØ³ÙŠØªÙ… Ø­ÙØ¸Ù‡ ÙƒÙ…Ù„Ù `default_config.yaml` ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ ğŸ¤— Accelerate.

ÙŠÙØ³ØªØ®Ø¯Ù… Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù„ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ù†Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

```bash
accelerate config --config_file deepspeed_config.yaml
```

Ø³ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ø¥Ø¹Ø¯Ø§Ø¯ÙƒØŒ ÙˆØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„ØªØ§Ù„ÙŠØ©. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³ØªØ³ØªØ®Ø¯Ù… ZeRO-3 Ù„Ø°Ø§ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø®ØªÙŠØ§Ø± ØªÙ„Ùƒ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª.

```bash
`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning
`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them. Pass the same value as you would pass via cmd argument else you will encounter mismatch error.
`gradient_clipping`: Enable gradient clipping with value. Don't set this as you will be passing it via cmd arguments.
`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2. Set this as `none` as don't want to enable offloading.
`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3. Set this as `none` as don't want to enable offloading.
`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3. Set this to `True`.
`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3. Set this to `True`.
`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training. Set this to `True`.
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø°Ù„ÙƒØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ùˆ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ ÙƒÙ…Ø§ ÙŠÙ„ÙŠ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙÙŠ [deepspeed_config.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/deepspeed_config.yaml):

```yml
compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
    deepspeed_multinode_launcher: standard
    gradient_accumulation_steps: 4
    offload_optimizer_device: none
    offload_param_device: none
    zero3_init_flag: true
    zero3_save_16bit_model: true
    zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

## Ø£Ù…Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚

Ø£Ù…Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ù…ØªØ§Ø­ ÙÙŠ [run_peft_deepspeed.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_deepspeed.sh) ÙˆÙ‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡ Ø£ÙŠØ¶Ù‹Ø§:

```bash
accelerate launch --config_file "configs/deepspeed_config.yaml"  train.py \
--seed 100 \
--model_name_or_path "meta-llama/Llama-2-70b-hf" \
--dataset_name "smangrul/ultrachat-10k-chatml" \
--chat_template_format "chatml" \
--add_special_tokens False \
--append_concat_token False \
--splits "train,test" \
--max_seq_len 2048 \
--num_train_epochs 1 \
--logging_steps 5 \
--log_level "info" \
--logging_strategy "steps" \
--evaluation_strategy "epoch" \
--save_strategy "epoch" \
--push_to_hub \
--hub_private_repo True \
--hub_strategy "every_save" \
--bf16 True \
--packing True \
--learning_rate 1e-4 \
--lr_scheduler_type "cosine" \
--weight_decay 1e-4 \
--warmup_ratio 0.0 \
--max_grad_norm 1.0 \
--output_dir "llama-sft-lora-deepspeed" \
--per_device_train_batch_size 8 \
--per_device_eval_batch_size 8 \
--gradient_accumulation_steps 4 \
--gradient_checkpointing True \
--use_reentrant False \
--dataset_text_field "content" \
--use_flash_attn True \
--use_peft_lora True \
--lora_r 8 \
--lora_alpha 16 \
--lora_dropout 0.1 \
--lora_target_modules "all-linear" \
--use_4bit_quantization False
```

Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… LoRA Ù…Ø¹ Ø§Ù„Ø±ØªØ¨Ø©=8ØŒ alpha=16 ÙˆØ§Ø³ØªÙ‡Ø¯Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©. Ù†Ù‚ÙˆÙ… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ù„Ù ØªÙ‡ÙŠØ¦Ø© DeepSpeed ÙˆÙ†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù†Ù…ÙˆØ°Ø¬ Llama 70B Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ultrachat.

## Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø©

Ø¯Ø¹ÙˆÙ†Ø§ Ù†ØªØ¹Ù…Ù‚ Ù‚Ù„ÙŠÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ø±Ø¤ÙŠØ© Ù…Ø§ ÙŠØ­Ø¯Ø«ØŒ ÙˆÙÙ‡Ù… ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„Ù‡.

Ø£ÙˆÙ„ Ø´ÙŠØ¡ ÙŠØ¬Ø¨ Ù…Ø¹Ø±ÙØªÙ‡ Ù‡Ùˆ Ø£Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ ÙŠØ³ØªØ®Ø¯Ù… DeepSpeed Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø­ÙŠØ« ØªÙ… ØªÙ…Ø±ÙŠØ± ØªÙ‡ÙŠØ¦Ø© DeepSpeed. ØªØªÙˆÙ„Ù‰ ÙØ¦Ø© `SFTTrainer` Ø±ÙØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø«Ù‚Ø§Ù„ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ PEFT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‡ÙŠØ¦Ø© PEFT Ø§Ù„ØªÙŠ ØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø¹Ù†Ø¯Ù…Ø§ ØªØ³ØªØ¯Ø¹ÙŠ `trainer.train()`ØŒ ÙŠØ³ØªØ®Ø¯Ù… `SFTTrainer` Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§ ğŸ¤— Accelerate Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­Ø³Ù† ÙˆØ§Ù„Ù…Ø¯Ø±Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‡ÙŠØ¦Ø© DeepSpeed Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ DeepSpeed Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ. Ù…Ù‚ØªØ·Ù Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù‡Ùˆ Ø£Ø¯Ù†Ø§Ù‡:

```python
# trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    packing=data_args.packing,
    dataset_kwargs={
        "append_concat_token": data_args.append_concat_token,
        "add_special_tokens": data_args.add_special_tokens,
    },
    dataset_text_field=data_args.dataset_text_field,
    max_seq_length=data_args.max_seq_length,
)
trainer.accelerator.print(f"{trainer.model}")

# train
checkpoint = None
if training_args.resume_from_checkpoint is not None:
    checkpoint = training_args.resume_from_checkpoint
trainer.train(resume_from_checkpoint=checkpoint)

# saving final model
trainer.save_model()
```

## Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©

ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ ØªØ¨Ù„Øº Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙŠ ØªØ³ØªÙ‡Ù„ÙƒÙ‡Ø§ ÙƒÙ„ ÙˆØ­Ø¯Ø© GPU 64 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª (80Ùª) ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ ÙÙŠ Ù„Ù‚Ø·Ø© Ø§Ù„Ø´Ø§Ø´Ø© Ø£Ø¯Ù†Ø§Ù‡:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_deepspeed_mem_usage.png"/>
</div>
<small>Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© GPU Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨</small>

## Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø±Ø¯

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„ØªØ¯ÙˆÙŠÙ†Ø© [Falcon 180B Finetuning using ğŸ¤— PEFT and DeepSpeed](https://medium.com/@sourabmangrulkar/falcon-180b-finetuning-using-peft-and-deepspeed-b92643091d99) Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù†Ù…ÙˆØ°Ø¬ Falcon 180B Ø¹Ù„Ù‰ 16 GPU A100 Ø¹Ù„Ù‰ Ø¢Ù„ØªÙŠÙ†.

# Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT QLoRA Ùˆ DeepSpeed Ù…Ø¹ ZeRO3 Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª GPU Ù…ØªØ¹Ø¯Ø¯Ø©

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…ØŒ Ø³Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… QLoRA Ùˆ DeepSpeed Stage-3 Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù†Ù…ÙˆØ°Ø¬ Llama 70B Ø¹Ù„Ù‰ 2X40GB GPU.

Ù„Ù‡Ø°Ø§ØŒ Ù†Ø­ØªØ§Ø¬ Ø£ÙˆÙ„Ø§Ù‹ Ø¥Ù„Ù‰ `bitsandbytes>=0.43.0`ØŒ `accelerate>=0.28.0`ØŒ `transformers>4.38.2`ØŒ `trl>0.7.11` Ùˆ `peft>0.9.0`. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¹ÙŠÙŠÙ† `zero3_init_flag` Ø¥Ù„Ù‰ true Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‡ÙŠØ¦Ø© Accelerate. ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡ ÙÙŠ [deepspeed_config_z3_qlora.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/deepspeed_config_z3_qlora.yaml):

```yml
compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
    deepspeed_multinode_launcher: standard
    offload_optimizer_device: none
    offload_param_device: none
    zero3_init_flag: true
    zero3_save_16bit_model: true
    zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

Ø£Ù…Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡ ÙˆÙ‡Ùˆ Ù…ØªØ§Ø­ ÙÙŠ [run_peft_qlora_deepspeed_stage3.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_deepspeed.sh):

```
accelerate launch --config_file "configs/deepspeed_config_z3_qlora.yaml"  train.py \
--seed 100 \
--model_name_or_path "meta-llama/Llama-2-70b-hf" \
--dataset_name "smangrul/ultrachat-10k-chatml" \
--chat_template_format "chatml" \
--add_special_tokens False \
--append_concat_token False \
--splits "train,test" \
--max_seq_len 2048 \
--num_train_epochs 1 \
--logging_steps 5 \
--log_level "info" \
--logging_strategy "steps" \
--evaluation_strategy "epoch" \
--save_strategy "epoch" \
--push_to_hub \
--hub_private_repo True \
--hub_strategy "every_save" \
--bf16 True \
--packing True \
--learning_rate 1e-4 \
--lr_scheduler_type "cosine" \
--weight_decay 1e-4 \
--warmup_ratio 0.0 \
--max_grad_norm 1.0 \
--output_dir "llama-sft-qlora-dsz3" \
--per_device_train_batch_size 2 \
--per_device_eval_batch_size 2 \
--gradient_accumulation_steps 2 \
--gradient_checkpointing True \
--use_reentrant True \
--dataset_text_field "content" \
--use_flash_attn True \
--use_peft_lora True \
--lora_r 8 \
--lora_alpha 16 \
--lora_dropout 0.1 \
--lora_target_modules "all-linear" \
--use_4bit_quantization True \
--use_nested_quant True \
--bnb_4bit_compute_dtype "bfloat16" \
--bnb_4bit_quant_storage_dtype "bfloat16"
```

Ù„Ø§Ø­Ø¸ Ø§Ù„Ø­Ø¬Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§ `bnb_4bit_quant_storage_dtype` ÙˆØ§Ù„ØªÙŠ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª 4-Ø¨Øª. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡ Ø¹Ù„Ù‰ `bfloat16`ØŒ ÙŠØªÙ… ØªØ¹Ø¨Ø¦Ø© **8** Ù…Ø¹Ù„Ù…Ø§Øª 4 Ø¨Øª Ù…Ø¹Ù‹Ø§ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙƒÙ…ÙŠÙ….

Ù…Ù† Ø­ÙŠØ« ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ØªØªÙ…Ø«Ù„ ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ù…Ø§ ÙŠÙ„ÙŠ:

```diff
....

bnb_config = BitsAndBytesConfig(
    load_in_4bit=args.use_4bit_quantization,
    bnb_4bit_quant_type=args.bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=args.use_nested_quant,
+   bnb_4bit_quant_storage=quant_storage_dtype,
)

...

model = AutoModelForCausalLM.from_pretrained(
    args.model_name_or_path,
    quantization_config=bnb_config,
    trust_remote_code=True,
    attn_implementation="flash_attention_2" if args.use_flash_attn else "eager",
+   torch_dtype=quant_storage_dtype or torch.float32,
)

```

Notice that `torch_dtype` for `AutoModelForCausalLM` is same as the `bnb_4bit_quant_storage` data type. That's it. Everything else is handled by Trainer and TRL.

## Memory usage

In the above example, the memory consumed per GPU is **36.6 GB**. Therefore, what took 8X80GB GPUs with DeepSpeed Stage 3+LoRA and a couple of 80GB GPUs with DDP+QLoRA now requires 2X40GB GPUs. This makes finetuning of large models more accessible.

# Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT Ùˆ DeepSpeed Ù…Ø¹ ZeRO3 Ùˆ CPU Offloading Ù„Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³ÙˆÙ…ÙŠØ© ÙˆØ§Ø­Ø¯Ø©

Ø³ÙŠØ³Ø§Ø¹Ø¯Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ù…Ù† Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ØµÙ†Ø§ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ DeepSpeed. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¶Ø¨Ø· Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ÙƒØ¨ÙŠØ± Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø´Ø±Ø·ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ZeRO-3 Ùˆ CPU Offload.

<Tip>

ğŸ’¡ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ØµÙˆØµÙ†Ø§ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù„Ù…Ø«Ø§Ù„ [Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py) Ùˆ [Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø´Ø±Ø·ÙŠ](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). ÙŠÙ…ÙƒÙ†Ùƒ ØªÙƒÙŠÙŠÙ Ù‡Ø°Ù‡ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚Ø§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ© Ø£Ùˆ Ø­ØªÙ‰ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ù‡Ù…ØªÙƒ Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„ØªÙ„Ùƒ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©.

</Tip>

## Ø§Ù„ØªÙ‡ÙŠØ¦Ø©

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙ‡ÙŠØ¦Ø© DeepSpeed Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Accelerate. ÙŠØ³Ù…Ø­ Ù„Ùƒ Ø¹Ù„Ù… `--config_file` Ø¨Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙÙŠ Ù…ÙˆÙ‚Ø¹ Ù…Ø­Ø¯Ø¯ØŒ ÙˆØ¥Ù„Ø§ ÙØ³ÙŠØªÙ… Ø­ÙØ¸Ù‡ ÙƒÙ…Ù„Ù `default_config.yaml` ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ù„Ù€ ğŸ¤— Accelerate.

ÙŠÙØ³ØªØ®Ø¯Ù… Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù„ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ù†Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

```bash
accelerate config --config_file ds_zero3_cpu.yaml
```

Ø³ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ø¥Ø¹Ø¯Ø§Ø¯ÙƒØŒ ÙˆØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„ØªØ§Ù„ÙŠØ©. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³ØªØ³ØªØ®Ø¯Ù… ZeRO-3 Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ CPU-OffloadØŒ Ù„Ø°Ø§ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø®ØªÙŠØ§Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª.

```bash
`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning
`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.
`gradient_clipping`: Enable gradient clipping with value.
`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.
`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.
`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.
`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.
`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training.
```

Ù‚Ø¯ ÙŠØ¨Ø¯Ùˆ Ù…Ù„Ù [Ø§Ù„ØªÙ‡ÙŠØ¦Ø©](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/accelerate_ds_zero3_cpu_offload_config.yaml) Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„ØªØ§Ù„ÙŠ. Ø£Ù‡Ù… Ù…Ø§ ÙŠØ¬Ø¨ Ù…Ù„Ø§Ø­Ø¸ØªÙ‡ Ù‡Ùˆ Ø£Ù† `zero_stage` ØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡ Ø¥Ù„Ù‰ `3`ØŒ Ùˆ `offload_optimizer_device` Ùˆ `offload_param_device` ØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡Ù…Ø§ Ø¥Ù„Ù‰ `cpu`.

```yml
compute_environment: LOCAL_MACHINE
deepspeed_config:
    gradient_accumulation_steps: 1
    gradient_clipping: 1.0
    offload_optimizer_device: cpu
    offload_param_device: cpu
    zero3_init_flag: true
    zero3_save_16bit_model: true
    zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
dynamo_backend: 'NO'
fsdp_config: {}
machine_rank: 0
main_training_function: main
megatron_lm_config: {}
mixed_precision: 'no'
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
use_cpu: false
```

## Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø©

Ø¯Ø¹ÙˆÙ†Ø§ Ù†ØªØ¹Ù…Ù‚ Ù‚Ù„ÙŠÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ø±Ø¤ÙŠØ© Ù…Ø§ ÙŠØ­Ø¯Ø«ØŒ ÙˆÙÙ‡Ù… ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„Ù‡.

Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø§Ù„Ø© [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103)ØŒ ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙØ¦Ø© [`~accelerate.Accelerator`] Ù„ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹.

<Tip>

ğŸ’¡ Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ ØªØºÙŠÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¯Ø§Ø®Ù„ Ø¯Ø§Ù„Ø© `main`. Ø¥Ø°Ø§ ÙƒØ§Ù† ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø®ØªÙ„ÙÙ‹Ø§ Ø¹Ù† Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØŒ ÙÙ‚Ø¯ ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ÙƒØªØ§Ø¨Ø© Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.

</Tip>

ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù„Ø·Ø±ÙŠÙ‚Ø© ğŸ¤— PEFT Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ØŒ ÙˆØ§Ù„ØªÙŠ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ Ù‡ÙŠ LoRA. ØªØ­Ø¯Ø¯ [`LoraConfig`] Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ø«Ù„ Ø¨ÙØ¹Ø¯ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª Ø°Ø§Øª Ø§Ù„Ø±ØªØ¨Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©ØŒ ÙˆØ¹Ø§Ù…Ù„ Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ØµÙÙˆÙØ§ØªØŒ ÙˆØ§Ø­ØªÙ…Ø§Ù„ Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø·Ø¨Ù‚Ø§Øª LoRA. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© ğŸ¤— PEFT Ù…Ø®ØªÙ„ÙØ©ØŒ ÙØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ¨Ø¯Ø§Ù„ `LoraConfig` Ø¨Ø§Ù„ÙØµÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ [class](../package_reference/tuners).

```diff
 def main():
+    accelerator = Accelerator()
     model_name_or_path = "facebook/bart-large"
     dataset_name = "twitter_complaints"
+    peft_config = LoraConfig(
         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
     )
```

ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØŒ Ø³ØªØ±Ù‰ ÙˆØ¸Ø§Ø¦Ù [`~accelerate.Accelerator.main_process_first`] Ùˆ [`~accelerate.Accelerator.wait_for_everyone`] Ø§Ù„ØªÙŠ ØªØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© ÙˆØªÙ†ÙÙŠØ°Ù‡Ø§.

ØªØ£Ø®Ø° ÙˆØ¸ÙŠÙØ© [`get_peft_model`] Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ø£Ø³Ø§Ø³ÙŠÙ‹Ø§ Ùˆ [`peft_config`] Ø§Ù„Ø°ÙŠ Ù‚Ù…Øª Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø¨Ù‚Ù‹Ø§ Ù„Ø¥Ù†Ø´Ø§Ø¡ [`PeftModel`]:

```diff
model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+ model = get_peft_model(model, peft_config)
```

Ù…Ø±Ø± Ø¬Ù…ÙŠØ¹ ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¥Ù„Ù‰ ÙˆØ¸ÙŠÙØ© [`~accelerate.Accelerator.prepare`] Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ ğŸ¤— AccelerateØŒ ÙˆØ§Ù„ØªÙŠ ØªØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ¯Ø±ÙŠØ¨:

```py
model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(
    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler
)
```

ÙŠÙØ­Øµ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙƒÙˆÙ† Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ DeepSpeed Ù…Ø³ØªØ®Ø¯Ù…Ù‹Ø§ ÙÙŠ `Accelerator`ØŒ ÙˆØ¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙƒÙˆÙ† Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ØŒ ÙØ¥Ù†Ù†Ø§ Ù†ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒÙ†Ø§ Ù†Ø³ØªØ®Ø¯Ù… ZeRO-3. ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù„Ù… Ø§Ù„Ø´Ø±Ø·ÙŠ Ø¹Ù†Ø¯ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ÙˆØ¸ÙŠÙØ© `generate` Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù„Ù…Ø²Ø§Ù…Ù†Ø© ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ø²Ø£Ø©:

```py
is_ds_zero_3 = False
if getattr(accelerator.state, "deepspeed_plugin", None):
    is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3
```

Ø¯Ø§Ø®Ù„ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ `loss.backward()` Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø© Ø¨Ù€ [`~accelerate.Accelerator.backward`] Ù…Ù† ğŸ¤— AccelerateØŒ ÙˆØ§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© `backward()` Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªÙ‡ÙŠØ¦ØªÙƒ:

```diff
  for epoch in range(num_epochs):
      with TorchTracemalloc() as tracemalloc:
          model.train()
          total_loss = 0
          for step, batch in enumerate(tqdm(train_dataloader)):
              outputs = model(**batch)
              loss = outputs.loss
              total_loss += loss.detach().float()
+             accelerator.backward(loss)
              optimizer.step()
              lr_scheduler.step()
              optimizer.zero_grad()
```

Ù‡Ø°Ø§ ÙƒÙ„ Ø´ÙŠØ¡! ØªØªØ¹Ø§Ù…Ù„ Ø¨Ù‚ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ø¹ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…ØŒ Ø¨Ù„ ÙˆØªØ¯ÙØ¹Ù‡Ø§ Ø¥Ù„Ù‰ Hub Ù…Ù† Ø£Ø¬Ù„Ùƒ.

## Ø§Ù„ØªØ¯Ø±ÙŠØ¨

Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¨Ø¯Ø¡ Ù†Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨. ÙÙŠ ÙˆÙ‚Øª Ø³Ø§Ø¨Ù‚ØŒ Ù‚Ù…Øª Ø¨Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø¥Ù„Ù‰ `ds_zero3_cpu.yaml`ØŒ Ù„Ø°Ø§ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ launcher Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ³ÙŠØ· `--config_file` Ù…Ø«Ù„ Ù‡Ø°Ø§:

```bash
accelerate launch --config_file ds_zero3_cpu.yaml examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py
```

Ø³ØªØ±Ù‰ Ø¨Ø¹Ø¶ Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØªÙŠ ØªØªØªØ¨Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙˆØ¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„Ù‡ØŒ ÙŠØ¹ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø§Ù„Ø¯Ù‚Ø© ÙˆÙŠÙ‚Ø§Ø±Ù† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø¨Ø§Ù„Ù…Ù„ØµÙ‚Ø§Øª:

```bash
GPU Memory before entering the train : 1916
GPU Memory consumed at the end of the train (end-begin): 66
GPU Peak Memory consumed during the train (max-begin): 7488
GPU Total Peak Memory consumed during the train (max): 9404
CPU Memory before entering the train : 19411
CPU Memory consumed at the end of the train (end-begin): 0
CPU Peak Memory consumed during the train (max-begin): 0
CPU Total Peak Memory consumed during the train (max): 19411
epoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27<00:00,  3.92s/it]
GPU Memory before entering the eval : 1982
GPU Memory consumed at the end of the eval (end-begin): -66
GPU Peak Memory consumed during the eval (max-begin): 672
GPU Total Peak Memory consumed during the eval (max): 2654
CPU Memory before entering the eval : 19411
CPU Memory consumed at the end of the eval (end-begin): 0
CPU Peak Memory consumed during the eval (max-begin): 0
CPU Total Peak Memory consumed during the eval (max): 19411
accuracy=100.0
eval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
dataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
```

# Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª

1. Ø§Ù„Ø¯Ù…Ø¬ Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT Ùˆ DeepSpeed ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ø­Ø§Ù„ÙŠÙ‹Ø§ ÙˆØ³ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø®Ø·Ø£.
2. Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙØ±ÙŠØº Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU offloading)ØŒ Ø³ØªØªØ­Ù‚Ù‚ Ø§Ù„Ù…ÙƒØ§Ø³Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… PEFT Ù„ØªØµØºÙŠØ± Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù† ÙˆØ§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ù„ØªÙƒÙˆÙ† Ù…Ø«Ù„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙˆÙ„ ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ (RAM) Ù„Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU)ØŒ ÙˆÙ„Ù† ØªÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ ÙˆÙÙˆØ±Ø§Øª ÙÙŠÙ…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø°Ø§ÙƒØ±Ø© ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU).
3. ÙŠØ¤Ø¯ÙŠ DeepSpeed Stage 3 Ùˆ qlora Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ù…Ø§ Ù…Ø¹ ØªÙØ±ÙŠØº ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ø¹Ù†Ø¯ Ù…Ù‚Ø§Ø±Ù†ØªÙ‡ Ø¨ØªØ¹Ø·ÙŠÙ„ ØªÙØ±ÙŠØº ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ©.