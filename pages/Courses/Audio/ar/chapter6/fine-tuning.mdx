# ضبط نموذج SpeechT5

الآن بعد أن أصبحت على دراية بمهمة تحويل النص إلى كلام والأجزاء الداخلية لنموذج SpeechT5 الذي تم تدريبه مسبقًا على بيانات اللغة الإنجليزية، دعنا نرى كيف يمكننا ضبطه بدقة للعمل مع لغة أخرى.

## الإعدادات الأولية
تأكد من أن لديك وحدة معالجة الرسوميات (GPU) إذا كنت ترغب في إعادة إنتاج هذا المثال. في دفتر الملاحظات، يمكنك التحقق من ذلك باستخدام الأمر التالي:

```bash
nvidia-smi
```

<Tip warning={true}>

في مثالنا، سنستخدم حوالي 40 ساعة من بيانات التدريب. إذا كنت ترغب في المتابعة باستخدام وحدة معالجة الرسوميات (GPU) من المستوى المجاني في Google Colab، فستحتاج إلى تقليل كمية بيانات التدريب إلى حوالي 10-15 ساعة، وتقليل عدد خطوات التدريب.

</Tip>

ستحتاج أيضًا إلى بعض الاعتمادات الإضافية:

```bash
pip install transformers datasets soundfile speechbrain accelerate
```

أخيرًا، لا تنس تسجيل الدخول إلى حسابك على Hugging Face حتى تتمكن من تحميل ومشاركة نموذجك مع المجتمع:

```py
from huggingface_hub import notebook_login

notebook_login()
```

## مجموعة البيانات

في هذا المثال، سنأخذ المجموعة الفرعية للغة الهولندية (`nl`) من مجموعة بيانات [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli).
[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) هي مجموعة بيانات متعددة اللغات واسعة النطاق تتكون من بيانات مسجلة من أحداث البرلمان الأوروبي في الفترة من 2009 إلى 2020. تحتوي على بيانات صوتية-نصية معنونة لـ 15 لغة أوروبية. على الرغم من أننا سنستخدم المجموعة الفرعية للغة الهولندية، يمكنك اختيار مجموعة فرعية أخرى.

هذه هي مجموعة بيانات التعرف التلقائي على الكلام (ASR)، لذا، كما ذكرنا سابقًا، فهي ليست الخيار الأنسب لتدريب نماذج تحويل النص إلى كلام (TTS). ومع ذلك، ستكون جيدة بما يكفي لهذا التمرين.

دعنا نحمل البيانات:

```python
from datasets import load_dataset, Audio

dataset = load_dataset("facebook/voxpopuli", "nl", split="train")
len(dataset)
```

**الإخراج:**
```out
20968
```

20968 مثال يجب أن تكون كافية لضبط النموذج. يتوقع SpeechT5 أن يكون للبيانات الصوتية معدل أخذ عينات يبلغ 16 كيلو هرتز، لذا تأكد من أن الأمثلة في مجموعة البيانات تلبي هذا الشرط:

```python
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

## معالجة البيانات مسبقًا

دعنا نبدأ بتحديد نقطة تفتيش النموذج التي سنستخدمها وتحميل المعالج المناسب الذي يحتوي على كل من المعالج اللغوي (tokenizer) ومستخرج الميزات الذي سنحتاج إليه لإعداد البيانات للتدريب:

```py
from transformers import SpeechT5Processor

checkpoint = "microsoft/speecht5_tts"
processor = SpeechT5Processor.from_pretrained(checkpoint)
```

### تنظيف النص لتقسيم SpeechT5 إلى رموز

أولاً، بالنسبة لإعداد النص، سنحتاج إلى الجزء المعالج اللغوي (tokenizer) من المعالج، لذا دعنا نحصل عليه:

```py
tokenizer = processor.tokenizer
```

دعنا نلقي نظرة على مثال:

```python
dataset[0]
```

**الإخراج:**
```out
{'audio_id': '20100210-0900-PLENARY-3-nl_20100210-09:06:43_4',
 'language': 9,
 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/02ec6a19d5b97c03e1379250378454dbf3fa2972943504a91c7da5045aa26a89/train_part_0/20100210-0900-PLENARY-3-nl_20100210-09:06:43_4.wav',
  'array': array([ 4.27246094e-04,  1.31225586e-03,  1.03759766e-03, ...,
         -9.15527344e-05,  7.62939453e-04, -2.44140625e-04]),
  'sampling_rate': 16000},
 'raw_text': 'Dat kan naar mijn gevoel alleen met een brede meerderheid die wij samen zoeken.',
 'normalized_text': 'dat kan naar mijn gevoel alleen met een brede meerderheid die wij samen zoeken.',
 'gender': 'female',
 'speaker_id': '1122',
 'is_gold_transcript': True,
 'accent': 'None'}
```

ما قد تلاحظه هو أن أمثلة مجموعة البيانات تحتوي على ميزتي `raw_text` و`normalized_text`. عند تحديد الميزة التي ستستخدم كمدخل نصي، سيكون من المهم معرفة أن المعالج اللغوي (tokenizer) في SpeechT5 لا يحتوي على أي رموز للأرقام. في `normalized_text`، يتم كتابة الأرقام كنص. وبالتالي، فهي مناسبة أكثر، ويجب أن نستخدم `normalized_text` كنص إدخال.

بما أن SpeechT5 تم تدريبه على اللغة الإنجليزية، فقد لا يتعرف على بعض الأحرف في مجموعة البيانات الهولندية. إذا تُركت كما هي، فسيتم تحويل هذه الأحرف إلى رموز `<unk>`. ومع ذلك، في اللغة الهولندية، تُستخدم أحرف معينة مثل `à` للتشديد على المقاطع. من أجل الحفاظ على معنى النص، يمكننا استبدال هذا الحرف بحرف `a` عادي.

لتحديد الرموز غير المدعومة، قم باستخراج جميع الأحرف الفريدة في مجموعة البيانات باستخدام `SpeechT5Tokenizer` الذي يعمل مع الأحرف كرموز. للقيام بذلك، سنكتب دالة `extract_all_chars` التي تقوم بدمج النصوص من جميع الأمثلة في سلسلة واحدة وتحويلها إلى مجموعة من الأحرف. تأكد من تعيين `batched=True` و`batch_size=-1` في `dataset.map()` بحيث تكون جميع النصوص متاحة مرة واحدة للدالة الخرائطية.

```py
def extract_all_chars(batch):
    all_text = " ".join(batch["normalized_text"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}


vocabs = dataset.map(
    extract_all_chars,
    batched=True,
    batch_size=-1,
    keep_in_memory=True,
    remove_columns=dataset.column_names,
)

dataset_vocab = set(vocabs["vocab"][0])
tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}
```

الآن لديك مجموعتان من الأحرف: واحدة مع المفردات من مجموعة البيانات والأخرى مع المفردات من المعالج اللغوي. لتحديد أي أحرف غير مدعومة في مجموعة البيانات، يمكنك أخذ الفرق بين هاتين المجموعتين. المجموعة الناتجة ستتضمن الأحرف الموجودة في مجموعة البيانات ولكنها غير موجودة في المعالج اللغوي.

```py
dataset_vocab - tokenizer_vocab
```

**الإخراج:**
```out
{' ', 'à', 'ç', 'è', 'ë', 'í', 'ï', 'ö', 'ü'}
```

للتعامل مع الأحرف غير المدعومة التي تم تحديدها في الخطوة السابقة، يمكننا تحديد دالة تقوم بتعيين هذه الأحرف إلى رموز صالحة. لاحظ أن المسافات يتم استبدالها بالفعل بـ `▁` في المعالج اللغوي ولا تحتاج إلى معالجة منفصلة.

```py
replacements = [
    ("à", "a"),
    ("ç", "c"),
    ("è", "e"),
    ("ë", "e"),
    ("í", "i"),
    ("ï", "i"),
    ("ö", "o"),
    ("ü", "u"),
]


def cleanup_text(inputs):
    for src, dst in replacements:
        inputs["normalized_text"] = inputs["normalized_text"].replace(src, dst)
    return inputs


dataset = dataset.map(cleanup_text)
```

الآن بعد أن تعاملنا مع الأحرف الخاصة في النص، حان الوقت لتحويل التركيز إلى البيانات الصوتية.

### المتحدثون

تتضمن مجموعة بيانات VoxPopuli كلامًا من متحدثين متعددين، ولكن كم عدد المتحدثين الممثلين في مجموعة البيانات؟ لتحديد ذلك، يمكننا حساب عدد المتحدثين الفريدين وعدد الأمثلة التي يساهم بها كل متحدث في مجموعة البيانات. مع وجود ما مجموعه 20,968 مثال في مجموعة البيانات، ستمنحنا هذه المعلومات فهمًا أفضل لتوزيع المتحدثين والأمثلة في البيانات.

```py
from collections import defaultdict

speaker_counts = defaultdict(int)

for speaker_id in dataset["speaker_id"]:
    speaker_counts[speaker_id] += 1
```

من خلال رسم مخطط تكراري، يمكنك الحصول على فكرة عن كمية البيانات المتوفرة لكل متحدث.

```py
import matplotlib.pyplot as plt

plt.figure()
plt.hist(speaker_counts.values(), bins=20)
plt.ylabel("Speakers")
plt.xlabel("Examples")
plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_speakers_histogram.png" alt="Speakers histogram"/>
</div>

يكشف المخطط التكراري أن حوالي ثلث المتحدثين في مجموعة البيانات لديهم أقل من 100 مثال، في حين أن حوالي عشرة متحدثين لديهم أكثر من 500 مثال. لتحسين كفاءة التدريب وتوازن مجموعة البيانات، يمكننا تحديد البيانات للمتحدثين الذين لديهم ما بين 100 و400 مثال.

```py
def select_speaker(speaker_id):
    return 100 <= speaker_counts[speaker_id] <= 400


dataset = dataset.filter(select_speaker, input_columns=["speaker_id"])
```

دعنا نتحقق من عدد المتحدثين المتبقين:

```py
len(set(dataset["speaker_id"]))
```

**الإخراج:**
```out
42
```

دعنا نرى كم عدد الأمثلة المتبقية:

```py
len(dataset)
```

**الإخراج:**
```out
9973
دعنا نرى كم عدد الأمثلة المتبقية:

```py
len(dataset)
```

**الناتج:**
```out
9973
```

تبقى لديك أقل بقليل من 10,000 مثال من حوالي 40 متحدث فريد، وهو ما ينبغي أن يكون كافياً.

لاحظ أن بعض المتحدثين الذين لديهم أمثلة قليلة قد يكون لديهم في الواقع المزيد من الصوت المتاح إذا كانت الأمثلة طويلة. ومع ذلك،
تحديد إجمالي كمية الصوت لكل متحدث يتطلب فحصًا عبر مجموعة البيانات بأكملها، وهي
عملية تستغرق وقتًا طويلاً تتضمن تحميل وفك تشفير كل ملف صوتي. ولهذا، اخترنا تخطي هذه الخطوة هنا.

### تضمين المتحدث

لتمكين نموذج TTS من التمييز بين عدة متحدثين، ستحتاج إلى إنشاء تضمين متحدث لكل مثال.
تضمين المتحدث هو إدخال إضافي في النموذج الذي يلتقط خصائص صوت متحدث معين.
لإنشاء هذه تضمينات المتحدث، استخدم النموذج المدرب مسبقًا [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)
النموذج من SpeechBrain.

قم بإنشاء دالة `create_speaker_embedding()` التي تأخذ موجة صوت إدخال وتخرج متجهًا مكونًا من 512 عنصرًا
يحتوي على تضمين المتحدث المقابل.

```py
import os
import torch
from speechbrain.pretrained import EncoderClassifier

spk_model_name = "speechbrain/spkrec-xvect-voxceleb"

device = "cuda" if torch.cuda.is_available() else "cpu"
speaker_model = EncoderClassifier.from_hparams(
    source=spk_model_name,
    run_opts={"device": device},
    savedir=os.path.join("/tmp", spk_model_name),
)


def create_speaker_embedding(waveform):
    with torch.no_grad():
        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))
        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)
        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()
    return speaker_embeddings
```

من المهم ملاحظة أن النموذج `speechbrain/spkrec-xvect-voxceleb` تم تدريبه على الكلام الإنجليزي من مجموعة بيانات VoxCeleb
في حين أن الأمثلة التدريبية في هذا الدليل هي باللغة الهولندية. على الرغم من أننا نعتقد أن هذا النموذج سيظل يولد
تضمينات المتحدث معقولة لمجموعة بياناتنا الهولندية، فقد لا يكون هذا الافتراض صحيحًا في جميع الحالات.

للحصول على نتائج مثلى، سنحتاج إلى تدريب نموذج X-vector على الكلام المستهدف أولاً. هذا سيضمن أن النموذج
قادر بشكل أفضل على التقاط خصائص الصوت الفريدة الموجودة في اللغة الهولندية. إذا كنت ترغب في تدريب
نموذج X-vector الخاص بك، يمكنك استخدام [هذا النص البرمجي](https://huggingface.co/mechanicalsea/speecht5-vc/blob/main/manifest/utils/prep_cmu_arctic_spkemb.py)
كمثال.

### معالجة مجموعة البيانات

أخيرًا، دعنا نعالج البيانات إلى التنسيق الذي يتوقعه النموذج. قم بإنشاء دالة `prepare_dataset` التي تأخذ
مثال واحد ويستخدم كائن `SpeechT5Processor` لرموز نص الإدخال وتحميل الصوت المستهدف إلى مخطط طيفي لوغاريتمي.
كما يجب أن تضيف تضمينات المتحدث كإدخال إضافي.

```py
def prepare_dataset(example):
    audio = example["audio"]

    example = processor(
        text=example["normalized_text"],
        audio_target=audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_attention_mask=False,
    )

    # إزالة البعد الدفعي
    example["labels"] = example["labels"][0]

    # استخدم SpeechBrain للحصول على x-vector
    example["speaker_embeddings"] = create_speaker_embedding(audio["array"])

    return example
```

تحقق من صحة المعالجة من خلال النظر إلى مثال واحد:

```py
processed_example = prepare_dataset(dataset[0])
list(processed_example.keys())
```

**الناتج:**
```out
['input_ids', 'labels', 'stop_labels', 'speaker_embeddings']
```

يجب أن يكون تضمين المتحدث متجهًا مكونًا من 512 عنصرًا:

```py
processed_example["speaker_embeddings"].shape
```

**الناتج:**
```out
(512,)
```

يجب أن تكون العلامات عبارة عن مخطط طيفي لوغاريتمي مع 80 علبة ميل.

```py
import matplotlib.pyplot as plt

plt.figure()
plt.imshow(processed_example["labels"].T)
plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_logmelspectrogram_1.png" alt="مخطط طيفي لوغاريتمي مع 80 علبة ميل"/>
</div>

ملاحظة جانبية: إذا وجدت هذا المخطط الطيفي مربكًا، فقد يكون ذلك بسبب إلمامك باتفاقية وضع الترددات المنخفضة
في الأسفل والترددات العالية في أعلى المخطط. ومع ذلك، عند رسم المخططات الطيفية كصورة باستخدام مكتبة matplotlib،
يتم قلب محور الصادات وتظهر المخططات الطيفية مقلوبة.

الآن نحتاج إلى تطبيق دالة المعالجة على مجموعة البيانات بأكملها. سيستغرق هذا ما بين 5 و10 دقائق.

```py
dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)
```

سترى تحذيرًا يقول إن بعض الأمثلة في مجموعة البيانات أطول من طول الإدخال الأقصى الذي يمكن للنموذج التعامل معه (600 رمز).
قم بإزالة تلك الأمثلة من مجموعة البيانات. هنا نذهب إلى أبعد من ذلك وللسماح بأحجام دفعات أكبر، نقوم بإزالة أي شيء يزيد عن 200 رمز.

```py
def is_not_too_long(input_ids):
    input_length = len(input_ids)
    return input_length < 200


dataset = dataset.filter(is_not_too_long, input_columns=["input_ids"])
len(dataset)
```

**الناتج:**
```out
8259
```

بعد ذلك، قم بإنشاء تقسيم تدريبي/اختباري أساسي:

```py
dataset = dataset.train_test_split(test_size=0.1)
```

### جامع البيانات

من أجل دمج أمثلة متعددة في دفعة، تحتاج إلى تحديد جامع بيانات مخصص. سيقوم جامع البيانات هذا بتعبئة التسلسلات الأقصر برموز التعبئة
، مما يضمن أن يكون لجميع الأمثلة نفس الطول. بالنسبة لعلامات المخطط الطيفي، يتم استبدال الأجزاء المعبأة
بالقيمة الخاصة `-100`. هذه القيمة الخاصة توعز للنموذج بتجاهل هذا الجزء من المخطط الطيفي عند حساب خسارة المخطط الطيفي.

```py
from dataclasses import dataclass
from typing import Any, Dict, List, Union


@dataclass
class TTSDataCollatorWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_ids = [{"input_ids": feature["input_ids"]} for feature in features]
        label_features = [{"input_values": feature["labels"]} for feature in features]
        speaker_features = [feature["speaker_embeddings"] for feature in features]

        # تجميع الإدخالات والأهداف في دفعة
        batch = processor.pad(
            input_ids=input_ids, labels=label_features, return_tensors="pt"
        )

        # استبدال التعبئة بـ -100 لتجاهل الخسارة بشكل صحيح
        batch["labels"] = batch["labels"].masked_fill(
            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100
        )

        # غير مستخدم أثناء الضبط الدقيق
        del batch["decoder_attention_mask"]

        # تقريب أطوال الهدف لأسفل إلى مضاعف عامل التخفيض
        if model.config.reduction_factor > 1:
            target_lengths = torch.tensor(
                [len(feature["input_values"]) for feature in label_features]
            )
            target_lengths = target_lengths.new(
                [
                    length - length % model.config.reduction_factor
                    for length in target_lengths
                ]
            )
            max_length = max(target_lengths)
            batch["labels"] = batch["labels"][:, :max_length]

        # أضف أيضًا تضمينات المتحدث
        batch["speaker_embeddings"] = torch.tensor(speaker_features)

        return batch
```

في SpeechT5، يتم تقليل الإدخال في الجزء فك الترميز من النموذج بعامل 2. وبعبارة أخرى، فإنه يتخلص من كل
خطوة زمنية أخرى من التسلسل المستهدف. بعد ذلك، يتنبأ فك الترميز بتسلسل يكون ضعف الطول. نظرًا لأن طول التسلسل المستهدف الأصلي قد يكون فرديًا،
يتأكد جامع البيانات من تقريب الطول الأقصى للدفعة لأسفل ليكون مضاعفًا لـ 2.

```py
data_collator = TTSDataCollatorWithPadding(processor=processor)
```

## تدريب النموذج

قم بتحميل النموذج المدرب مسبقًا من نفس نقطة التفتيش التي استخدمتها لتحميل المعالج:

```py
from transformers import SpeechT5ForTextToSpeech

model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)
```
خيار `use_cache=True` غير متوافق مع حفظ نقاط التدرج. قم بتعطيله أثناء التدريب، وأعد تفعيل الذاكرة المؤقتة للجيل لتسريع وقت الاستدلال:

```py
from functools import partial

# تعطيل الذاكرة المؤقتة أثناء التدريب بسبب عدم توافقها مع حفظ نقاط التدرج
model.config.use_cache = False

# تحديد اللغة والمهام للجيل وإعادة تفعيل الذاكرة المؤقتة
model.generate = partial(model.generate, use_cache=True)
```

قم بتعريف حجج التدريب. هنا، لن نقوم بحساب أي مقاييس تقييم أثناء عملية التدريب، وسنتحدث عن التقييم لاحقاً في هذا الفصل. بدلاً من ذلك، سننظر فقط إلى الخسارة:

```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="speecht5_finetuned_voxpopuli_nl"،  # قم بتغييره إلى اسم مستودع من اختيارك
    per_device_train_batch_size=4،
    gradient_accumulation_steps=8،
    learning_rate=1e-5،
    warmup_steps=500،
    max_steps=4000،
    gradient_checkpointing=True،
    fp16=True،
    evaluation_strategy="steps"،
    per_device_eval_batch_size=2،
    save_steps=1000،
    eval_steps=1000،
    logging_steps=25،
    report_to=["tensorboard"]،
    load_best_model_at_end=True،
    greater_is_better=False،
    label_names=["labels"]،
    push_to_hub=True،
)
```

قم بإنشاء كائن المدرب `Trainer` ومرر إليه النموذج، ومجموعة البيانات، ومجمّع البيانات.

```py
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    data_collator=data_collator,
    tokenizer=processor,
)
```

وبذلك، نكون جاهزين لبدء التدريب! سيستغرق التدريب عدة ساعات. اعتماداً على وحدة معالجة الرسومات (GPU) الخاصة بك، من الممكن أن تواجه خطأ "نفاد الذاكرة" في CUDA عند بدء التدريب. في هذه الحالة، يمكنك تقليل `per_device_train_batch_size` تدريجياً بعوامل 2 وزيادة `gradient_accumulation_steps` بمقدار 2x للتعويض.

```py
trainer.train()
```

قم بدفع النموذج النهائي إلى منصة 🤗 Hub:

```py
trainer.push_to_hub()
```

## الاستدلال

بمجرد ضبط نموذج، يمكنك استخدامه للاستدلال! قم بتحميل النموذج من منصة 🤗 Hub (تأكد من استخدام اسم حسابك في مقتطف الكود التالي):

```py
model = SpeechT5ForTextToSpeech.from_pretrained(
    "YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl"
)
```

اختر مثالاً، هنا سنأخذ مثالاً من مجموعة بيانات الاختبار. احصل على تضمين المتحدث.

```py
example = dataset["test"][304]
speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
```

قم بتعريف نص الإدخال وقم بتقسيمه إلى رموز.

```py
text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"
```

قم بمعالجة نص الإدخال:

```py
inputs = processor(text=text, return_tensors="pt")
```

قم بإنشاء جهاز توليف الكلام وتوليد الكلام:

```py
from transformers import SpeechT5HifiGan

vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)
```

هل أنت مستعد للاستماع إلى النتيجة؟

```py
from IPython.display import Audio

Audio(speech.numpy(), rate=16000)
```

قد يكون الحصول على نتائج مرضية من هذا النموذج على لغة جديدة أمراً صعباً. يمكن أن تكون جودة تضمين المتحدث عاملاً مهماً. حيث أن SpeechT5 تم تدريبه مسبقاً باستخدام x-vectors الإنجليزية، فهو يعمل بشكل أفضل عند استخدام تضمين المتحدث باللغة الإنجليزية. إذا كان الكلام المُصنّع يبدو رديئاً، حاول استخدام تضمين متحدث مختلف.

من المرجح أن يؤدي زيادة مدة التدريب إلى تحسين جودة النتائج أيضاً. ومع ذلك، من الواضح أن الكلام باللغة الهولندية بدلاً من الإنجليزية، ويستطيع التقاط خصائص صوت المتحدث (قارن مع الصوت الأصلي في المثال).

هناك شيء آخر يمكن تجربته وهو تكوين النموذج. على سبيل المثال، جرب استخدام `config.reduction_factor = 1` لمعرفة ما إذا كان هذا يحسن النتائج.

في القسم التالي، سنتحدث عن كيفية تقييم نماذج تحويل النص إلى كلام.
