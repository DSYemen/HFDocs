# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS

ÙÙŠ [Ø§Ù„Ù‚Ø³Ù… 5](/course/chapter5/5)ØŒ Ù‚Ù…Ù†Ø§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù‚Ø¶Ø§ÙŠØ§ GitHub ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…Ù† Ù…Ø³ØªÙˆØ¯Ø¹ ğŸ¤— Datasets. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…ØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø« ÙŠÙ…ÙƒÙ†Ù‡ Ù…Ø³Ø§Ø¹Ø¯ØªÙ†Ø§ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù„Ø£Ø³Ø¦Ù„ØªÙ†Ø§ Ø§Ù„Ø£ÙƒØ«Ø± Ø¥Ù„Ø­Ø§Ø­Ù‹Ø§ Ø­ÙˆÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø©!

## Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ

ÙƒÙ…Ø§ Ø±Ø£ÙŠÙ†Ø§ ÙÙŠ [Ø§Ù„ÙØµÙ„ 1](/course/chapter1)ØŒ ØªÙ‚ÙˆÙ… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ÙˆÙ„ Ø¨ØªÙ…Ø«ÙŠÙ„ ÙƒÙ„ Ø±Ù…Ø² ÙÙŠ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ Ø¹Ù„Ù‰ Ø£Ù†Ù‡ _Ù…ØªØ¬Ù‡ ØªØ¶Ù…ÙŠÙ†_. Ø§ØªØ¶Ø­ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ† "ØªØ¬Ù…Ø¹" Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù…ØªØ¬Ù‡ Ù„Ù„Ø¬Ù…Ù„ Ø£Ùˆ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø£Ùˆ (ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ù„Ø§Øª) Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù…Ø§Ø«Ù„Ø© ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†Ù‚Ø§Ø· (Ø£Ùˆ Ø¨Ø¹Ø¶ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ø®Ø±Ù‰) Ø¨ÙŠÙ† ÙƒÙ„ ØªØ¶Ù…ÙŠÙ† ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ø£ÙƒØ¨Ø±.

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…ØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù„ØªØ·ÙˆÙŠØ± Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ. ØªÙˆÙØ± Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.

## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­Ø¶ÙŠØ± Ù„Ù‡Ø§

Ø£ÙˆÙ„ Ø´ÙŠØ¡ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙØ¹Ù„Ù‡ Ù‡Ùˆ ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¶Ø§ÙŠØ§ GitHub Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ØŒ Ù„Ø°Ø§ Ø¯Ø¹Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© `load_dataset()` ÙƒÙ…Ø§ Ù‡Ùˆ Ù…Ø¹ØªØ§Ø¯:

```py
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")
issues_dataset
```

```python out
Dataset({
features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_
```

Ù„Ù‚Ø¯ Ø­Ø¯Ø¯Ù†Ø§ Ù‡Ù†Ø§ Ø§Ù„Ø§Ù†Ù‚Ø³Ø§Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ `train` ÙÙŠ `load_dataset()`ØŒ Ù„Ø°Ø§ ÙØ¥Ù†Ù‡ ÙŠØ¹ÙŠØ¯ `Dataset` Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† `DatasetDict`. Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø¹Ù…Ù„ Ù‡ÙŠ ØªØµÙÙŠØ© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø³Ø­Ø¨ØŒ Ø­ÙŠØ« Ù†Ø§Ø¯Ø±Ù‹Ø§ Ù…Ø§ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØ³ØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø¶ÙˆØ¶Ø§Ø¡ ÙÙŠ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§. ÙƒÙ…Ø§ Ù‡Ùˆ Ù…Ø£Ù„ÙˆÙ Ø§Ù„Ø¢Ù†ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© `Dataset.filter()` Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù‡Ø°Ù‡ Ø§Ù„ØµÙÙˆÙ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§. Ø£Ø«Ù†Ø§Ø¡ Ù‚ÙŠØ§Ù…Ù†Ø§ Ø¨Ø°Ù„ÙƒØŒ Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¨ØªØµÙÙŠØ© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØ¹Ù„ÙŠÙ‚Ø§ØªØŒ Ø­ÙŠØ« Ù„Ø§ ØªÙˆÙØ± Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†:

```py
issues_dataset = issues_dataset.filter(
lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
num_rows: 771
})
```

ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†Ø±Ù‰ Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ØŒ Ù…Ø¹Ø¸Ù…Ù‡Ø§ Ù„Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§. Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø§Ù„Ø¨Ø­Ø«ØŒ ÙØ¥Ù† Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¥ÙØ§Ø¯Ø© Ù‡ÙŠ `title`ØŒ Ùˆ`body`ØŒ Ùˆ`comments`ØŒ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† `html_url` ÙŠÙˆÙØ± Ù„Ù†Ø§ Ø±Ø§Ø¨Ø·Ù‹Ø§ ÙŠØ¹ÙŠØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ù…ØµØ¯Ø± Ø§Ù„Ù‚Ø¶ÙŠØ©. Ø¯Ø¹Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© `Dataset.remove_columns()` Ù„Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø¨Ø§Ù‚ÙŠ:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
features: ['html_url', 'title', 'comments', 'body'],
num_rows: 771
})
```

Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ØŒ Ø³Ù†Ù‚ÙˆÙ… Ø¨Ø²ÙŠØ§Ø¯Ø© ÙƒÙ„ ØªØ¹Ù„ÙŠÙ‚ Ù…Ø¹ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø¶ÙŠØ© ÙˆÙˆØµÙÙ‡Ø§ØŒ Ø­ÙŠØ« ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ØªØªØ¶Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³ÙŠØ§Ù‚ÙŠØ© Ù…ÙÙŠØ¯Ø©. Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ø¹Ù…ÙˆØ¯ `comments` Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§ Ù‡Ùˆ Ø­Ø§Ù„ÙŠÙ‹Ø§ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù„ÙƒÙ„ Ù‚Ø¶ÙŠØ©ØŒ ÙÙ†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ "ØªÙØ¬ÙŠØ±" Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø¨Ø­ÙŠØ« ÙŠØªÙƒÙˆÙ† ÙƒÙ„ ØµÙ Ù…Ù† Ø±Ø§Ø¨Ø·Ø© `(html_urlØŒ titleØŒ bodyØŒ comment)`. ÙÙŠ PandasØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© [`DataFrame.explode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html)ØŒ ÙˆØ§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙ Ø¬Ø¯ÙŠØ¯ Ù„ÙƒÙ„ Ø¹Ù†ØµØ± ÙÙŠ Ø¹Ù…ÙˆØ¯ ÙŠØ´Ø¨Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŒ Ù…Ø¹ ØªÙƒØ±Ø§Ø± Ø¬Ù…ÙŠØ¹ Ù‚ÙŠÙ… Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø®Ø±Ù‰. Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø°Ù„Ùƒ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ØŒ Ø¯Ø¹Ù†Ø§ Ù†ØªØ­ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `DataFrame` Ù„Ù€ Pandas:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

Ø¥Ø°Ø§ Ù‚Ù…Ù†Ø§ Ø¨ÙØ­Øµ Ø§Ù„ØµÙ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù‡Ø°Ø§ `DataFrame`ØŒ ÙÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†Ø±Ù‰ Ø£Ù†Ù‡ ØªÙˆØ¬Ø¯ Ø£Ø±Ø¨Ø¹Ø© ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø¶ÙŠØ©:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in ï¼š
if data_args.task_name is not None:
# Downloading and loading a dataset from the hub.
datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
'Hi @jinec,

From time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com

Normally, it should work if you wait a little and then retry.

Could you please confirm if the problem persists?',
'cannot connectØŒeven by Web browserØŒplease check that there is some problems.',
'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

Ø¹Ù†Ø¯Ù…Ø§ Ù†Ù‚ÙˆÙ… Ø¨ØªÙØ¬ÙŠØ± `df`ØŒ Ù†ØªÙˆÙ‚Ø¹ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØµÙ ÙˆØ§Ø­Ø¯ Ù„ÙƒÙ„ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª. Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ø§Ù„:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
<thead>
<tr style="text-align: right;">
<th></th>
<th>html_url</th>
<th>title</th>
<th>comments</th>
<th>body</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>the bug code locate in  :
if data_args.task_name is not None ...</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
<tr>
<th>1</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>Hi @jinec,

From time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com ...</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
<tr>
<th>2</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>cannot connectØŒeven by Web browserØŒplease check that there is some problems.</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
<tr>
<th>3</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem ...</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
</tbody>
</table>

Ø±Ø§Ø¦Ø¹ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†Ø±Ù‰ Ø£Ù† Ø§Ù„ØµÙÙˆÙ Ù‚Ø¯ ØªÙ… ØªÙƒØ±Ø§Ø±Ù‡Ø§ØŒ Ù…Ø¹ Ø§Ø­ØªÙˆØ§Ø¡ Ø¹Ù…ÙˆØ¯ "Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª" Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©! Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø§Ù†ØªÙ‡ÙŠÙ†Ø§ Ù…Ù† PandasØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¨Ø³Ø±Ø¹Ø© Ø¥Ù„Ù‰ "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ù…ÙŠÙ„ "Ø¥Ø·Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª" ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
features: ['html_url', 'title', 'comments', 'body'],
num_rows: 2842
})
```

Ø­Ø³Ù†Ù‹Ø§ØŒ Ù„Ù‚Ø¯ Ø£Ø¹Ø·Ø§Ù†Ø§ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹Ø© Ø¢Ù„Ø§Ù Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù‡Ø§!

âœï¸ **Ø¬Ø±Ø¨Ù‡!** Ø§Ù†Ø¸Ø± Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ·ÙŠØ¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… `Dataset.map()` Ù„ØªÙØ¬ÙŠØ± Ø¹Ù…ÙˆØ¯ "Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª" Ù…Ù† `issues_dataset` _Ø¨Ø¯ÙˆÙ†_ Ø§Ù„Ù„Ø¬ÙˆØ¡ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Pandas. Ù‡Ø°Ø§ Ø£Ù…Ø± ØµØ¹Ø¨ Ø¨Ø¹Ø¶ Ø§Ù„Ø´ÙŠØ¡Ø› Ù‚Ø¯ ØªØ¬Ø¯ Ù‚Ø³Ù… ["Ø§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯ÙØ¹ÙŠ"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) Ù…Ù† ÙˆØ«Ø§Ø¦Ù‚ ğŸ¤— Datasets Ù…ÙÙŠØ¯Ù‹Ø§ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©.

Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø£ØµØ¨Ø­ Ù„Ø¯ÙŠÙ†Ø§ ØªØ¹Ù„ÙŠÙ‚ ÙˆØ§Ø­Ø¯ Ù„ÙƒÙ„ ØµÙØŒ Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙˆØ¯ Ø¬Ø¯ÙŠØ¯ ÙŠØ³Ù…Ù‰ `comment_length` ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ ØªØ¹Ù„ÙŠÙ‚:

```py
comments_dataset = comments_dataset.map(
lambda x: {"comment_length": len(x["comments"].split())}
)
```

ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„ØªØµÙÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ØªØªØ¶Ù…Ù† Ø£Ø´ÙŠØ§Ø¡ Ù…Ø«Ù„ "cc @lewtun" Ø£Ùˆ "Ø´ÙƒØ±Ù‹Ø§!" ÙˆØ§Ù„ØªÙŠ Ù„Ø§ ØªØªØ¹Ù„Ù‚ Ø¨Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§. Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ù‚Ù… Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ø£Ø¬Ù„ Ø§Ù„ØªØµÙÙŠØ©ØŒ ÙˆÙ„ÙƒÙ† Ø­ÙˆØ§Ù„ÙŠ 15 ÙƒÙ„Ù…Ø© ØªØ¨Ø¯Ùˆ Ø¨Ø¯Ø§ÙŠØ© Ø¬ÙŠØ¯Ø©:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
num_rows: 2098
})
```

Ø¨Ø¹Ø¯ ØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ Ù‚Ù„ÙŠÙ„Ø§Ù‹ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø¯Ù…Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø¶ÙŠØ© ÙˆØ§Ù„ÙˆØµÙ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…Ø¹Ù‹Ø§ ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø¬Ø¯ÙŠØ¯ ÙŠØ³Ù…Ù‰ `text`. ÙƒÙ…Ø§ Ù‡Ùˆ Ù…Ø¹ØªØ§Ø¯ØŒ Ø³Ù†Ù‚ÙˆÙ… Ø¨ÙƒØªØ§Ø¨Ø© Ø¯Ø§Ù„Ø© Ø¨Ø³ÙŠØ·Ø© ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ø¥Ù„Ù‰ `Dataset.map()`:

```py
def concatenate_text(examples):
return {
"text": examples["title"]
+ " \n "
+ examples["body"]
+ " \n "
+ examples["comments"]
}

comments_dataset = comments_dataset.map(concatenate_text)
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ù†Ø­Ù† Ù…Ø³ØªØ¹Ø¯ÙˆÙ† Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø¹Ø¶ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª! Ø¯Ø¹Ù†Ø§ Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø©.
## Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ† Ù†ØµÙŠ [[creating-text-embeddings]]

Ø±Ø£ÙŠÙ†Ø§ ÙÙŠ [Ø§Ù„ÙØµÙ„ 2](/course/chapter2) Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ¦Ø© `AutoModel`. ÙƒÙ„ Ù…Ø§ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙØ¹Ù„Ù‡ Ù‡Ùˆ Ø§Ø®ØªÙŠØ§Ø± Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†Ù‡Ø§. Ù„Ø­Ø³Ù† Ø§Ù„Ø­Ø¸ØŒ Ù‡Ù†Ø§Ùƒ Ù…ÙƒØªØ¨Ø© ØªØ³Ù…Ù‰ `sentence-transformers` Ù…Ø®ØµØµØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª. ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ ÙÙŠ [ÙˆØ«Ø§Ø¦Ù‚] (https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) Ø§Ù„Ù…ÙƒØªØ¨Ø©ØŒ ÙØ¥Ù† Ø­Ø§Ù„ØªÙ†Ø§ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…ÙŠØ© Ù‡ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ _Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ØºÙŠØ± Ø§Ù„Ù…ØªÙ…Ø§Ø«Ù„_ Ù„Ø£Ù† Ù„Ø¯ÙŠÙ†Ø§ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ù†Ø±ØºØ¨ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨ØªÙ‡ ÙÙŠ ÙˆØ«ÙŠÙ‚Ø© Ø£Ø·ÙˆÙ„ØŒ Ù…Ø«Ù„ ØªØ¹Ù„ÙŠÙ‚ Ø¹Ù„Ù‰ Ø¥ØµØ¯Ø§Ø±. ÙŠØ´ÙŠØ± Ø¬Ø¯ÙˆÙ„ [Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬] (https://www.sbert.net/docs/pretrained_models.html#model-overview) Ø§Ù„Ù…ÙÙŠØ¯ ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¥Ù„Ù‰ Ø£Ù† Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ `multi-qa-mpnet-base-dot-v1` Ù„Ø¯ÙŠÙ‡Ø§ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØŒ Ù„Ø°Ø§ ÙØ³Ù†Ø³ØªØ®Ø¯Ù… Ø°Ù„Ùƒ Ù„ØªØ·Ø¨ÙŠÙ‚Ù†Ø§. Ø³Ù†Ù‚ÙˆÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

Ù„ØªØ³Ø±ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¶Ù…ÙŠÙ†ØŒ Ù…Ù† Ø§Ù„Ù…ÙÙŠØ¯ ÙˆØ¶Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² GPUØŒ Ù„Ø°Ø§ Ø¯Ø¹Ù†Ø§ Ù†ÙØ¹Ù„ Ø°Ù„Ùƒ Ø§Ù„Ø¢Ù†:

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ù‚Ù…Ù†Ø§ Ø¨ØªØ¹ÙŠÙŠÙ† `from_pt=True` ÙƒØ­Ø¬Ø© Ù„Ø·Ø±ÙŠÙ‚Ø© `from_pretrained()`. ÙˆÙŠØ±Ø¬Ø¹ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø£Ù† Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ `multi-qa-mpnet-base-dot-v1` ØªØ­ØªÙˆÙŠ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† PyTorchØŒ Ù„Ø°Ø§ ÙØ¥Ù† ØªØ¹ÙŠÙŠÙ† `from_pt=True` Ø³ÙŠØ­ÙˆÙ„Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ TensorFlow Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ø§. ÙƒÙ…Ø§ ØªØ±ÙˆÙ†ØŒ Ù…Ù† Ø§Ù„Ø³Ù‡Ù„ Ø¬Ø¯Ù‹Ø§ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø·Ø± ÙÙŠ ğŸ¤— Transformers!

{/if}

ÙƒÙ…Ø§ Ø°ÙƒØ±Ù†Ø§ Ø³Ø§Ø¨Ù‚Ù‹Ø§ØŒ Ù†Ø±ØºØ¨ ÙÙŠ ØªÙ…Ø«ÙŠÙ„ ÙƒÙ„ Ø¥Ø¯Ø®Ø§Ù„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª GitHub Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ Ø¹Ù„Ù‰ Ø£Ù†Ù‡Ø§ Ù…ØªØ¬Ù‡ ÙˆØ§Ø­Ø¯ØŒ Ù„Ø°Ù„Ùƒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ "ØªØ¬Ù…Ø¹" Ù…ØªÙˆØ³Ø·Ø§Øª ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø§. Ø£Ø­Ø¯ Ø§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù‡Ùˆ ØªÙ†ÙÙŠØ° *ØªØ¬Ù…Ø¹ CLS* Ø¹Ù„Ù‰ Ù†ÙˆØ§ØªØ¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ Ø­ÙŠØ« Ù†Ù‚ÙˆÙ… Ø¨Ø¨Ø³Ø§Ø·Ø© Ø¨Ø¬Ù…Ø¹ Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ø®Ø§Øµ `[CLS]`. ØªÙ‚ÙˆÙ… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ù„Ø®Ø¯Ø¹Ø© Ù„Ù†Ø§:

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø³Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ø³ØªÙ‚ÙˆÙ… Ø¨Ø±Ù…Ø²ÙŠØ© Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ ÙˆÙˆØ¶Ø¹ Ø§Ù„Ù…Ù†Ø³ÙˆØ¬Ø§Øª Ø¹Ù„Ù‰ GPUØŒ ÙˆØ¥Ø·Ø¹Ø§Ù…Ù‡Ø§ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆØ£Ø®ÙŠØ±Ù‹Ø§ ØªØ·Ø¨ÙŠÙ‚ ØªØ¬Ù…ÙŠØ¹ CLS Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙˆØ§ØªØ¬:

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØºØ°ÙŠØªÙ‡Ø§ Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ ÙˆÙØ­Øµ Ø´ÙƒÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

Ø±Ø§Ø¦Ø¹ØŒ Ù„Ù‚Ø¯ Ù‚Ù…Ù†Ø§ Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡ 768-Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯! ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… `Dataset.map()` Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ¸ÙŠÙØ© `get_embeddings()` Ø¹Ù„Ù‰ ÙƒÙ„ ØµÙ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ØŒ Ù„Ø°Ø§ Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙˆØ¯ "ØªØ¶Ù…ÙŠÙ†Ø§Øª" Ø¬Ø¯ÙŠØ¯ ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØºØ°ÙŠØªÙ‡Ø§ Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ ÙˆÙØ­Øµ Ø´ÙƒÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

Ø±Ø§Ø¦Ø¹ØŒ Ù„Ù‚Ø¯ Ù‚Ù…Ù†Ø§ Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡ 768-Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯! ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… `Dataset.map()` Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ¸ÙŠÙØ© `get_embeddings()` Ø¹Ù„Ù‰ ÙƒÙ„ ØµÙ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ØŒ Ù„Ø°Ø§ Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙˆØ¯ "ØªØ¶Ù…ÙŠÙ†Ø§Øª" Ø¬Ø¯ÙŠØ¯ ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}

Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ù‚Ù…Ù†Ø§ Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ§Øª NumPy - ÙˆÙŠØ±Ø¬Ø¹ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø£Ù† ğŸ¤— Datasets ØªØªØ·Ù„Ø¨ Ù‡Ø°Ø§ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¹Ù†Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙ‡Ø±Ø³ØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISSØŒ ÙˆØ§Ù„ØªÙŠ Ø³Ù†Ù‚ÙˆÙ… Ø¨Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ.

## Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙƒÙØ§Ø¡Ø© [[using-faiss-for-efficient-similarity-search]]

Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø£ØµØ¨Ø­ Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§ØªØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø·Ø±ÙŠÙ‚Ø© Ù„Ù„Ø¨Ø­Ø« ÙÙŠÙ‡Ø§. Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„ÙƒØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ù†ÙŠØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§ØµØ© ÙÙŠ ğŸ¤— Datasets ØªØ³Ù…Ù‰ _FAISS index_. [FAISS] (https://faiss.ai/) (Ø§Ø®ØªØµØ§Ø± Ù„Ù€ Facebook AI Similarity Search) Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© ØªÙˆÙØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§ Ø¨Ø³Ø±Ø¹Ø©.

Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ±Ø§Ø¡ FAISS Ù‡ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ù†ÙŠØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§ØµØ© ØªØ³Ù…Ù‰ _index_ ÙˆØ§Ù„ØªÙŠ ØªØªÙŠØ­ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù…Ø¹ ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„. Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ ğŸ¤— Datasets Ø£Ù…Ø± Ø¨Ø³ÙŠØ· - Ù†Ø³ØªØ®Ø¯Ù… ÙˆØ¸ÙŠÙØ© `Dataset.add_faiss_index()` ÙˆÙ†Ø­Ø¯Ø¯ Ø¹Ù…ÙˆØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙŠ Ù†Ø±ØºØ¨ ÙÙŠ ÙÙ‡Ø±Ø³ØªÙ‡:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ø¢Ù† Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ Ø¬Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `Dataset.get_nearest_examples()`. Ø¯Ø¹Ù†Ø§ Ù†Ø¬Ø±Ø¨ Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¶Ù…ÙŠÙ† Ø³Ø¤Ø§Ù„ ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

Ù…Ø«Ù„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ Ù„Ø¯ÙŠÙ†Ø§ Ø§Ù„Ø¢Ù† Ù…ØªØ¬Ù‡ 768-Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ÙŠÙ…Ø«Ù„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù…Ù‚Ø§Ø±Ù†ØªÙ‡ Ø¨Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ØªØ´Ø§Ø¨Ù‡Ù‹Ø§:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

ØªÙØ±Ø¬Ø¹ ÙˆØ¸ÙŠÙØ© `Dataset.get_nearest_examples()` Ø²ÙˆØ¬Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØªÙŠ ØªØµÙ†Ù Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù‚Ø§Ø¨Ù„Ø© Ù…Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª (Ø£ÙØ¶Ù„ 5 Ù…Ø¨Ø§Ø±ÙŠØ§Øª Ù‡Ù†Ø§). Ø¯Ø¹Ù†Ø§ Ù†Ø¬Ù…Ø¹Ù‡Ø§ ÙÙŠ `pandas.DataFrame` Ø­ØªÙ‰ Ù†ØªÙ…ÙƒÙ† Ù…Ù† ÙØ±Ø²Ù‡Ø§ Ø¨Ø³Ù‡ÙˆÙ„Ø©:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙˆÙ‚ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù‚Ù„ÙŠÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø¯Ù‰ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

Ù„ÙŠØ³ Ø³ÙŠØ¦Ø§! ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø¥ØµØ§Ø¨ØªÙ†Ø§ Ø§Ù„Ø«Ø§Ù†ÙŠØ© ØªØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….

<Tip>

âœï¸ **Ø¬Ø±Ø¨Ù‡!** Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ùƒ Ø§Ù„Ø®Ø§Øµ ÙˆØ´Ø§Ù‡Ø¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¨Ø¥Ù…ÙƒØ§Ù†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¯Ø©. Ù‚Ø¯ ØªØ¶Ø·Ø± Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ù…Ø¹Ù„Ù…Ø© `k` ÙÙŠ `Dataset.get_nearest_examples()` Ù„ØªÙˆØ³ÙŠØ¹ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¨Ø­Ø«.

</Tip>