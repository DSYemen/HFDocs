<FrameworkSwitchCourse {fw} />

# تصحيح أخطاء خط أنابيب التدريب [[debugging-the-training-pipeline]]

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section4_tf.ipynb"},
]} />

لقد كتبت سيناريو جميلًا لتدريب أو ضبط نموذج على مهمة معينة، مع اتباع النصيحة من [الفصل 7](/course/chapter7) بكل أمانة. ولكن عندما تطلق الأمر `model.fit()`، يحدث شيء مروع: تحصل على خطأ 😱! أو الأسوأ من ذلك، يبدو أن كل شيء على ما يرام، ويتم التدريب بدون أخطاء، ولكن النموذج الناتج سيء. في هذا القسم، سنريكم ما يمكنكم فعله لتصحيح هذه الأنواع من المشاكل.

## تصحيح أخطاء خط أنابيب التدريب [[debugging-the-training-pipeline]]

<Youtube id="N9kO52itd0Q"/>

المشكلة عند مواجهة خطأ في `model.fit()` هي أنه قد يأتي من مصادر متعددة، حيث أن التدريب عادة ما يجمع الكثير من الأشياء التي كنت تعمل عليها حتى تلك النقطة. قد تكون المشكلة خطأ ما في مجموعة بياناتك، أو بعض المشاكل عند محاولة تجميع عناصر مجموعات البيانات معًا. أو قد يكون هناك خطأ ما في كود النموذج، أو دالة الخسارة أو المحسن. وحتى إذا سار كل شيء على ما يرام للتدريب، فقد يحدث خطأ ما أثناء التقييم إذا كان هناك مشكلة في مقياسك.

أفضل طريقة لتصحيح خطأ ينشأ في `model.fit()` هي المرور يدويًا عبر هذا الخط الكامل لرؤية المكان الذي حدثت فيه الأمور بشكل خاطئ. بعد ذلك، يكون الخطأ سهل الحل في الغالب.

لإثبات ذلك، سنستخدم السيناريو التالي الذي (يحاول) ضبط نموذج DistilBERT على [مجموعة بيانات MNLI](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)
```

إذا حاولت تنفيذه، فقد تحصل على بعض `VisibleDeprecationWarning`s عند إجراء تحويل مجموعة البيانات - هذه مشكلة UX معروفة لدينا، لذا يرجى تجاهلها. إذا كنت تقرأ الدورة بعد، على سبيل المثال، نوفمبر 2021 ولا تزال تحدث، فابعث بتغريدات غاضبة إلى @carrigmat حتى يصلحها.

ما هي المشكلة الأكثر خطورة، على الرغم من ذلك، هي أننا نحصل على خطأ صريح. وهو طويل حقًا، ومخيف:

```python out
ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']
```

ماذا يعني ذلك؟ لقد حاولنا التدريب على بياناتنا، ولكننا لم نحصل على تدرج؟ هذا محير للغاية؛ كيف يمكننا حتى البدء في تصحيح شيء مثل ذلك؟ عندما لا يشير الخطأ الذي تحصل عليه مباشرة إلى مكان المشكلة، فإن الحل الأفضل غالبًا هو المرور عبر الأشياء بالتسلسل، والتأكد في كل مرحلة من أن كل شيء يبدو صحيحًا. وبالطبع، المكان الذي يجب أن نبدأ منه دائمًا هو...

### تحقق من بياناتك [[check-your-data]]

هذا أمر بديهي، ولكن إذا كانت بياناتك فاسدة، فلن يتمكن Keras من إصلاحها لك. لذا، يجب عليك أولاً وقبل كل شيء، أن تلقي نظرة على ما بداخل مجموعة التدريب الخاصة بك.

على الرغم من أن من المغري النظر داخل `raw_datasets` و`tokenized_datasets`، إلا أننا نوصي بشدة بالذهاب إلى البيانات مباشرة في النقطة التي ستدخل فيها النموذج. وهذا يعني قراءة مخرجات `tf.data.Dataset` التي أنشأتها باستخدام وظيفة `to_tf_dataset()`! إذن كيف نفعل ذلك؟ تعطينا كائنات `tf.data.Dataset` مجموعات كاملة في كل مرة ولا تدعم الفهرسة، لذا لا يمكننا ببساطة أن نطلب `train_dataset[0]`. ومع ذلك، يمكننا أن نطلب من ذلك بلباقة للحصول على مجموعة:

```py
for batch in train_dataset:
    break
```

`break` ينهي الحلقة بعد تكرار واحد، لذا فإن هذا يمسك بالمجموعة الأولى التي تخرج من `train_dataset` ويحفظها كـ `batch`. الآن، دعونا نلقي نظرة على ما بداخلها:

```python out
{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}
```

يبدو هذا صحيحًا، أليس كذلك؟ نحن نمرر `labels`، و`attention_mask`، و`input_ids` إلى النموذج، والذي يجب أن يكون كل ما يحتاجه لحساب المخرجات وحساب الخسارة. إذن لماذا لا يوجد لدينا تدرج؟ انظر عن كثب: نحن نمرر قاموسًا واحدًا كإدخال، ولكن مجموعة التدريب عادة ما تكون عبارة عن قاموس إدخال أو قاموس إدخال، بالإضافة إلى قاموس علامات. علاماتنا هي مجرد مفتاح في قاموس الإدخال الخاص بنا.

هل هذه مشكلة؟ في الواقع، ليست دائمًا! ولكنها واحدة من أكثر المشاكل شيوعًا التي ستواجهها عند تدريب نماذج Transformer باستخدام TensorFlow. يمكن لجميع نماذجنا حساب الخسارة داخليًا، ولكن للقيام بذلك، تحتاج العلامات إلى تمريرها في قاموس الإدخال. هذه هي الخسارة التي يتم استخدامها عندما لا نقوم بتحديد قيمة الخسارة إلى `compile()`. من ناحية أخرى، يتوقع Keras عادةً تمرير العلامات بشكل منفصل عن قاموس الإدخال، وستفشل حسابات الخسارة عادةً إذا لم تفعل ذلك.

لقد أصبحت المشكلة الآن أكثر وضوحًا: لقد مررنا بحجة `loss`، مما يعني أننا نطلب من Keras حساب الخسائر لنا، ولكننا مررنا بعلاماتنا كإدخالات للنموذج، وليس كعلامات في المكان الذي يتوقعها Keras! نحن بحاجة إلى اختيار أحدهما: إما أن نستخدم الخسارة الداخلية للنموذج ونحتفظ بالعلامات في مكانها، أو أننا نواصل استخدام خسائر Keras، ولكننا ننقل العلامات إلى المكان الذي يتوقعها فيه Keras. للتبسيط، دعنا نأخذ النهج الأول. قم بتغيير المكالمة إلى `compile()` لقراءة:

```py
model.compile(optimizer="adam")
```
الآن سنستخدم الخسارة الداخلية للنماذج، وينبغي حل هذه المشكلة!

<نصيحة>

✏️ **دورك!** كتحدٍ اختياري بعد حل المشاكل الأخرى، يمكنك أن تحاول العودة إلى هذه الخطوة وجعل النموذج يعمل مع الخسارة المحسوبة أصلاً في Keras بدلاً من الخسارة الداخلية. ستحتاج إلى إضافة `"labels"` إلى حجة `label_cols` لـ `to_tf_dataset()` للتأكد من أن العلامات يتم إخراجها بشكل صحيح، مما سيمنحك التدرجات -- ولكن هناك مشكلة أخرى مع الخسارة التي حددناها. سيستمر التدريب مع هذه المشكلة، ولكن التعلم سيكون بطيئًا للغاية وسيتوقف عند خسارة تدريب عالية. هل يمكنك معرفة ما هي؟

تلميح مشفر بـ ROT13، إذا كنت عالقًا: إذا نظرت إلى خصائص طبقة FrdhraprPynffvsvpngvba في Genafsbezref، فإن أول خاصية لها هي `ybtvgf`. ما هي ybtvgf؟

وهناك تلميح ثانٍ: عندما تتعامل مع المتجهات، أو المصفوفات، أو التنسورات مع القيم، تطلب Keras جميع الخصائص إلى أجهزتها. ما هي الخصائص التي يمتلكها FcnefrPngrtbevpnyPebffragebcl، وما هي أجهزتها؟

</نصيحة>

الآن، دعنا نحاول التدريب. يجب أن نحصل على التدرجات الآن، لذا نأمل (تُعزف هنا موسيقى مخيفة) أن نتمكن من استدعاء `model.fit()` وسيعمل كل شيء بشكل جيد!

```python out
  246/24543 [..............................] - ETA: 15:52 - loss: nan
```

يا إلهي.

`nan` ليست قيمة خسارة مشجعة للغاية. مع ذلك، فقد تحققنا من بياناتنا، وهي تبدو جيدة جدًا. إذا لم تكن هذه هي المشكلة، فأين يمكننا أن نذهب بعد ذلك؟ الخطوة التالية الواضحة هي...

### تحقق من نموذجك [[check-your-model]]

`model.fit()` هي وظيفة راحة رائعة في Keras، ولكنها تقوم بالكثير من الأشياء من أجلك، وهذا يمكن أن يجعل من الصعب تحديد مكان حدوث المشكلة بالضبط. إذا كنت تقوم بتصحيح أخطاء نموذجك، فهناك استراتيجية يمكن أن تساعدك حقًا وهي تمرير دفعة واحدة فقط إلى النموذج، والنظر إلى المخرجات لتلك الدفعة بالتفصيل. وهناك نصيحة أخرى مفيدة حقًا إذا كان النموذج يرمي أخطاء وهي `compile()` النموذج مع `run_eagerly=True`. سيجعل هذا الأمر أبطأ بكثير، ولكنه سيجعل رسائل الخطأ أكثر قابلية للفهم، لأنها ستشير بالضبط إلى المكان الذي حدثت فيه المشكلة في كود نموذجك.

ولكن، في الوقت الحالي، لا نحتاج إلى `run_eagerly` بعد. دعنا نُشغّل `batch` التي حصلنا عليها من قبل من خلال النموذج ونرى كيف تبدو المخرجات:

```py
model(batch)
```

```python out
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)
```

حسنًا، هذا أمر معقد. كل شيء هو `nan`! لكن هذا غريب، أليس كذلك؟ كيف يمكن أن تصبح جميع قيمتنا اللوجستية `nan`؟ `nan` تعني "ليس رقمًا". غالبًا ما تحدث قيم `nan` عندما تقوم بعملية محظورة، مثل القسمة على صفر. لكن هناك شيء مهم جدًا يجب معرفته عن `nan` في التعلم الآلي وهو أن هذه القيمة تميل إلى *الانتشار*. إذا قمت بضرب رقم بـ `nan`، فإن الإخراج يكون أيضًا `nan`. وإذا حصلت على `nan` في أي مكان في إخراجك، أو خسارتك، أو تدرجك، فسوف ينتشر بسرعة في جميع أنحاء نموذجك بالكامل -- لأنك عندما يتم نشر قيمة `nan` هذه مرة أخرى عبر شبكتك، ستحصل على تدرجات `nan`، وعندما يتم حساب تحديثات الأوزان بهذه التدرجات، ستحصل على أوزان `nan`، وستحسب هذه الأوزان المزيد من المخرجات `nan`! في وقت قريب، ستصبح الشبكة بأكملها مجرد كتلة كبيرة من `nan`s. بمجرد حدوث ذلك، يكون من الصعب جدًا معرفة مكان بداية المشكلة. كيف يمكننا عزل المكان الذي تسلل فيه `nan` لأول مرة؟

الإجابة هي محاولة *إعادة التهيئة* لنموذجنا. بمجرد أن بدأنا التدريب، حصلنا على `nan` في مكان ما وانتشر بسرعة عبر النموذج بالكامل. لذا، دعنا نحمل النموذج من نقطة تفتيش ولا نقوم بأي تحديثات للأوزان، ونرى أين نحصل على قيمة `nan`:

```py
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)
```

عندما نقوم بتشغيل ذلك، نحصل على:

```py out
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)
```

*الآن* نحن نحصل على شيء ما! لا توجد قيم `nan` في قيمنا اللوجستية، وهو أمر مطمئن. لكننا نرى بعض قيم `nan` في خسارتنا! هل هناك شيء ما حول هذه العينات على وجه الخصوص يسبب هذه المشكلة؟ دعنا نرى أي منها (ملاحظة: إذا قمت بتشغيل هذا الكود بنفسك، فقد تحصل على مؤشرات مختلفة لأن مجموعة البيانات تم خلطها):

```python
import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices
```

```python out
array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])
```

دعنا ننظر إلى العينات التي جاءت منها هذه المؤشرات:

```python
input_ids = batch["input_ids"].numpy()
input_ids[indices]
```
```
array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,
        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,
         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,
         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,
         2158,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,
         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,
         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,
          102,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,
         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,
         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,
         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,
         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,
         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,
         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,
         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,
         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,
         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,
         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,
         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,
         2105,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,
         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,
         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,
         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,
         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,
        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,
         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,
        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0]])
```

حسناً، هناك الكثير هنا، ولكن لا شيء يبدو غير عادي. دعنا نلقي نظرة على التصنيفات:

```python out
labels = batch['labels'].numpy()
labels[indices]
```

```python out
array([2, 2, 2, 2, 2, 2, 2, 2, 2])
```

آه! جميع العينات التي تحتوي على `nan` لديها نفس التصنيف، وهو التصنيف 2. هذه إشارة قوية جداً. حقيقة أننا نحصل فقط على خسارة `nan` عندما يكون تصنيفنا هو 2 يشير إلى أن هذا هو الوقت المناسب للتحقق من عدد التصنيفات في نموذجنا:

```python
model.config.num_labels
```

```python out
2
```

الآن نرى المشكلة: يعتقد النموذج أنه هناك فقط فئتين، ولكن التصنيفات تصل إلى 2، مما يعني أنه في الواقع هناك ثلاث فئات (لأن 0 هي أيضاً فئة). بهذه الطريقة حصلنا على `nan` - من خلال محاولة حساب الخسارة لفئة غير موجودة! دعنا نحاول تغيير ذلك وتدريب النموذج مرة أخرى:

```
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)
```

```python out
  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032

```

نحن في مرحلة التدريب! لا مزيد من `nan`s، وخسارتنا في انخفاض... نوعًا ما. إذا تابعت ذلك لفترة، فقد تبدأ في الشعور بالقلق، لأن قيمة الخسارة تظل مرتفعة. دعنا نتوقف عن التدريب هنا ونحاول التفكير فيما قد يكون سبب هذه المشكلة. في هذه المرحلة، نحن متأكدون تمامًا من أن البيانات والنموذج على ما يرام، لكن نموذجنا لا يتعلم بشكل جيد. ماذا بقي؟ حان الوقت...

### تحقق من فرط المعاملات[[check-your-hyperparameters]]

إذا نظرت إلى الكود أعلاه، فقد لا تتمكن من رؤية أي فرط معاملات على الإطلاق، باستثناء ربما `batch_size`، وهذا لا يبدو أنه المتهم المحتمل. لكن لا تنخدع؛ فهناك دائمًا فرط معاملات، وإذا لم تتمكن من رؤيتها، فهذا يعني ببساطة أنك لا تعرف ما هي قيمتها. على وجه الخصوص، تذكر أمرًا حاسمًا حول Keras: إذا قمت بضبط الخسارة أو المحسن أو دالة التنشيط باستخدام سلسلة، _سيتم ضبط جميع حججها على قيمها الافتراضية_. هذا يعني أنه على الرغم من أن استخدام السلاسل لهذا الأمر مناسب جدًا، إلا أنه يجب أن تكون حذرًا جدًا عند القيام بذلك، لأنه يمكن أن يخفي عنك أمورًا حاسمة. (يجب على أي شخص يحاول التحدي الاختياري أعلاه أن يأخذ ملاحظة دقيقة لهذه الحقيقة.)

في هذه الحالة، أين قمنا بضبط حجة باستخدام سلسلة؟ كنا نضبط الخسارة باستخدام سلسلة في البداية، لكننا لم نعد نفعل ذلك. ومع ذلك، فإننا نضبط المحسن باستخدام سلسلة. هل يمكن أن يخفي ذلك أي شيء عنا؟ دعنا نلقي نظرة على [حججه](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).

هل هناك أي شيء بارز هنا؟ هذا صحيح -- معدل التعلم! عندما نستخدم فقط السلسلة `'adam'`، سنحصل على معدل التعلم الافتراضي، والذي هو 0.001، أو 1e-3. هذا مرتفع للغاية لنموذج Transformer! بشكل عام، نوصي بتجربة معدلات التعلم بين 1e-5 و 1e-4 لنماذجك؛ وهذا ما بين 10X و 100X أصغر من القيمة التي نستخدمها بالفعل هنا. يبدو ذلك وكأنه قد يكون مشكلة كبيرة، لذا دعنا نحاول تقليله. للقيام بذلك، نحتاج إلى استيراد كائن `optimizer` الفعلي. وبينما نفعل ذلك، دعنا نعيد تهيئة النموذج من نقطة التفتيش، في حالة تسبب التدريب بمعدل التعلم المرتفع في إتلاف أوزانه:

```python
from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))
```

<Tip>

💡 يمكنك أيضًا استيراد دالة `create_optimizer()` من 🤗 Transformers، والتي ستمنحك محسن AdamW مع تدهور الوزن الصحيح بالإضافة إلى تسخين معدل التعلم وتدهوره. غالبًا ما ينتج هذا المحسن نتائج أفضل قليلاً من تلك التي تحصل عليها مع محسن Adam الافتراضي.

</Tip>

الآن، يمكننا محاولة ملاءمة النموذج بمعدل التعلم الجديد والمحسن:

```python
model.fit(train_dataset)
```

```python out
319/24543 [..............................] - ETA: 16:07 - الخسارة: 0.9718
```

الآن خسارتنا تذهب إلى مكان ما! التدريب يبدو أخيرًا أنه يعمل. هناك درس هنا: عندما يعمل نموذجك ولكن الخسارة لا تنخفض، وأنت متأكد من أن بياناتك على ما يرام، فمن الجيد التحقق من فرط المعاملات مثل معدل التعلم وتدهور الوزن. من المحتمل جدًا أن يؤدي ضبط أي منهما على ارتفاع كبير إلى تسبب التدريب في "التوقف" عند قيمة خسارة مرتفعة.

## قضايا محتملة أخرى[[other-potential-issues]]

لقد غطينا القضايا في السيناريو أعلاه، ولكن هناك العديد من الأخطاء الشائعة الأخرى التي قد تواجهها. دعنا نلقي نظرة على قائمة (غير مكتملة للغاية).

### التعامل مع أخطاء نفاد الذاكرة[[dealing-with-out-of-memory-errors]]

العلامة المميزة لنفاد الذاكرة هي خطأ مثل "OOM عند تخصيص tensor" -- OOM اختصار لـ "نفاد الذاكرة." هذا خطر شائع جدًا عند التعامل مع نماذج اللغة الكبيرة. إذا واجهت ذلك، فإن الاستراتيجية الجيدة هي تقليل حجم دفعتك إلى النصف والمحاولة مرة أخرى. ولكن ضع في اعتبارك، مع ذلك، أن بعض النماذج كبيرة جدًا. على سبيل المثال، يحتوي GPT-2 بالحجم الكامل على 1.5 مليار معامل، مما يعني أنك ستحتاج إلى 6 جيجابايت من الذاكرة لتخزين النموذج فقط، و 6 جيجابايت أخرى لمشتقاته! عادة ما يتطلب تدريب نموذج GPT-2 الكامل أكثر من 20 جيجابايت من ذاكرة VRAM بغض النظر عن حجم الدفعة الذي تستخدمه، والذي يمتلكه عدد قليل من وحدات معالجة الرسومات. النماذج الأكثر خفة مثل `distilbert-base-cased` أسهل في التشغيل، وتدريبها أسرع أيضًا.

<Tip>

في الجزء التالي من الدورة التدريبية، سنلقي نظرة على تقنيات أكثر تقدمًا يمكن أن تساعدك في تقليل بصمة الذاكرة الخاصة بك والسماح لك بضبط أكبر النماذج.

</Tip>

### TensorFlow الجائع 🦛[[hungry-hungry-tensorflow]]

هناك خاصية معينة في TensorFlow يجب أن تكون على دراية بها وهي أنه يقوم بتخصيص *كل* ذاكرة GPU الخاصة بك لنفسه بمجرد تحميل نموذج أو القيام بأي تدريب، ثم يقسم تلك الذاكرة حسب الحاجة. هذا يختلف عن سلوك الأطر الأخرى، مثل PyTorch، والتي تقوم بتخصيص الذاكرة حسب الحاجة باستخدام CUDA بدلاً من القيام بذلك داخليًا. إحدى مزايا نهج TensorFlow هي أنه يمكنه غالبًا إعطاء أخطاء مفيدة عندما تنفد الذاكرة، ويمكنه التعافي من تلك الحالة دون تحطم نواة CUDA بالكامل. ولكن هناك أيضًا جانب سلبي مهم: إذا قمت بتشغيل عمليتي TensorFlow في وقت واحد، فـ **ستمر بوقت سيء**.

إذا كنت تعمل على Colab، فلا داعي للقلق بشأن ذلك، ولكن إذا كنت تعمل محليًا، فهذا بالتأكيد شيء يجب أن تكون حذرًا بشأنه. على وجه الخصوص، كن على دراية بأن إغلاق علامة تبويب الدفتر لا يؤدي بالضرورة إلى إيقاف تشغيل هذا الدفتر! قد تحتاج إلى تحديد الدفاتر قيد التشغيل (التي تحمل أيقونة خضراء) وإيقاف تشغيلها يدويًا في قائمة الدليل. قد يحتفظ أي دفتر ملاحظات قيد التشغيل والذي كان يستخدم TensorFlow بمجموعة من ذاكرة GPU الخاصة بك، وهذا يعني أن أي دفتر ملاحظات جديد تبدأ به قد يواجه بعض القضايا الغريبة جدًا.

إذا بدأت في الحصول على أخطاء حول CUDA أو BLAS أو cuBLAS في الكود الذي كان يعمل من قبل، فهذا غالبًا ما يكون المتهم. يمكنك استخدام أمر مثل `nvidia-smi` للتحقق -- عندما تقوم بإيقاف تشغيل أو إعادة تشغيل دفتر ملاحظاتك الحالي، هل معظم ذاكرتك حرة، أم أنها لا تزال قيد الاستخدام؟ إذا كانت لا تزال قيد الاستخدام، فهناك شيء آخر يحتفظ بها!


### تحقق من بياناتك (مرة أخرى!)[[check-your-data-again]]

لن يتعلم نموذجك شيئًا إلا إذا كان من الممكن بالفعل تعلم أي شيء من بياناتك. إذا كان هناك خطأ يؤدي إلى تلف البيانات أو تم تعيين العلامات بشكل عشوائي، فمن المحتمل جدًا ألا تحصل على أي تدريب للنموذج على مجموعة بياناتك. هناك أداة مساعدة مفيدة هنا وهي `tokenizer.decode()`. سيحول هذا `input_ids` مرة أخرى إلى سلاسل، بحيث يمكنك عرض البيانات ومعرفة ما إذا كانت بيانات التدريب تعلم ما تريد أن تعلمه. على سبيل المثال، بعد حصولك على `batch` من `tf.data.Dataset` كما فعلنا أعلاه، يمكنك فك ترميز العنصر الأول على النحو التالي:

```py
input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])
```

ثم يمكنك مقارنته بالعلامة الأولى، كما يلي:

```py
labels = batch["labels"].numpy()
label = labels[0]
```

بمجرد أن تتمكن من عرض بياناتك بهذه الطريقة، يمكنك أن تسأل نفسك الأسئلة التالية:

- هل البيانات المفككة مفهومة؟
- هل توافق على التصنيفات؟
- هل هناك تصنيف واحد أكثر شيوعًا من التصنيفات الأخرى؟
- ما الذي يجب أن يكون عليه الخسارة/المقياس إذا تنبأ النموذج بإجابة عشوائية/نفس الإجابة دائمًا؟

بعد النظر في بياناتك، قم بالمرور على بعض تنبؤات النموذج -- إذا كان نموذجك ينتج رموز، حاول فك ترميزها أيضًا! إذا كان النموذج يتنبأ دائمًا بنفس الشيء، فقد يكون ذلك لأن مجموعة بياناتك متحيزة نحو فئة واحدة (لمشاكل التصنيف)، لذا فإن التقنيات مثل الإفراط في أخذ العينات من الفئات النادرة قد تساعد. أو قد يكون ذلك بسبب مشاكل التدريب مثل إعدادات فرط المعاملات السيئة.

إذا كانت الخسارة/المقياس الذي تحصل عليه في نموذجك الأولي قبل أي تدريب مختلف جدًا عن الخسارة/المقياس الذي تتوقعه للتنبؤات العشوائية، تحقق مرتين من طريقة حساب الخسارة أو المقياس، حيث من المحتمل وجود خطأ هناك. إذا كنت تستخدم عدة خسائر تضيفها في النهاية، تأكد من أنها بنفس المقياس.

عندما تكون متأكدًا من أن بياناتك مثالية، يمكنك أن ترى ما إذا كان النموذج قادرًا على التدريب عليها باختبار بسيط واحد.

### قم بضبط نموذجك على دفعة واحدة [[overfit-your-model-on-one-batch]]

عادة ما يكون الضبط الزائد شيئًا نحاول تجنبه عند التدريب، لأنه يعني أن النموذج لا يتعلم التعرف على الميزات العامة التي نريد منه أن يتعلمها، ولكنه بدلاً من ذلك يقوم فقط بحفظ عينات التدريب. ومع ذلك، فإن محاولة تدريب نموذجك على دفعة واحدة مرارًا وتكرارًا هو اختبار جيد للتحقق مما إذا كانت المشكلة كما صغتها يمكن حلها بواسطة النموذج الذي تحاول تدريبه. كما سيساعدك ذلك على رؤية ما إذا كان معدل التعلم الأولي الخاص بك مرتفعًا جدًا.

القيام بذلك بمجرد تحديد نموذجك أمر سهل للغاية؛ فقط احصل على دفعة من بيانات التدريب، ثم عالج تلك الدفعة `batch` كمجموعة بياناتك بالكامل، وقم بضبطها لعدد كبير من العصور:

```py
for batch in train_dataset:
    break

# تأكد من أنك قمت بتشغيل model.compile() وتعيين المحسن الخاص بك،
# وخسارتك/مقاييسك إذا كنت تستخدمها

model.fit(batch, epochs=20)
```

<Tip>

💡 إذا كانت بيانات التدريب غير متوازنة، فتأكد من بناء دفعة من بيانات التدريب تحتوي على جميع التصنيفات.

</Tip>

يجب أن يكون النموذج الناتج ذو نتائج قريبة من الكمال على الدفعة `batch`، مع خسارة تنخفض بسرعة نحو 0 (أو القيمة الدنيا للخسارة التي تستخدمها).

إذا لم تتمكن من الحصول على نتائج مثالية لنموذجك مثل هذا، فهذا يعني أن هناك خطأ ما في طريقة صياغة المشكلة أو بياناتك، لذا يجب عليك إصلاح ذلك. فقط عندما تتمكن من اجتياز اختبار الضبط الزائد يمكنك التأكد من أن نموذجك قادر بالفعل على التعلم.

<Tip warning={true}>

⚠️ سيتعين عليك إعادة إنشاء نموذجك وإعادة تجميعه بعد اختبار الضبط الزائد هذا، لأن النموذج الذي تم الحصول عليه لن يكون قادرًا على التعافي والتعلم من شيء مفيد على مجموعة بياناتك الكاملة.

</Tip>

### لا تضبط أي شيء حتى يكون لديك خط أساس أولي [[dont-tune-anything-until-you-have-a-first-baseline]]

يتم التأكيد دائمًا على الضبط المكثف للمعاملات على أنه الجزء الأصعب في التعلم الآلي، ولكنه مجرد الخطوة الأخيرة لمساعدتك في تحقيق القليل على المقياس. بالطبع، فإن القيم السيئة جدًا لمعاملاتك، مثل استخدام معدل التعلم الافتراضي لآدم 1e-3 مع نموذج المحول، ستجعل التعلم يسير ببطء شديد أو يتوقف تمامًا، ولكن في معظم الأوقات، تكون المعاملات "المعقولة"، مثل معدل التعلم من 1e-5 إلى 5e-5، ستعمل بشكل جيد لإعطائك نتائج جيدة. لذا، لا تبدأ في البحث المكلف والمستهلك للوقت عن المعاملات حتى يكون لديك شيء يتفوق على خط الأساس الذي لديك في مجموعة بياناتك.

بمجرد أن يكون لديك ن جيد بما فيه الكفاية، يمكنك البدء في التعديل قليلاً. لا تحاول إطلاق ألف عملية بفرض معاملات مختلفة، ولكن قارن بين عدد قليل من العمليات بفرض قيم مختلفة لمعامل واحد للحصول على فكرة عن التأثير الأكبر.

إذا كنت تقوم بتعديل النموذج نفسه، فاحرص على إبقائه بسيطًا ولا تحاول أي شيء لا يمكنك تبريره بشكل معقول. تأكد دائمًا من العودة إلى اختبار الضبط الزائد للتحقق من أن تغييرك لم يكن له أي عواقب غير مقصودة.

### اطلب المساعدة [[ask-for-help]]

نأمل أن تكون قد وجدت بعض النصائح في هذا القسم التي ساعدتك على حل مشكلتك، ولكن إذا لم يكن الأمر كذلك، فتذكر أنه يمكنك دائمًا طلب المساعدة من المجتمع على [المنتديات](https://discuss.huggingface.co/).

هنا بعض الموارد الإضافية التي قد تكون مفيدة:

- ["القابلية للتكرار كوسيلة لأفضل الممارسات الهندسية"](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) بواسطة جويل جروس
- ["قائمة مراجعة لتصحيح الأخطاء في الشبكات العصبية"](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) بواسطة سيسيليا شاو
- ["كيفية اختبار وحدة التعلم الآلي"](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) بواسطة تشيس روبرتس
- ["وصفة لتدريب الشبكات العصبية"](http://karpathy.github.io/2019/04/25/recipe/) بواسطة أندري كارباثي

بالطبع، ليست كل مشكلة تواجهها عند تدريب الشبكات العصبية هي خطأك! إذا واجهت شيئًا في مكتبة 🤗 Transformers أو 🤗 Datasets لا يبدو صحيحًا، فقد تكون قد واجهت خطأ. يجب أن تخبرنا بكل شيء عنه، وفي القسم التالي سنشرح بالضبط كيفية القيام بذلك.