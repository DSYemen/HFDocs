# نماذج التسلسل إلى التسلسل[sequence-to-sequence-models]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

تستخدم نماذج المُشفِّر-المُفكِّك (وتسمى أيضًا *نماذج التسلسل إلى التسلسل*) كلا الجزأين من بنية المحول. في كل مرحلة، يمكن لطبقات الانتباه الخاصة بالمُشفِّر الوصول إلى جميع الكلمات في الجملة الأولية، بينما يمكن لطبقات الانتباه الخاصة بالمُفكِّك الوصول فقط إلى الكلمات الموجودة قبل كلمة معينة في المُدخل.

يمكن إجراء التدريب المسبق لهذه النماذج باستخدام أهداف نماذج المُشفِّر أو المُفكِّك، ولكنه عادةً ما يتضمن شيئًا أكثر تعقيدًا بعض الشيء. على سبيل المثال، يتم تدريب [T5](https://huggingface.co/t5-base) مسبقًا عن طريق استبدال امتدادات نصية عشوائية (يمكن أن تحتوي على عدة كلمات) بكلمة خاصة مُقنَّعة واحدة، ويكون الهدف هو التنبؤ بالنص الذي تحل محله هذه الكلمة المُقنَّعة.

تُعد نماذج التسلسل إلى التسلسل الأنسب للمهام التي تدور حول إنشاء جمل جديدة اعتمادًا على مُدخل معين، مثل التلخيص أو الترجمة أو الإجابة على الأسئلة التوليدية.

من بين ممثلي هذه العائلة من النماذج:

- [BART](https://huggingface.co/transformers/model_doc/bart)
- [mBART](https://huggingface.co/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/transformers/model_doc/marian)
- [T5](https://huggingface.co/transformers/model_doc/t5)