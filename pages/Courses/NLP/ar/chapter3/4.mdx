# ุชุฏุฑูุจ ูุงูู [[a-full-training]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

ุงูุขู ุณูุฑู ููููุฉ ุชุญููู ููุณ ุงููุชุงุฆุฌ ุงูุชู ุชูุตููุง ุฅูููุง ูู ุงููุณู ุงูุณุงุจู ุฏูู ุงุณุชุฎุฏุงู ูุฆุฉ `Trainer`. ูุฑุฉ ุฃุฎุฑูุ ููุชุฑุถ ุฃูู ููุช ุจูุนุงูุฌุฉ ุงูุจูุงูุงุช ูู ุงููุณู 2. ูููุง ููู ููุฎุต ูุตูุฑ ูุบุทู ูู ูุง ุณุชุญุชุงุฌ ุฅููู:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### ุงูุงุณุชุนุฏุงุฏ ููุชุฏุฑูุจ [[prepare-for-training]]

ูุจู ูุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจูุงุ ุณูุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ุจุนุถ ุงูุฃุดูุงุก. ุฃูููุง ูู ูุญููุงุช ุงูุจูุงูุงุช ุงูุชู ุณูุณุชุฎุฏููุง ููุชููู ุจูู ุงูุฏูุนุงุช. ูููู ูุจู ุฃู ูุชููู ูู ุชุญุฏูุฏ ูุญููุงุช ุงูุจูุงูุงุช ูุฐูุ ูุญุชุงุฌ ุฅูู ุชุทุจูู ุจุนุถ ุงููุนุงูุฌุฉ ุงููุงุญูุฉ ุนูู `tokenized_datasets`ุ ููุงูุชูุงู ุจุจุนุถ ุงูุฃุดูุงุก ุงูุชู ูุงู ุจูุง `Trainer` ุชููุงุฆููุง. ุนูู ูุฌู ุงูุชุญุฏูุฏุ ูุญุชุงุฌ ุฅูู:

- ุฅุฒุงูุฉ ุงูุฃุนูุฏุฉ ุงูููุงุจูุฉ ููููู ุงูุชู ูุง ูุชููุนูุง ุงููููุฐุฌ (ูุซู ุฃุนูุฏุฉ `sentence1` ู`sentence2`).
- ุฅุนุงุฏุฉ ุชุณููุฉ ุงูุนููุฏ `label` ุฅูู `labels` (ูุฃู ุงููููุฐุฌ ูุชููุน ุฃู ูููู ุงุณู ุงูุญุฌุฉ `labels`).
- ุชุนููู ุชูุณูู ุงููุฌููุนุงุช ุจุญูุซ ุชุนูุฏ ุชูุณูุฑ PyTorch ุจุฏูุงู ูู ุงูููุงุฆู.

ูุฏู `tokenized_datasets` ุทุฑููุฉ ููู ูู ูุฐู ุงูุฎุทูุงุช:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ูููููุง ุจุนุฏ ุฐูู ุงูุชุญูู ูู ุฃู ุงููุชูุฌุฉ ุชุญุชูู ููุท ุนูู ุฃุนูุฏุฉ ุณููุจููุง ูููุฐุฌูุง:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

ุงูุขู ุจุนุฏ ุฃู ุชู ุฐููุ ูููููุง ุจุณูููุฉ ุชุญุฏูุฏ ูุญููุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ููุชุฃูุฏ ุจุณุฑุนุฉ ูู ุนุฏู ูุฌูุฏ ุฎุทุฃ ูู ูุนุงูุฌุฉ ุงูุจูุงูุงุชุ ูููููุง ูุญุต ุฏูุนุฉ ูุซู ูุฐู:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

ูุงุญุธ ุฃู ุงูุฃุดูุงู ุงููุนููุฉ ุณุชููู ูุฎุชููุฉ ููููุงู ุจุงููุณุจุฉ ูู ุนูู ุงูุฃุฑุฌุญ ูุฃููุง ูููุง ุจุชุนููู `shuffle=True` ููุญูู ุงูุจูุงูุงุช ุงูุชุฏุฑูุจู ููููู ุจุงูุชุญุฏูุฏ ุฅูู ุงูุทูู ุงูุฃูุตู ุฏุงุฎู ุงูุฏูุนุฉ.

ุงูุขู ุจุนุฏ ุฃู ุงูุชูููุง ุชูุงููุง ูู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ููุจูุงูุงุช (ูุฏู ููุฑุถู ููููู ุจุนูุฏ ุงูููุงู ูุฃู ููุงุฑุณ ููุชุนูู ุงูุขูู)ุ ุฏุนูุง ููุชูู ุฅูู ุงููููุฐุฌ. ูููู ุจุชูุดูุทู ุจุงูุถุจุท ููุง ูุนููุง ูู ุงููุณู ุงูุณุงุจู:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ููุชุฃูุฏ ูู ุฃู ูู ุดูุก ุณูุณูุฑ ุจุณูุงุณุฉ ุฃุซูุงุก ุงูุชุฏุฑูุจุ ูููู ุจุชูุฑูุฑ ุฏูุนุชูุง ุฅูู ูุฐุง ุงููููุฐุฌ:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ุณุชุนูุฏ ุฌููุน ููุงุฐุฌ ๐ค Transformers ุงูุฎุณุงุฑุฉ ุนูุฏ ุชูููุฑ `labels`ุ ููุญุตู ุฃูุถูุง ุนูู ุงูููุบุงุฑูุชูุงุช (ุงุซูุงู ููู ุฅุฏุฎุงู ูู ุฏูุนุชูุงุ ูุฐุง ูุฅู ุชูุณูุฑ ุจุญุฌู 8 ร 2).

ูุญู ุนูู ุงุณุชุนุฏุงุฏ ุชูุฑูุจูุง ููุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจูุง! ููุชูุฏ ุดูุฆูู ููุท: ูุญุณู ููุฎุทุท ูุนุฏู ุงูุชุนูู. ุญูุซ ุฃููุง ูุญุงูู ุชูุฑุงุฑ ูุง ูุงู ูููู ุจู `Trainer` ูุฏูููุงุ ุณูุณุชุฎุฏู ููุณ ุงูุงูุชุฑุงุถุงุช. ุงููุญุณู ุงูุฐู ูุณุชุฎุฏูู `Trainer` ูู `AdamW`ุ ููู ููุณ Adamุ ูููู ูุน ุชุนุฏูู ูุชูุธูู ุงูุฎูุงุถ ุงููุฒู (ุงูุธุฑ ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) ุจููู ุฅูููุง ููุดุดูููู ููุฑุงูู ููุชูุฑ):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

ุฃุฎูุฑูุงุ ูุฅู ูุฎุทุท ูุนุฏู ุงูุชุนูู ุงููุณุชุฎุฏู ุจุดูู ุงูุชุฑุงุถู ูู ูุฌุฑุฏ ุงูุฎูุงุถ ุฎุทู ูู ุงููููุฉ ุงููุตูู (5e-5) ุฅูู 0. ูุชุญุฏูุฏ ุฐูู ุจุดูู ุตุญูุญุ ูุญุชุงุฌ ุฅูู ูุนุฑูุฉ ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ ุงูุชู ุณูููู ุจูุงุ ููู ุนุฏุฏ ุงูุฏูุฑุงุช ุงูุชู ูุฑูุฏ ุชุดุบูููุง ูุถุฑูุจุฉ ูู ุนุฏุฏ ุฏูุนุงุช ุงูุชุฏุฑูุจ (ูุงูุฐู ูู ุทูู ูุญูู ุงูุจูุงูุงุช ุงูุชุฏุฑูุจู ุงูุฎุงุต ุจูุง). ูุณุชุฎุฏู `Trainer` ุซูุงุซ ุฏูุฑุงุช ุจุดูู ุงูุชุฑุงุถูุ ูุฐูู ุณูุชุจุน ุฐูู:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### ุญููุฉ ุงูุชุฏุฑูุจ [[the-training-loop]]

ุดูุก ูุงุญุฏ ุฃุฎูุฑ: ุณูุฑุบุจ ูู ุงุณุชุฎุฏุงู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุฅุฐุง ูุงู ูุฏููุง ุฅููุงููุฉ ุงููุตูู ุฅูู ูุงุญุฏุฉ (ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉุ ูุฏ ูุณุชุบุฑู ุงูุชุฏุฑูุจ ุนุฏุฉ ุณุงุนุงุช ุจุฏูุงู ูู ุจุถุน ุฏูุงุฆู). ููููุงู ุจุฐููุ ูุญุฏุฏ `device` ุณูุถุน ูููุฐุฌูุง ูุฏูุนุงุชูุง ุนูููุง:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

ูุญู ุงูุขู ูุณุชุนุฏูู ููุชุฏุฑูุจ! ููุญุตูู ุนูู ุจุนุถ ุงูุฅุญุณุงุณ ุจููุนุฏ ุงูุชูุงุก ุงูุชุฏุฑูุจุ ูุถูู ุดุฑูุท ุชูุฏู ููู ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจุ ุจุงุณุชุฎุฏุงู ููุชุจุฉ `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ููููู ุฃู ุชุฑู ุฃู ุฌููุฑ ุญููุฉ ุงูุชุฏุฑูุจ ูุดุจู ุฅูู ุญุฏ ูุจูุฑ ุชูู ุงูููุฌูุฏุฉ ูู ุงูููุฏูุฉ. ูู ูุทูุจ ุฃู ุชูุฑูุฑุ ูุฐุง ูู ุชุฎุจุฑูุง ูุฐู ุงูุญููุฉ ุงูุชุฏุฑูุจูุฉ ุจุฃู ุดูุก ุญูู ููููุฉ ุฃุฏุงุก ุงููููุฐุฌ. ูุญุชุงุฌ ุฅูู ุฅุถุงูุฉ ุญููุฉ ุชูููู ูุฐูู.

### ุญููุฉ ุงูุชูููู [[the-evaluation-loop]]

ููุง ูุนููุง ุณุงุจููุงุ ุณูุณุชุฎุฏู ูููุงุณูุง ูููุฑู ููุชุจุฉ ๐ค Evaluate. ููุฏ ุฑุฃููุง ุจุงููุนู ุทุฑููุฉ `metric.compute()`ุ ูููู ูููู ููููุงููุณ ูู ุงููุงูุน ุชุฑุงูู ุงูุฏูุนุงุช ููุง ุฃุซูุงุก ุงูุงูุชูุงู ุฅูู ุญููุฉ ุงูุชูุจุค ุจุงูุทุฑููุฉ `add_batch()`. ุจูุฌุฑุฏ ุฃู ูููู ูุฏ ุชุฑุงูููุง ุฌููุน ุงูุฏูุนุงุชุ ูููููุง ุงูุญุตูู ุนูู ุงููุชูุฌุฉ ุงูููุงุฆูุฉ ูุน `metric.compute()`. ูููุง ููู ููููุฉ ุชูููุฐ ูู ูุฐุง ูู ุญููุฉ ุชูููู:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

ุณุชุฎุชูู ูุชุงุฆุฌู ููููุงู ุจุณุจุจ ุงูุนุดูุงุฆูุฉ ูู ุชููุฆุฉ ุฑุฃุณ ุงููููุฐุฌ ูุงูุฎูุท ุจูู ุงูุจูุงูุงุชุ ูููููุง ูุฌุจ ุฃู ุชููู ูู ููุณ ุงููุฌุงู.

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ุนุฏู ุญููุฉ ุงูุชุฏุฑูุจ ุงูุณุงุจูุฉ ูุถุจุท ูููุฐุฌู ุนูู ูุฌููุนุฉ ุจูุงูุงุช SST-2.

</Tip>

### ูู ุจุชุนุฒูุฒ ุญููุฉ ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจู ูุน ๐ค Accelerate[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

ุญููุฉ ุงูุชุฏุฑูุจ ุงูุชู ูููุง ุจุชุนุฑูููุง ุณุงุจูุงู ุชุนูู ุจุดูู ุฌูุฏ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ูุฑูุฒูุฉ ูุงุญุฏุฉ ุฃู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช. ูููู ุจุงุณุชุฎุฏุงู ููุชุจุฉ [๐ค Accelerate](https://github.com/huggingface/accelerate)ุ ูุน ุจุนุถ ุงูุชุนุฏููุงุช ุงูุจุณูุทุฉ ูููููุง ุชูููู ุงูุชุฏุฑูุจ ุงูููุฒุน ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ุฃู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ุงููุงุจูุฉ ููุจุฑูุฌุฉ. ุจุฏุกุงู ูู ุฅูุดุงุก ูุญููุงุช ุงูุจูุงูุงุช ููุชุฏุฑูุจ ูุงูุชุญููุ ุฅููู ูุง ุชุจุฏู ุนููู ุญููุฉ ุงูุชุฏุฑูุจ ุงููุฏููุฉ ุงูุฎุงุตุฉ ุจูุง:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ูููุง ุงูุชุบููุฑุงุช:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

ุงูุฎุท ุงูุฃูู ุงูุฐู ูุฌุจ ุฅุถุงูุชู ูู ุณุทุฑ ุงูุงุณุชูุฑุงุฏ. ุงูุณุทุฑ ุงูุซุงูู ููุดุฆ ูุงุฆู `Accelerator` ุงูุฐู ุณููุธุฑ ุฅูู ุงูุจูุฆุฉ ููุจุฏุฃ ุงูุฅุนุฏุงุฏ ุงูููุฒุน ุงูููุงุณุจ. ๐ค Accelerate ูุชุนุงูู ูุน ูุถุน ุงูุฌูุงุฒ ููุ ูุฐุง ููููู ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงูุชู ุชุถุน ุงููููุฐุฌ ุนูู ุงูุฌูุงุฒ (ุฃูุ ุฅุฐุง ููุช ุชูุถูุ ููููู ุชุบููุฑูุง ูุงุณุชุฎุฏุงู `accelerator.device` ุจุฏูุงู ูู `device`).

ุซู ูุชู ุฅูุฌุงุฒ ุงูุฌุฒุก ุงูุฃูุจุฑ ูู ุงูุนูู ูู ุงูุณุทุฑ ุงูุฐู ูุฑุณู ูุญููุงุช ุงูุจูุงูุงุช ูุงููููุฐุฌ ูุงูููุญุณูู ุฅูู `accelerator.prepare()`. ุณูููู ูุฐุง ุจุชุบููู ุชูู ุงููุงุฆูุงุช ูู ุงูุญุงููุฉ ุงูููุงุณุจุฉ ููุชุฃูุฏ ูู ุฃู ุงูุชุฏุฑูุจ ุงูููุฒุน ูุนูู ููุง ูู ููุตูุฏ. ุงูุชุบููุฑุงุช ุงููุชุจููุฉ ุงูุชู ูุฌุจ ุฅุฌุฑุงุคูุง ูู ุฅุฒุงูุฉ ุงูุณุทุฑ ุงูุฐู ูุถุน ุงูุฏูุนุฉ ุนูู `device` (ูุฑุฉ ุฃุฎุฑูุ ุฅุฐุง ููุช ุชุฑูุฏ ุงูุงุญุชูุงุธ ุจูุฐุงุ ููููู ุชุบููุฑู ุจุจุณุงุทุฉ ูุงุณุชุฎุฏุงู `accelerator.device`) ูุงุณุชุจุฏุงู `loss.backward()` ุจู `accelerator.backward(loss)`.

<Tip>
โ๏ธ ููุงุณุชูุงุฏุฉ ูู ุงูุชุณุฑูุน ุงูุฐู ุชูุฏูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ุงูุณุญุงุจูุฉุ ููุตู ุจููุก ุนููุงุชู ุจุทูู ุซุงุจุช ุจุงุณุชุฎุฏุงู ุญุฌุฌ `padding="max_length"` ู `max_length` ูู ุงููุญูู ุงููุบูู.
</Tip>

ุฅุฐุง ููุช ุชุฑุบุจ ูู ูุณุฎูุง ููุตููุง ููุนุจ ุจูุงุ ุฅููู ูุง ุชุจุฏู ุนููู ุญููุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉ ูุน ๐ค Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ูุถุน ูุฐุง ูู ูุต ุจุฑูุฌู `train.py` ุณูุฌุนู ูุฐุง ุงููุต ุงูุจุฑูุฌู ูุงุจูุงู ููุชุดุบูู ุนูู ุฃู ููุน ูู ุงูุฅุนุฏุงุฏุงุช ุงูููุฒุนุฉ. ูุชุฌุฑุจุชู ูู ุฅุนุฏุงุฏู ุงูููุฒุนุ ูู ุจุชุดุบูู ุงูุฃูุฑ:

```bash
accelerate config
```

ุงูุฐู ุณูุทูุจ ููู ุงูุฅุฌุงุจุฉ ุนูู ุจุนุถ ุงูุฃุณุฆูุฉ ูุฅููุงุก ุฅุฌุงุจุงุชู ูู ููู ุชูููู ูุณุชุฎุฏูู ูุฐุง ุงูุฃูุฑ:

```
accelerate launch train.py
```

ุงูุฐู ุณูุจุฏุฃ ุงูุชุฏุฑูุจ ุงูููุฒุน.

ุฅุฐุง ููุช ุชุฑูุฏ ุชุฌุฑุจุฉ ูุฐุง ูู ุฏูุชุฑ ููุงุญุธุงุช (ุนูู ุณุจูู ุงููุซุงูุ ูุงุฎุชุจุงุฑู ูุน ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ุงููุงุจูุฉ ููุจุฑูุฌุฉ ุนูู Colab)ุ ููุท ูู ุจูุตู ุงูููุฏ ูู `training_function()` ููู ุจุชุดุบูู ุงูุฎููุฉ ุงูุฃุฎูุฑุฉ ูุน:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

ููููู ุงูุนุซูุฑ ุนูู ุงููุฒูุฏ ูู ุงูุฃูุซูุฉ ูู [ูุณุชูุฏุน ๐ค Accelerate](https://github.com/huggingface/accelerate/tree/main/examples).