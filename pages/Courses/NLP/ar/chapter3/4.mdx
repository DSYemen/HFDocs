# تدريب كامل [[a-full-training]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

الآن سنرى كيفية تحقيق نفس النتائج التي توصلنا إليها في القسم السابق دون استخدام فئة `Trainer`. مرة أخرى، نفترض أنك قمت بمعالجة البيانات في القسم 2. فيما يلي ملخص قصير يغطي كل ما ستحتاج إليه:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### الاستعداد للتدريب [[prepare-for-training]]

قبل كتابة حلقة التدريب الخاصة بنا، سنحتاج إلى تحديد بعض الأشياء. أولها هي محملات البيانات التي سنستخدمها للتنقل بين الدفعات. ولكن قبل أن نتمكن من تحديد محملات البيانات هذه، نحتاج إلى تطبيق بعض المعالجة اللاحقة على `tokenized_datasets`، للاهتمام ببعض الأشياء التي قام بها `Trainer` تلقائيًا. على وجه التحديد، نحتاج إلى:

- إزالة الأعمدة المقابلة للقيم التي لا يتوقعها النموذج (مثل أعمدة `sentence1` و`sentence2`).
- إعادة تسمية العمود `label` إلى `labels` (لأن النموذج يتوقع أن يكون اسم الحجة `labels`).
- تعيين تنسيق المجموعات بحيث تعيد تنسور PyTorch بدلاً من القوائم.

لدى `tokenized_datasets` طريقة لكل من هذه الخطوات:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

يمكننا بعد ذلك التحقق من أن النتيجة تحتوي فقط على أعمدة سيقبلها نموذجنا:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

الآن بعد أن تم ذلك، يمكننا بسهولة تحديد محملات البيانات الخاصة بنا:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

للتأكد بسرعة من عدم وجود خطأ في معالجة البيانات، يمكننا فحص دفعة مثل هذه:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

لاحظ أن الأشكال الفعلية ستكون مختلفة قليلاً بالنسبة لك على الأرجح لأننا قمنا بتعيين `shuffle=True` لمحمل البيانات التدريبي ونقوم بالتحديد إلى الطول الأقصى داخل الدفعة.

الآن بعد أن انتهينا تمامًا من المعالجة المسبقة للبيانات (هدف مُرضٍ ولكنه بعيد المنال لأي ممارس للتعلم الآلي)، دعنا ننتقل إلى النموذج. نقوم بتنشيطه بالضبط كما فعلنا في القسم السابق:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

للتأكد من أن كل شيء سيسير بسلاسة أثناء التدريب، نقوم بتمرير دفعتنا إلى هذا النموذج:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ستعيد جميع نماذج 🤗 Transformers الخسارة عند توفير `labels`، ونحصل أيضًا على اللوغاريتمات (اثنان لكل إدخال في دفعتنا، لذا فإن تنسور بحجم 8 × 2).

نحن على استعداد تقريبًا لكتابة حلقة التدريب الخاصة بنا! نفتقد شيئين فقط: محسن ومخطط معدل التعلم. حيث أننا نحاول تكرار ما كان يقوم به `Trainer` يدويًا، سنستخدم نفس الافتراضات. المحسن الذي يستخدمه `Trainer` هو `AdamW`، وهو نفس Adam، ولكن مع تعديل لتنظيم انخفاض الوزن (انظر ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) بقلم إيليا لوششيلوف وفرانك هوتير):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

أخيرًا، فإن مخطط معدل التعلم المستخدم بشكل افتراضي هو مجرد انخفاض خطي من القيمة القصوى (5e-5) إلى 0. لتحديد ذلك بشكل صحيح، نحتاج إلى معرفة عدد خطوات التدريب التي سنقوم بها، وهو عدد الدورات التي نريد تشغيلها مضروبة في عدد دفعات التدريب (والذي هو طول محمل البيانات التدريبي الخاص بنا). يستخدم `Trainer` ثلاث دورات بشكل افتراضي، لذلك سنتبع ذلك:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### حلقة التدريب [[the-training-loop]]

شيء واحد أخير: سنرغب في استخدام وحدة معالجة الرسومات إذا كان لدينا إمكانية الوصول إلى واحدة (على وحدة المعالجة المركزية، قد يستغرق التدريب عدة ساعات بدلاً من بضع دقائق). للقيام بذلك، نحدد `device` سنضع نموذجنا ودفعاتنا عليها:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

نحن الآن مستعدون للتدريب! للحصول على بعض الإحساس بموعد انتهاء التدريب، نضيف شريط تقدم فوق عدد خطوات التدريب، باستخدام مكتبة `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

يمكنك أن ترى أن جوهر حلقة التدريب يشبه إلى حد كبير تلك الموجودة في المقدمة. لم نطلب أي تقرير، لذا لن تخبرنا هذه الحلقة التدريبية بأي شيء حول كيفية أداء النموذج. نحتاج إلى إضافة حلقة تقييم لذلك.

### حلقة التقييم [[the-evaluation-loop]]

كما فعلنا سابقًا، سنستخدم مقياسًا يوفره مكتبة 🤗 Evaluate. لقد رأينا بالفعل طريقة `metric.compute()`، ولكن يمكن للمقاييس في الواقع تراكم الدفعات لنا أثناء الانتقال إلى حلقة التنبؤ بالطريقة `add_batch()`. بمجرد أن نكون قد تراكمنا جميع الدفعات، يمكننا الحصول على النتيجة النهائية مع `metric.compute()`. فيما يلي كيفية تنفيذ كل هذا في حلقة تقييم:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

ستختلف نتائجك قليلاً بسبب العشوائية في تهيئة رأس النموذج والخلط بين البيانات، ولكنها يجب أن تكون في نفس المجال.

<Tip>

✏️ **جربها!** عدل حلقة التدريب السابقة لضبط نموذجك على مجموعة بيانات SST-2.

</Tip>

### قم بتعزيز حلقة التدريب الخاصة بك مع 🤗 Accelerate[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

حلقة التدريب التي قمنا بتعريفها سابقاً تعمل بشكل جيد على وحدة معالجة مركزية واحدة أو وحدة معالجة الرسوميات. ولكن باستخدام مكتبة [🤗 Accelerate](https://github.com/huggingface/accelerate)، مع بعض التعديلات البسيطة يمكننا تمكين التدريب الموزع على وحدات معالجة الرسوميات أو وحدات معالجة الرسوميات القابلة للبرمجة. بدءاً من إنشاء محملات البيانات للتدريب والتحقق، إليك ما تبدو عليه حلقة التدريب اليدوية الخاصة بنا:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

وهنا التغييرات:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

الخط الأول الذي يجب إضافته هو سطر الاستيراد. السطر الثاني ينشئ كائن `Accelerator` الذي سينظر إلى البيئة ويبدأ الإعداد الموزع المناسب. 🤗 Accelerate يتعامل مع وضع الجهاز لك، لذا يمكنك إزالة الأسطر التي تضع النموذج على الجهاز (أو، إذا كنت تفضل، يمكنك تغييرها لاستخدام `accelerator.device` بدلاً من `device`).

ثم يتم إنجاز الجزء الأكبر من العمل في السطر الذي يرسل محملات البيانات والنموذج والمُحسّن إلى `accelerator.prepare()`. سيقوم هذا بتغليف تلك الكائنات في الحاوية المناسبة للتأكد من أن التدريب الموزع يعمل كما هو مقصود. التغييرات المتبقية التي يجب إجراؤها هي إزالة السطر الذي يضع الدفعة على `device` (مرة أخرى، إذا كنت تريد الاحتفاظ بهذا، يمكنك تغييره ببساطة لاستخدام `accelerator.device`) واستبدال `loss.backward()` بـ `accelerator.backward(loss)`.

<Tip>
⚠️ للاستفادة من التسريع الذي تقدمه وحدات معالجة الرسوميات السحابية، نوصي بملء عيناتك بطول ثابت باستخدام حجج `padding="max_length"` و `max_length` من المحلل اللغوي.
</Tip>

إذا كنت ترغب في نسخها ولصقها للعب بها، إليك ما تبدو عليه حلقة التدريب الكاملة مع 🤗 Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

وضع هذا في نص برمجي `train.py` سيجعل هذا النص البرمجي قابلاً للتشغيل على أي نوع من الإعدادات الموزعة. لتجربته في إعدادك الموزع، قم بتشغيل الأمر:

```bash
accelerate config
```

الذي سيطلب منك الإجابة على بعض الأسئلة وإلقاء إجاباتك في ملف تكوين يستخدمه هذا الأمر:

```
accelerate launch train.py
```

الذي سيبدأ التدريب الموزع.

إذا كنت تريد تجربة هذا في دفتر ملاحظات (على سبيل المثال، لاختباره مع وحدات معالجة الرسوميات القابلة للبرمجة على Colab)، فقط قم بلصق الكود في `training_function()` وقم بتشغيل الخلية الأخيرة مع:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

يمكنك العثور على المزيد من الأمثلة في [مستودع 🤗 Accelerate](https://github.com/huggingface/accelerate/tree/main/examples).