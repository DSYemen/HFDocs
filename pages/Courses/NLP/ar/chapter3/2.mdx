<FrameworkSwitchCourse {fw} />

# ูุนุงูุฌุฉ ุงูุจูุงูุงุช[[processing-the-data]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
ุงุณุชููุงูุงู ูููุซุงู ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ุฅููู ููููุฉ ุชุฏุฑูุจ ูุตูู ุชุณูุณู ุนูู ุฏูุนุฉ ูุงุญุฏุฉ ูู PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# ููุง ูู ุงูุณุงุจู
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# ูุฐุง ุฌุฏูุฏ
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
ุงุณุชููุงูุงู ูููุซุงู ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ุฅููู ููููุฉ ุชุฏุฑูุจ ูุตูู ุชุณูุณู ุนูู ุฏูุนุฉ ูุงุญุฏุฉ ูู TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# ููุง ูู ุงูุณุงุจู
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# ูุฐุง ุฌุฏูุฏ
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

ุจุงูุทุจุนุ ุชุฏุฑูุจ ุงููููุฐุฌ ุนูู ุฌููุชูู ููุท ูู ูุนุทู ูุชุงุฆุฌ ุฌูุฏุฉ. ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถูุ ุณุชุญุชุงุฌ ุฅูู ุฅุนุฏุงุฏ ูุฌููุนุฉ ุจูุงูุงุช ุฃูุจุฑ.

ูู ูุฐุง ุงููุณูุ ุณูุณุชุฎุฏู ููุซุงู ูุฌููุนุฉ ุจูุงูุงุช MRPC (Microsoft Research Paraphrase Corpus) ุงูุชู ุชู ุชูุฏูููุง ูู [ูุฑูุฉ ุจุญุซูุฉ](https://www.aclweb.org/anthology/I05-5002.pdf) ูู ูุจู William B. Dolan ู Chris Brockett. ุชุชููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู 5,801 ุฒูุฌ ูู ุงูุฌููุ ูุน ูุฌูุฏ ุนูุงูุฉ ุชุดูุฑ ุฅูู ูุง ุฅุฐุง ูุงูุช ูุชุดุงุจูุฉ ูู ุงููุนูู ุฃู ูุง (ุฃูุ ุฅุฐุง ูุงู ููุง ุงูุฌููุชูู ุชุนููุงู ููุณ ุงูุดูุก). ููุฏ ุงุฎุชุฑูุงูุง ููุฐุง ุงููุตู ูุฃููุง ูุฌููุนุฉ ุจูุงูุงุช ุตุบูุฑุฉุ ููุง ูุฌุนู ูู ุงูุณูู ุฅุฌุฑุงุก ุงูุชุฌุงุฑุจ ุนูููุง.

### ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ูู ุงููุฑูุฒ[[loading-a-dataset-from-the-hub]]

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

ุงููุฑูุฒ ูุง ูุญุชูู ููุท ุนูู ุงูููุงุฐุฌุ ุจู ูุญุชูู ุฃูุถูุง ุนูู ูุฌููุนุงุช ุจูุงูุงุช ูุชุนุฏุฏุฉ ุจูุบุงุช ูุฎุชููุฉ. ููููู ุชุตูุญ ูุฌููุนุงุช ุงูุจูุงูุงุช [ููุง](https://huggingface.co/datasets)ุ ููุญู ููุตู ุจุชุฌุฑุจุฉ ุชุญููู ููุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ ุจุนุฏ ุงูุงูุชูุงุก ูู ูุฐุง ุงููุณู (ุฑุงุฌุน ุงููุซุงุฆู ุงูุนุงูุฉ [ููุง](https://huggingface.co/docs/datasets/loading)). ูููู ุงูุขูุ ุฏุนูุง ูุฑูุฒ ุนูู ูุฌููุนุฉ ุจูุงูุงุช MRPC! ูุฐู ูุงุญุฏุฉ ูู 10 ูุฌููุนุงุช ุจูุงูุงุช ุชุดูู [ูุนูุงุฑ GLUE](https://gluebenchmark.com/)ุ ููู ูุนูุงุฑ ุฃูุงุฏููู ูุณุชุฎุฏู ูููุงุณ ุฃุฏุงุก ููุงุฐุฌ ML ุนุจุฑ 10 ููุงู ูุฎุชููุฉ ูุชุตููู ุงููุตูุต.

ุชููุฑ ููุชุจุฉ ๐ค Datasets ุฃูุฑูุง ุจุณูุทูุง ุฌุฏูุง ูุชุญููู ูุชุฎุฒูู ูุฌููุนุฉ ุจูุงูุงุช ุนูู ุงููุฑูุฒ. ูููููุง ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช MRPC ุนูู ุงููุญู ุงูุชุงูู:

<Tip>
โ๏ธ **ุชุญุฐูุฑ** ุชุฃูุฏ ูู ุชุซุจูุช `datasets` ุนู ุทุฑูู ุชุดุบูู `pip install datasets`. ุซู ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช MRPC ูุทุจุงุนุชูุง ููุนุฑูุฉ ูุญุชููุงุชูุง.
</Tip> 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

ููุง ุชุฑูุ ูุญุตู ุนูู ูุงุฆู `DatasetDict` ุงูุฐู ูุญุชูู ุนูู ูุฌููุนุฉ ุงูุชุฏุฑูุจุ ููุฌููุนุฉ ุงูุชุญููุ ููุฌููุนุฉ ุงูุงุฎุชุจุงุฑ. ูุญุชูู ูู ูููุง ุนูู ุนุฏุฉ ุฃุนูุฏุฉ (`sentence1`ุ `sentence2`ุ `label`ุ ู`idx`) ูุนุฏุฏ ูุชุบูุฑ ูู ุงูุตูููุ ููู ุนุฏุฏ ุงูุนูุงุตุฑ ูู ูู ูุฌููุนุฉ (ูุฐููุ ููุงู 3,668 ุฒูุฌ ูู ุงูุฌูู ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจุ ู408 ูู ูุฌููุนุฉ ุงูุชุญููุ ู1,725 ูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ).

ูููู ูุฐุง ุงูุฃูุฑ ุจุชุญููู ูุชุฎุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ุจุดูู ุงูุชุฑุงุถู ูู *~/.cache/huggingface/datasets*. ุชุฐูุฑ ูู ุงููุตู 2 ุฃูู ููููู ุชุฎุตูุต ูุฌูุฏ ุงูุชุฎุฒูู ุงููุคูุช ุงูุฎุงุต ุจู ุนู ุทุฑูู ุชุนููู ูุชุบูุฑ ุงูุจูุฆุฉ `HF_HOME`.

ูููููุง ุงููุตูู ุฅูู ูู ุฒูุฌ ูู ุงูุฌูู ูู ูุงุฆู `raw_datasets` ุงูุฎุงุต ุจูุง ุนู ุทุฑูู ุงูููุฑุณุฉุ ูุซู ุงููุงููุณ:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

ูููููุง ุฃู ูุฑู ุฃู ุงูุนูุงูุงุช ูู ุจุงููุนู ุฃุนุฏุงุฏ ุตุญูุญุฉุ ูุฐูู ูู ูุญุชุงุฌ ุฅูู ุฅุฌุฑุงุก ุฃู ูุนุงูุฌุฉ ูุณุจูุฉ ููุงู. ููุนุฑูุฉ ุฃู ุนุฏุฏ ุตุญูุญ ููุงุจู ุฃู ุนูุงูุฉุ ูููููุง ูุญุต `features` ุงูุฎุงุตุฉ ุจู `raw_train_dataset`. ุณูุฎุจุฑูุง ูุฐุง ุจููุน ูู ุนููุฏ:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

ูู ุงูุฎูููุฉุ `label` ูู ููุน `ClassLabel`ุ ููุชู ุชุฎุฒูู ุชุนููู ุงูุฃุนุฏุงุฏ ุงูุตุญูุญุฉ ุฅูู ุงุณู ุงูุนูุงูุฉ ูู ูุฌูุฏ *names*. `0` ููุงุจู `not_equivalent`ุ ู`1` ููุงุจู `equivalent`.

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ุงูุธุฑ ุฅูู ุงูุนูุตุฑ 15 ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ูุงูุนูุตุฑ 87 ูู ูุฌููุนุฉ ุงูุชุญูู. ูุง ูู ุนูุงูุงุชูุงุ

</Tip>

### ูุนุงูุฌุฉ ูุณุจูุฉ ููุฌููุนุฉ ุจูุงูุงุช[[preprocessing-a-dataset]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ููุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุณุจููุงุ ูุญุชุงุฌ ุฅูู ุชุญููู ุงููุต ุฅูู ุฃุฑูุงู ูููู ูููููุฐุฌ ููููุง. ููุง ุฑุฃูุช ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ูุชู ุฐูู ุจุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุชุฌุฒุฆุฉ ุงููููุงุช. ูููููุง ุฅุฏุฎุงู ุฃุฏุงุฉ ุชุฌุฒุฆุฉ ุงููููุงุช ุฌููุฉ ูุงุญุฏุฉ ุฃู ูุงุฆูุฉ ูู ุงูุฌููุ ูุฐูู ูููููุง ุชุฌุฒุฆุฉ ุฌููุน ุงูุฌูู ุงูุฃููู ูุฌููุน ุงูุฌูู ุงูุซุงููุฉ ูู ูู ุฒูุฌ ุนูู ุงููุญู ุงูุชุงูู:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ููุน ุฐููุ ูุง ูููููุง ููุท ุชูุฑูุฑ ุชุณูุณููู ุฅูู ุงููููุฐุฌ ูุงูุญุตูู ุนูู ุชูุจุค ุจูุง ุฅุฐุง ูุงูุช ุงูุฌููุชูู ูุชุดุงุจูุชูู ูู ุงููุนูู ุฃู ูุง. ูุญุชุงุฌ ุฅูู ุงูุชุนุงูู ูุน ุงูุชุณูุณููู ูุฒูุฌุ ูุชุทุจูู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูููุงุณุจุฉ. ูุญุณู ุงูุญุธุ ูููู ูุฃุฏุงุฉ ุชุฌุฒุฆุฉ ุงููููุงุช ุฃูุถูุง ุฃุฎุฐ ุฒูุฌ ูู ุงูุชุณูุณูุงุช ูุฅุนุฏุงุฏู ุจุงูุทุฑููุฉ ุงูุชู ูุชููุนูุง ูููุฐุฌ BERT ุงูุฎุงุต ุจูุง: 

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
ูุงูุดูุง ููุงุชูุญ `input_ids` ู `attention_mask` ูู [ุงููุตู 2](/course/chapter2)ุ ููููุง ุฃุฌููุง ุงูุญุฏูุซ ุนู `token_type_ids`. ูู ูุฐุง ุงููุซุงูุ ูุฐุง ูุง ูุฎุจุฑ ุงููููุฐุฌ ุจุฃู ุฌุฒุก ูู ุงูุฅุฏุฎุงู ูู ุงูุฌููุฉ ุงูุฃููู ูุฃููุง ุงูุฌููุฉ ุงูุซุงููุฉ.

<ูุตูุญุฉ>

โ๏ธ **ุฌุฑุจูุง!** ุฎุฐ ุงูุนูุตุฑ 15 ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ููู ุจุชูุณูู ุงูุฌููุชูู ุฅูู ุฑููุฒ ูููุตูุฉ ููุฒูุฌ. ูุง ูู ุงููุฑู ุจูู ุงููุชูุฌุชููุ

</ูุตูุญุฉ>

ุฅุฐุง ูููุง ุจูู ุชุดููุฑ ุงูู IDs ุฏุงุฎู `input_ids` ุฅูู ูููุงุช:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

ุณูุญุตู ุนูู:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

ูุฐุง ูุฑู ุฃู ุงููููุฐุฌ ูุชููุน ุฃู ุชููู ุงููุฏุฎูุงุช ุนูู ุงูุดูู `[CLS] sentence1 [SEP] sentence2 [SEP]` ุนูุฏูุง ูููู ููุงู ุฌููุชูู. ูููุงุกูุฉ ูุฐุง ูุน `token_type_ids` ูุนุทููุง:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

ููุง ุชุฑูุ ูุฅู ุฃุฌุฒุงุก ุงูุฅุฏุฎุงู ุงูููุงุจูุฉ ูู `[CLS] sentence1 [SEP]` ูุฏููุง ุฌููุนูุง ูุนุฑู ููุน ุงูุฑูุฒ `0`ุ ุจูููุง ุงูุฃุฌุฒุงุก ุงูุฃุฎุฑูุ ุงูููุงุจูุฉ ูู `sentence2 [SEP]`ุ ูุฏููุง ุฌููุนูุง ูุนุฑู ููุน ุงูุฑูุฒ `1`.

ูุงุญุธ ุฃูู ุฅุฐุง ููุช ุจุงุฎุชูุงุฑ ููุทุฉ ุชูุชูุด ูุฎุชููุฉุ ููู ูููู ูุฏูู ุจุงูุถุฑูุฑุฉ `token_type_ids` ูู ูุฏุฎูุงุชู ุงููุฑูุฒุฉ (ุนูู ุณุจูู ุงููุซุงูุ ูุง ูุชู ุฅุฑุฌุงุนูุง ุฅุฐุง ููุช ุชุณุชุฎุฏู ูููุฐุฌ DistilBERT). ูุชู ุฅุฑุฌุงุนูุง ููุท ุนูุฏูุง ุณูุนุฑู ุงููููุฐุฌ ูุง ุงูุฐู ูุฌุจ ูุนูู ุจูุงุ ูุฃูู ุฑุขูุง ุฎูุงู ุงูุชุฏุฑูุจ ุงููุณุจู.

ููุงุ ูุชู ุชุฏุฑูุจ BERT ูุณุจููุง ุจูุนุฑูุงุช ููุน ุงูุฑูุฒุ ูุจุงูุฅุถุงูุฉ ุฅูู ูุฏู ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ ุงูุฐู ุชุญุฏุซูุง ุนูู ูู [ุงููุตู 1](/course/chapter1)ุ ูุฏูู ูุฏู ุฅุถุงูู ูุณูู _ุชููุน ุงูุฌููุฉ ุงูุชุงููุฉ_. ุงููุฏู ูู ูุฐู ุงููููุฉ ูู ููุฐุฌุฉ ุงูุนูุงูุฉ ุจูู ุฃุฒูุงุฌ ุงูุฌูู.

ูุน ุชููุน ุงูุฌููุฉ ุงูุชุงููุฉุ ูุชู ุชุฒููุฏ ุงููููุฐุฌ ุจุฃุฒูุงุฌ ูู ุงูุฌูู (ูุน ุฑููุฒ ูููุนุฉ ุนุดูุงุฆููุง) ูููุทูุจ ููู ุงูุชูุจุค ุจูุง ุฅุฐุง ูุงูุช ุงูุฌููุฉ ุงูุซุงููุฉ ุชุชุจุน ุงูุฃููู. ูุฌุนู ุงููููุฉ ุบูุฑ ุชุงููุฉุ ูุตู ุงูููุช ุชุชุจุน ุงูุฌูู ุจุนุถูุง ุงูุจุนุถ ูู ุงููุณุชูุฏ ุงูุฃุตูู ุงูุฐู ุชู ุงุณุชุฎุฑุงุฌูุง ูููุ ูุงููุตู ุงูุขุฎุฑ ูู ุงูููุช ุชุฃุชู ุงูุฌููุชุงู ูู ูุซููุชูู ูุฎุชููุชูู.

ุจุดูู ุนุงูุ ูุง ุฏุงุนู ููููู ุจุดุฃู ูุฌูุฏ `token_type_ids` ูู ูุฏุฎูุงุชู ุงููุฑูุฒุฉ: ุทุงููุง ุชุณุชุฎุฏู ููุณ ููุทุฉ ุงูุชูุชูุด ูููุฑูุฒ ูุงููููุฐุฌุ ุณูููู ูู ุดูุก ุนูู ูุง ูุฑุงู ูุฃู ุงููุฑูุฒ ูุนุฑู ูุง ุงูุฐู ูุฌุจ ุฃู ููุฏูู ููููุฐุฌู.

ุงูุขู ุจุนุฏ ุฃู ุฑุฃููุง ููู ูููู ููุฑูุฒูุง ุงูุชุนุงูู ูุน ุฒูุฌ ูุงุญุฏ ูู ุงูุฌููุ ูููููุง ุงุณุชุฎุฏุงูู ูุชุฑููุฒ ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง: ูุซููุง ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ูููููุง ุฅุทุนุงู ุงููุฑูุฒ ูุงุฆูุฉ ูู ุฃุฒูุงุฌ ุงูุฌูู ุนู ุทุฑูู ุฅุนุทุงุฆู ูุงุฆูุฉ ุงูุฌูู ุงูุฃูููุ ุซู ูุงุฆูุฉ ุงูุฌูู ุงูุซุงููุฉ. ูุฐุง ูุชูุงูู ุฃูุถูุง ูุน ุฎูุงุฑุงุช ุงูุญุดู ูุงูุชูุทูุน ุงูุชู ุฑุฃููุงูุง ูู [ุงููุตู 2](/course/chapter2). ูุฐุงุ ุฅุญุฏู ุทุฑู ูุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช ุงูุชุฏุฑูุจ ูู:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

ูุฐุง ูุนูู ุจุดูู ุฌูุฏุ ูููู ูุนุงูู ูู ุนูุจ ุฅุฑุฌุงุน ูุงููุณ (ุจููุงุชูุญูุงุ `input_ids`ุ `attention_mask`ุ ู `token_type_ids`ุ ูุงูููู ุงูุชู ูู ููุงุฆู ูู ุงูููุงุฆู). ููุง ุฃูู ูู ูุนูู ุฅูุง ุฅุฐุง ูุงู ูุฏูู ุฐุงูุฑุฉ ูุตูู ุนุดูุงุฆู ูุงููุฉ ูุชุฎุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุฃุซูุงุก ุงูุชุฑููุฒ (ูู ุญูู ุฃู ูุฌููุนุงุช ุงูุจูุงูุงุช ูู ููุชุจุฉ ๐ค Datasets ูู ูููุงุช [Apache Arrow](https://arrow.apache.org/) ูุฎุฒูุฉ ุนูู ุงููุฑุตุ ูุฐูู ุชุญุชูุธ ููุท ุจุงูุนููุงุช ุงูุชู ุชุทูุจูุง ูุญููุฉ ูู ุงูุฐุงูุฑุฉ).

ููุญูุงุธ ุนูู ุงูุจูุงูุงุช ููุฌููุนุฉ ุจูุงูุงุชุ ุณูุณุชุฎุฏู ุทุฑููุฉ [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map). ูุฐุง ูุณูุญ ููุง ุฃูุถูุง ุจุจุนุถ ุงููุฑููุฉ ุงูุฅุถุงููุฉุ ุฅุฐุง ููุง ุจุญุงุฌุฉ ุฅูู ุงููุฒูุฏ ูู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุฃูุซุฑ ูู ูุฌุฑุฏ ุงูุชุฑููุฒ. ุชุนูู ุทุฑููุฉ `map()` ุนู ุทุฑูู ุชุทุจูู ุฏุงูุฉ ุนูู ูู ุนูุตุฑ ูู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุฐุง ุฏุนูุง ูุญุฏุฏ ุฏุงูุฉ ุชููู ุจุชุฑููุฒ ูุฏุฎูุงุชูุง:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

ุชุฃุฎุฐ ูุฐู ุงูุฏุงูุฉ ูุงููุณูุง (ูุซู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง) ูุชุนูุฏ ูุงููุณูุง ุฌุฏูุฏูุง ุจููุงุชูุญ `input_ids`ุ `attention_mask`ุ ู `token_type_ids`. ูุงุญุธ ุฃููุง ุชุนูู ุฃูุถูุง ุฅุฐุง ูุงู ูุงููุณ `example` ูุญุชูู ุนูู ุนุฏุฉ ุนููุงุช (ูู ููุชุงุญ ููุงุฆูุฉ ูู ุงูุฌูู) ูุธุฑูุง ูุฃู `tokenizer` ูุนูู ุนูู ููุงุฆู ูู ุฃุฒูุงุฌ ุงูุฌููุ ููุง ุฑุฃููุง ูู ูุจู. ุณูุณูุญ ููุง ูุฐุง ุจุงุณุชุฎุฏุงู ุงูุฎูุงุฑ `batched=True` ูู ููุงููุชูุง ูู `map()`ุ ูุงูุฐู ุณูุณุฑุน ุงูุชุฑููุฒ ุจุดูู ูุจูุฑ. ูุชู ุฏุนู `tokenizer` ุจูุงุณุทุฉ ูุฑูุฒ ููุชูุจ ุจูุบุฉ Rust ูู ููุชุจุฉ [๐ค Tokenizers](https://github.com/huggingface/tokenizers). ูููู ุฃู ูููู ูุฐุง ุงููุฑูุฒ ุณุฑูุนูุง ุฌุฏูุงุ ูููู ููุท ุฅุฐุง ุฃุนุทููุงู ุงููุซูุฑ ูู ุงููุฏุฎูุงุช ูู ููุช ูุงุญุฏ.

ูุงุญุธ ุฃููุง ุชุฑููุง ุญุฌุฉ `padding` ุฎุงุฑุฌ ุฏุงูุฉ ุงูุชุฑููุฒ ุงูุฎุงุตุฉ ุจูุง ูู ุงูููุช ุงูุญุงูู. ูุฐูู ูุฃู ุญุดู ุฌููุน ุงูุนููุงุช ุฅูู ุงูุทูู ุงูุฃูุตู ุบูุฑ ูุนุงู: ูู ุงูุฃูุถู ุญุดู ุงูุนููุงุช ุนูุฏูุง ูููู ุจุจูุงุก ุฏูุนุฉุ ุญูุซ ุฃููุง ูุญุชุงุฌ ููุท ุฅูู ุงูุญุดู ุฅูู ุงูุทูู ุงูุฃูุตู ูู ุชูู ุงูุฏูุนุฉุ ูููุณ ุงูุทูู ุงูุฃูุตู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง. ูููู ุฃู ูููุฑ ูุฐุง ุงููุซูุฑ ูู ุงูููุช ูููุฉ ุงููุนุงูุฌุฉ ุนูุฏูุง ุชููู ุงููุฏุฎูุงุช ุฐุงุช ุฃุทูุงู ูุชุบูุฑุฉ ููุบุงูุฉ!

ููุง ููู ูุทุจู ุฏุงูุฉ ุงูุชุฑููุฒ ุนูู ุฌููุน ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ูู ููุช ูุงุญุฏ. ูุณุชุฎุฏู `batched=True` ูู ููุงููุฉ `map` ุจุญูุซ ูุชู ุชุทุจูู ุงูุฏุงูุฉ ุนูู ุนูุงุตุฑ ูุชุนุฏุฏุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ูู ููุช ูุงุญุฏุ ูููุณ ุนูู ูู ุนูุตุฑ ุจุดูู ูููุตู. ูุณูุญ ูุฐุง ุจูุนุงูุฌุฉ ูุณุจูุฉ ุฃุณุฑุน.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

ุงูุทุฑููุฉ ุงูุชู ุชุทุจู ุจูุง ููุชุจุฉ ๐ค Datasets ูุฐู ุงููุนุงูุฌุฉ ูู ุนู ุทุฑูู ุฅุถุงูุฉ ุญููู ุฌุฏูุฏุฉ ุฅูู ูุฌููุนุงุช ุงูุจูุงูุงุชุ ูุงุญุฏ ููู ููุชุงุญ ูู ุงููุงููุณ ุงูุฐู ุชุนูุฏู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
ููููู ุญุชู ุงุณุชุฎุฏุงู ุงููุนุงูุฌุฉ ุงููุชุนุฏุฏุฉ ุฃุซูุงุก ุชุทุจูู ุฏุงูุฉ ูุง ูุจู ุงููุนุงูุฌุฉ ุงูุฎุงุตุฉ ุจู ุจุงุณุชุฎุฏุงู `map()` ุนู ุทุฑูู ุชูุฑูุฑ ุญุฌุฉ `num_proc`. ูู ููู ุจุฐูู ููุง ูุฃู ููุชุจุฉ ๐ค Tokenizers ุชุณุชุฎุฏู ุจุงููุนู ุฎููุทูุง ูุชุนุฏุฏุฉ ูุชููููุฒ ุนููุงุชูุง ุจุดูู ุฃุณุฑุนุ ูููู ุฅุฐุง ููุช ูุง ุชุณุชุฎุฏู ุชููููุฒ ุณุฑูุนูุง ูุฏุนูููุง ุจูุฐู ุงูููุชุจุฉุ ููุฏ ูุคุฏู ุฐูู ุฅูู ุชุณุฑูุน ูุง ูุจู ุงููุนุงูุฌุฉ ูุฏูู.

ุชุนูุฏ ุฏุงูุชูุง `tokenize_function` ูุงููุณูุง ุจุงูููุงุชูุญ `input_ids`ุ ู`attention_mask`ุ ู`token_type_ids`ุ ูุฐุง ุชุชู ุฅุถุงูุฉ ูุฐู ุงูุญููู ุงูุซูุงุซุฉ ุฅูู ุฌููุน ุฃูุณุงู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง. ูุงุญุธ ุฃูู ูุงู ุจุฅููุงููุง ุฃูุถูุง ุชุบููุฑ ุงูุญููู ุงูููุฌูุฏุฉ ุฅุฐุง ุฃุนุงุฏุช ุฏุงูุฉ ูุง ูุจู ุงููุนุงูุฌุฉ ูุฏููุง ูููุฉ ุฌุฏูุฏุฉ ูููุชุงุญ ููุฌูุฏ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ุทุจููุง ุนูููุง `map()`.

ุงูุดูุก ุงูุฃุฎูุฑ ุงูุฐู ุณูุญุชุงุฌ ุฅูู ุงูููุงู ุจู ูู ุฅุถุงูุฉ ุงูุญุดู ุฅูู ุฌููุน ุงูุฃูุซูุฉ ุจุทูู ุงูุนูุตุฑ ุงูุฃุทูู ุนูุฏูุง ูููู ุจุชุฌููุน ุงูุนูุงุตุฑ ูุนูุง - ููู ุชูููุฉ ูุดูุฑ ุฅูููุง ุจุงุณู *ุงูุญุดู ุงูุฏููุงูููู*.

### ุงูุญุดู ุงูุฏููุงูููู [[dynamic-padding]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
ุงูุฏุงูุฉ ุงููุณุคููุฉ ุนู ุชุฌููุน ุงูุนููุงุช ุฏุงุฎู ุฏูุนุฉ ุชุณูู *ุฏุงูุฉ ุชุฌููุน*. ุฅููุง ุญุฌุฉ ููููู ุชูุฑูุฑูุง ุนูุฏ ุจูุงุก `DataLoader`ุ ูุงูุงูุชุฑุงุถูุฉ ูู ุฏุงูุฉ ุณุชููู ููุท ุจุชุญููู ุนููุงุชู ุฅูู ุชูุณูุฑุงุช PyTorch ูุฏูุฌูุง (ุจุดูู ูุชูุฑุฑ ุฅุฐุง ูุงูุช ุนูุงุตุฑู ููุงุฆู ุฃู ุฃุฒูุงุฌูุง ุฃู ููุงููุณูุง). ูู ูููู ุฐูู ูููููุง ูู ุญุงูุชูุง ูุฃู ุงููุฏุฎูุงุช ุงูุชู ูุฏููุง ูู ุชููู ุฌููุนูุง ุจููุณ ุงูุญุฌู. ููุฏ ุฃุฎุฑูุง ุงูุญุดู ุนู ุนูุฏุ ูุชุทุจููู ููุท ุญุณุจ ุงูุญุงุฌุฉ ุนูู ูู ุฏูุนุฉ ูุชุฌูุจ ูุฌูุฏ ูุฏุฎูุงุช ุทูููุฉ ุฌุฏูุง ูุน ุงููุซูุฑ ูู ุงูุญุดู. ุณูุคุฏู ุฐูู ุฅูู ุชุณุฑูุน ุงูุชุฏุฑูุจ ุฅูู ุญุฏ ูุงุ ูููู ูุงุญุธ ุฃูู ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุชุณุฑูุน (TPU) ููุฏ ูุณุจุจ ุฐูู ูุดุงูู - ุชูุถู ูุญุฏุงุช ูุนุงูุฌุฉ ุชุณุฑูุน ุงูุฃุดูุงู ุงูุซุงุจุชุฉุ ุญุชู ุนูุฏูุง ูุชุทูุจ ุฐูู ุญุดููุง ุฅุถุงูููุง.

{:else}

ุงูุฏุงูุฉ ุงููุณุคููุฉ ุนู ุชุฌููุน ุงูุนููุงุช ุฏุงุฎู ุฏูุนุฉ ุชุณูู *ุฏุงูุฉ ุชุฌููุน*. ุงูุฏุงูุฉ ุงูุงูุชุฑุงุถูุฉ ููุชุฌููุน ูู ุฏุงูุฉ ุณุชููู ููุท ุจุชุญููู ุนููุงุชู ุฅูู `tf.Tensor` ูุฏูุฌูุง (ุจุดูู ูุชูุฑุฑ ุฅุฐุง ูุงูุช ุนูุงุตุฑู ููุงุฆู ุฃู ุฃุฒูุงุฌูุง ุฃู ููุงููุณูุง). ูู ูููู ุฐูู ูููููุง ูู ุญุงูุชูุง ูุฃู ุงููุฏุฎูุงุช ุงูุชู ูุฏููุง ูู ุชููู ุฌููุนูุง ุจููุณ ุงูุญุฌู. ููุฏ ุฃุฎุฑูุง ุงูุญุดู ุนู ุนูุฏุ ูุชุทุจููู ููุท ุญุณุจ ุงูุญุงุฌุฉ ุนูู ูู ุฏูุนุฉ ูุชุฌูุจ ูุฌูุฏ ูุฏุฎูุงุช ุทูููุฉ ุฌุฏูุง ูุน ุงููุซูุฑ ูู ุงูุญุดู. ุณูุคุฏู ุฐูู ุฅูู ุชุณุฑูุน ุงูุชุฏุฑูุจ ุฅูู ุญุฏ ูุงุ ูููู ูุงุญุธ ุฃูู ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุชุณุฑูุน (TPU) ููุฏ ูุณุจุจ ุฐูู ูุดุงูู - ุชูุถู ูุญุฏุงุช ูุนุงูุฌุฉ ุชุณุฑูุน ุงูุฃุดูุงู ุงูุซุงุจุชุฉุ ุญุชู ุนูุฏูุง ูุชุทูุจ ุฐูู ุญุดููุง ุฅุถุงูููุง.

{/if}

ููุนู ุฐูู ุนููููุงุ ูุฌุจ ุนูููุง ุชุญุฏูุฏ ุฏุงูุฉ ุชุฌููุน ุณุชุทุจู ุงููููุฉ ุงูุตุญูุญุฉ ูู ุงูุญุดู ุนูู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ูุฑูุฏ ุชุฌููุนูุง ูุนูุง. ูุญุณู ุงูุญุธุ ุชููุฑ ููุง ููุชุจุฉ ๐ค Transformers ูุซู ูุฐู ุงูุฏุงูุฉ ุนุจุฑ `DataCollatorWithPadding`. ุชุฃุฎุฐ ุชููููุฒ ุนูุฏ ุฅูุดุงุก ูุซูู ููุง (ููุนุฑูุฉ ุฃู ุฑูุฒ ุญุดู ูุฌุจ ุงุณุชุฎุฏุงููุ ููุง ุฅุฐุง ูุงู ุงููููุฐุฌ ูุชููุน ุงูุญุดู ุนูู ุงููุณุงุฑ ุฃู ุนูู ูููู ุงููุฏุฎูุงุช) ูุณุชููู ุจูู ูุง ุชุญุชุงุฌ ุฅููู:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

ูุงุฎุชุจุงุฑ ูุฐู ุงูุฃุฏุงุฉ ุงูุฌุฏูุฏุฉุ ุฏุนูุง ูุฃุฎุฐ ุจุนุถ ุงูุนููุงุช ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงูุชู ููุฏ ุชุฌููุนูุง ูุนูุง. ููุงุ ูุฒูู ุงูุฃุนูุฏุฉ `idx`ุ ู`sentence1`ุ ู`sentence2` ูุฃููุง ูู ุชููู ุถุฑูุฑูุฉ ูุชุญุชูู ุนูู ุณูุงุณู (ููุง ูููููุง ุฅูุดุงุก ุชูุณูุฑุงุช ุจุณูุงุณู) ููููู ูุธุฑุฉ ุนูู ุฃุทูุงู ูู ุฅุฏุฎุงู ูู ุงูุฏูุนุฉ:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

ููุณ ูู ุงููุณุชุบุฑุจุ ูุญุตู ุนูู ุนููุงุช ุจุฃุทูุงู ูุชุบูุฑุฉุ ูู 32 ุฅูู 67. ูุนูู ุงูุญุดู ุงูุฏููุงูููู ุฃูู ูุฌุจ ุญุดู ุงูุนููุงุช ูู ูุฐู ุงูุฏูุนุฉ ุฌููุนูุง ุจุทูู 67ุ ููู ุงูุทูู ุงูุฃูุตู ุฏุงุฎู ุงูุฏูุนุฉ. ุจุฏูู ุงูุญุดู ุงูุฏููุงููููุ ุณูุชุนูู ุญุดู ุฌููุน ุงูุนููุงุช ุฅูู ุงูุทูู ุงูุฃูุตู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุงููุงููุ ุฃู ุงูุทูู ุงูุฃูุตู ุงูุฐู ูููู ูููููุฐุฌ ูุจููู. ุฏุนูุง ูุชุฃูุฏ ูู ุฃู `data_collator` ูุฏููุง ูููู ุจุญุดู ุงูุฏูุนุฉ ุจุดูู ุฏููุงูููู:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

ูุจุฏู ุฌูุฏูุง! ุงูุขู ุจุนุฏ ุฃู ุงูุชูููุง ูู ุงููุต ุงูุฎุงู ุฅูู ุงูุฏูุนุงุช ุงูุชู ูููู ููููุฐุฌูุง ุงูุชุนุงูู ูุนูุงุ ูุญู ูุณุชุนุฏูู ูุถุจุทู ุจุฏูุฉ!

{/if}

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ูุฑุฑ ูุง ูุจู ุงููุนุงูุฌุฉ ุนูู ูุฌููุนุฉ ุจูุงูุงุช GLUE SST-2. ุฅููุง ูุฎุชููุฉ ุจุนุถ ุงูุดูุก ูุฃููุง ุชุชููู ูู ุฌูู ููุฑุฏุฉ ุจุฏูุงู ูู ุฃุฒูุงุฌุ ูููู ุจููุฉ ูุง ูููุง ุจู ูุฌุจ ุฃู ูุจุฏู ูุชุทุงุจููุง. ูุชุญุฏู ุฃุตุนุจุ ุญุงูู ูุชุงุจุฉ ุฏุงูุฉ ูุง ูุจู ุงููุนุงูุฌุฉ ุงูุชู ุชุนูู ุนูู ุฃู ูู ููุงู GLUE.

</Tip>

{#if fw === 'tf'}

ุงูุขู ุจุนุฏ ุฃู ูุฏููุง ูุฌููุนุฉ ุงูุจูุงูุงุช ููุณููุฉ ุชุฌููุน ุงูุจูุงูุงุชุ ูุญุชุงุฌ ุฅูู ุชุฌููุนูุง ูุนูุง. ูููููุง ุชุญููู ุงูุฏูุนุงุช ูุชุฌููุนูุง ูุฏูููุงุ ูููู ูุฐุง ูุชุทูุจ ุงููุซูุฑ ูู ุงูุนููุ ูุฑุจูุง ูุง ูููู ูุนุงููุง ุฃูุถูุง. ุจุฏูุงู ูู ุฐููุ ููุงู ุทุฑููุฉ ุจุณูุทุฉ ุชููุฑ ุญูุงู ูุนุงููุง ููุฐู ุงููุดููุฉ: `to_tf_dataset()`. ุณูุบูู ูุฐุง `tf.data.Dataset` ุญูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุ ูุน ุฏุงูุฉ ุชุฌููุน ุงุฎุชูุงุฑูุฉ. `tf.data.Dataset` ูู ุชูุณูู ุฃุตูู ูู TensorFlow ูููู ูู Keras ุงุณุชุฎุฏุงูู ูู `model.fit()`ุ ูุฐุง ูุฅู ูุฐู ุงูุทุฑููุฉ ุชุญูู ูุฌููุนุฉ ุจูุงูุงุช ๐ค ููุฑูุง ุฅูู ุชูุณูู ุฌุงูุฒ ููุชุฏุฑูุจ. ุฏุนูุง ูุฑุงูุง ูู ุงูุนูู ูุน ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

ููุฐุง ูู ุดูุก! ูููููุง ุฃุฎุฐ ูุฐู ุงููุฌููุนุงุช ุฅูู ุงููุญุงุถุฑุฉ ุงูุชุงููุฉุ ุญูุซ ุณูููู ุงูุชุฏุฑูุจ ุจุณูุทูุง ุฌุฏูุง ุจุนุฏ ูู ุงูุนูู ุงูุดุงู ูู ูุง ูุจู ูุนุงูุฌุฉ ุงูุจูุงูุงุช.

{/if}