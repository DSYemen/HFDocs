<FrameworkSwitchCourse {fw} />

# معالجة البيانات[[processing-the-data]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
استكمالاً للمثال من [الفصل السابق](/course/chapter2)، إليك كيفية تدريب مصنف تسلسل على دفعة واحدة في PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# كما في السابق
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# هذا جديد
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
استكمالاً للمثال من [الفصل السابق](/course/chapter2)، إليك كيفية تدريب مصنف تسلسل على دفعة واحدة في TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# كما في السابق
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# هذا جديد
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

بالطبع، تدريب النموذج على جملتين فقط لن يعطي نتائج جيدة. للحصول على نتائج أفضل، ستحتاج إلى إعداد مجموعة بيانات أكبر.

في هذا القسم، سنستخدم كمثال مجموعة بيانات MRPC (Microsoft Research Paraphrase Corpus) التي تم تقديمها في [ورقة بحثية](https://www.aclweb.org/anthology/I05-5002.pdf) من قبل William B. Dolan و Chris Brockett. تتكون مجموعة البيانات من 5,801 زوج من الجمل، مع وجود علامة تشير إلى ما إذا كانت متشابهة في المعنى أم لا (أي، إذا كان كلا الجملتين تعنيان نفس الشيء). لقد اخترناها لهذا الفصل لأنها مجموعة بيانات صغيرة، مما يجعل من السهل إجراء التجارب عليها.

### تحميل مجموعة بيانات من المركز[[loading-a-dataset-from-the-hub]]

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

المركز لا يحتوي فقط على النماذج، بل يحتوي أيضًا على مجموعات بيانات متعددة بلغات مختلفة. يمكنك تصفح مجموعات البيانات [هنا](https://huggingface.co/datasets)، ونحن نوصي بتجربة تحميل ومعالجة مجموعة بيانات جديدة بعد الانتهاء من هذا القسم (راجع الوثائق العامة [هنا](https://huggingface.co/docs/datasets/loading)). ولكن الآن، دعنا نركز على مجموعة بيانات MRPC! هذه واحدة من 10 مجموعات بيانات تشكل [معيار GLUE](https://gluebenchmark.com/)، وهو معيار أكاديمي يستخدم لقياس أداء نماذج ML عبر 10 مهام مختلفة لتصنيف النصوص.

توفر مكتبة 🤗 Datasets أمرًا بسيطًا جدًا لتحميل وتخزين مجموعة بيانات على المركز. يمكننا تحميل مجموعة بيانات MRPC على النحو التالي:

<Tip>
⚠️ **تحذير** تأكد من تثبيت `datasets` عن طريق تشغيل `pip install datasets`. ثم قم بتحميل مجموعة بيانات MRPC وطباعتها لمعرفة محتوياتها.
</Tip> 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

كما ترى، نحصل على كائن `DatasetDict` الذي يحتوي على مجموعة التدريب، ومجموعة التحقق، ومجموعة الاختبار. يحتوي كل منها على عدة أعمدة (`sentence1`، `sentence2`، `label`، و`idx`) وعدد متغير من الصفوف، وهو عدد العناصر في كل مجموعة (لذلك، هناك 3,668 زوج من الجمل في مجموعة التدريب، و408 في مجموعة التحقق، و1,725 في مجموعة الاختبار).

يقوم هذا الأمر بتحميل وتخزين مجموعة البيانات، بشكل افتراضي في *~/.cache/huggingface/datasets*. تذكر من الفصل 2 أنه يمكنك تخصيص مجلد التخزين المؤقت الخاص بك عن طريق تعيين متغير البيئة `HF_HOME`.

يمكننا الوصول إلى كل زوج من الجمل في كائن `raw_datasets` الخاص بنا عن طريق الفهرسة، مثل القاموس:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

يمكننا أن نرى أن العلامات هي بالفعل أعداد صحيحة، لذلك لن نحتاج إلى إجراء أي معالجة مسبقة هناك. لمعرفة أي عدد صحيح يقابل أي علامة، يمكننا فحص `features` الخاصة بـ `raw_train_dataset`. سيخبرنا هذا بنوع كل عمود:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

في الخلفية، `label` من نوع `ClassLabel`، ويتم تخزين تعيين الأعداد الصحيحة إلى اسم العلامة في مجلد *names*. `0` يقابل `not_equivalent`، و`1` يقابل `equivalent`.

<Tip>

✏️ **جربها!** انظر إلى العنصر 15 من مجموعة التدريب والعنصر 87 من مجموعة التحقق. ما هي علاماتها؟

</Tip>

### معالجة مسبقة لمجموعة بيانات[[preprocessing-a-dataset]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

لمعالجة مجموعة البيانات مسبقًا، نحتاج إلى تحويل النص إلى أرقام يمكن للنموذج فهمها. كما رأيت في [الفصل السابق](/course/chapter2)، يتم ذلك باستخدام أداة تجزئة الكلمات. يمكننا إدخال أداة تجزئة الكلمات جملة واحدة أو قائمة من الجمل، لذلك يمكننا تجزئة جميع الجمل الأولى وجميع الجمل الثانية من كل زوج على النحو التالي:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ومع ذلك، لا يمكننا فقط تمرير تسلسلين إلى النموذج والحصول على تنبؤ بما إذا كانت الجملتين متشابهتين في المعنى أم لا. نحتاج إلى التعامل مع التسلسلين كزوج، وتطبيق المعالجة المسبقة المناسبة. لحسن الحظ، يمكن لأداة تجزئة الكلمات أيضًا أخذ زوج من التسلسلات وإعداده بالطريقة التي يتوقعها نموذج BERT الخاص بنا: 

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
ناقشنا مفاتيح `input_ids` و `attention_mask` في [الفصل 2](/course/chapter2)، لكننا أجلنا الحديث عن `token_type_ids`. في هذا المثال، هذا ما يخبر النموذج بأي جزء من الإدخال هو الجملة الأولى وأيها الجملة الثانية.

<نصيحة>

✏️ **جربها!** خذ العنصر 15 من مجموعة التدريب وقم بتقسيم الجملتين إلى رموز منفصلة وكزوج. ما هو الفرق بين النتيجتين؟

</نصيحة>

إذا قمنا بفك تشفير الـ IDs داخل `input_ids` إلى كلمات:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

سنحصل على:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

لذا نرى أن النموذج يتوقع أن تكون المدخلات على الشكل `[CLS] sentence1 [SEP] sentence2 [SEP]` عندما يكون هناك جملتين. ومواءمة هذا مع `token_type_ids` يعطينا:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

كما ترى، فإن أجزاء الإدخال المقابلة لـ `[CLS] sentence1 [SEP]` لديها جميعها معرف نوع الرمز `0`، بينما الأجزاء الأخرى، المقابلة لـ `sentence2 [SEP]`، لديها جميعها معرف نوع الرمز `1`.

لاحظ أنه إذا قمت باختيار نقطة تفتيش مختلفة، فلن يكون لديك بالضرورة `token_type_ids` في مدخلاتك المرمزة (على سبيل المثال، لا يتم إرجاعها إذا كنت تستخدم نموذج DistilBERT). يتم إرجاعها فقط عندما سيعرف النموذج ما الذي يجب فعله بها، لأنه رآها خلال التدريب المسبق.

هنا، يتم تدريب BERT مسبقًا بمعرفات نوع الرمز، وبالإضافة إلى هدف نمذجة اللغة المقنعة الذي تحدثنا عنه في [الفصل 1](/course/chapter1)، لديه هدف إضافي يسمى _توقع الجملة التالية_. الهدف من هذه المهمة هو نمذجة العلاقة بين أزواج الجمل.

مع توقع الجملة التالية، يتم تزويد النموذج بأزواج من الجمل (مع رموز مقنعة عشوائيًا) ويُطلب منه التنبؤ بما إذا كانت الجملة الثانية تتبع الأولى. لجعل المهمة غير تافهة، نصف الوقت تتبع الجمل بعضها البعض في المستند الأصلي الذي تم استخراجها منه، والنصف الآخر من الوقت تأتي الجملتان من وثيقتين مختلفتين.

بشكل عام، لا داعي للقلق بشأن وجود `token_type_ids` في مدخلاتك المرمزة: طالما تستخدم نفس نقطة التفتيش للمرمز والنموذج، سيكون كل شيء على ما يرام لأن المرمز يعرف ما الذي يجب أن يقدمه لنموذجه.

الآن بعد أن رأينا كيف يمكن لمرمزنا التعامل مع زوج واحد من الجمل، يمكننا استخدامه لترميز مجموعة البيانات بأكملها: مثلما في [الفصل السابق](/course/chapter2)، يمكننا إطعام المرمز قائمة من أزواج الجمل عن طريق إعطائه قائمة الجمل الأولى، ثم قائمة الجمل الثانية. هذا متوافق أيضًا مع خيارات الحشو والتقطيع التي رأيناها في [الفصل 2](/course/chapter2). لذا، إحدى طرق معالجة مجموعة بيانات التدريب هي:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

هذا يعمل بشكل جيد، لكنه يعاني من عيب إرجاع قاموس (بمفاتيحنا، `input_ids`، `attention_mask`، و `token_type_ids`، والقيم التي هي قوائم من القوائم). كما أنه لن يعمل إلا إذا كان لديك ذاكرة وصول عشوائي كافية لتخزين مجموعة البيانات بأكملها أثناء الترميز (في حين أن مجموعات البيانات من مكتبة 🤗 Datasets هي ملفات [Apache Arrow](https://arrow.apache.org/) مخزنة على القرص، لذلك تحتفظ فقط بالعينات التي تطلبها محملة في الذاكرة).

للحفاظ على البيانات كمجموعة بيانات، سنستخدم طريقة [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map). هذا يسمح لنا أيضًا ببعض المرونة الإضافية، إذا كنا بحاجة إلى المزيد من المعالجة المسبقة أكثر من مجرد الترميز. تعمل طريقة `map()` عن طريق تطبيق دالة على كل عنصر من عناصر مجموعة البيانات، لذا دعنا نحدد دالة تقوم بترميز مدخلاتنا:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

تأخذ هذه الدالة قاموسًا (مثل عناصر مجموعة البيانات الخاصة بنا) وتعيد قاموسًا جديدًا بمفاتيح `input_ids`، `attention_mask`، و `token_type_ids`. لاحظ أنها تعمل أيضًا إذا كان قاموس `example` يحتوي على عدة عينات (كل مفتاح كقائمة من الجمل) نظرًا لأن `tokenizer` يعمل على قوائم من أزواج الجمل، كما رأينا من قبل. سيسمح لنا هذا باستخدام الخيار `batched=True` في مكالمتنا لـ `map()`، والذي سيسرع الترميز بشكل كبير. يتم دعم `tokenizer` بواسطة مرمز مكتوب بلغة Rust من مكتبة [🤗 Tokenizers](https://github.com/huggingface/tokenizers). يمكن أن يكون هذا المرمز سريعًا جدًا، ولكن فقط إذا أعطيناه الكثير من المدخلات في وقت واحد.

لاحظ أننا تركنا حجة `padding` خارج دالة الترميز الخاصة بنا في الوقت الحالي. وذلك لأن حشو جميع العينات إلى الطول الأقصى غير فعال: من الأفضل حشو العينات عندما نقوم ببناء دفعة، حيث أننا نحتاج فقط إلى الحشو إلى الطول الأقصى في تلك الدفعة، وليس الطول الأقصى في مجموعة البيانات بأكملها. يمكن أن يوفر هذا الكثير من الوقت وقوة المعالجة عندما تكون المدخلات ذات أطوال متغيرة للغاية!

هنا كيف نطبق دالة الترميز على جميع مجموعات البيانات الخاصة بنا في وقت واحد. نستخدم `batched=True` في مكالمة `map` بحيث يتم تطبيق الدالة على عناصر متعددة من مجموعة البيانات الخاصة بنا في وقت واحد، وليس على كل عنصر بشكل منفصل. يسمح هذا بمعالجة مسبقة أسرع.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

الطريقة التي تطبق بها مكتبة 🤗 Datasets هذه المعالجة هي عن طريق إضافة حقول جديدة إلى مجموعات البيانات، واحد لكل مفتاح في القاموس الذي تعيده دالة المعالجة المسبقة:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
يمكنك حتى استخدام المعالجة المتعددة أثناء تطبيق دالة ما قبل المعالجة الخاصة بك باستخدام `map()` عن طريق تمرير حجة `num_proc`. لم نقم بذلك هنا لأن مكتبة 🤗 Tokenizers تستخدم بالفعل خيوطًا متعددة لتوكينز عيناتنا بشكل أسرع، ولكن إذا كنت لا تستخدم توكينز سريعًا مدعومًا بهذه المكتبة، فقد يؤدي ذلك إلى تسريع ما قبل المعالجة لديك.

تعيد دالتنا `tokenize_function` قاموسًا بالمفاتيح `input_ids`، و`attention_mask`، و`token_type_ids`، لذا تتم إضافة هذه الحقول الثلاثة إلى جميع أقسام مجموعة البيانات الخاصة بنا. لاحظ أنه كان بإمكاننا أيضًا تغيير الحقول الموجودة إذا أعادت دالة ما قبل المعالجة لدينا قيمة جديدة لمفتاح موجود في مجموعة البيانات التي طبقنا عليها `map()`.

الشيء الأخير الذي سنحتاج إلى القيام به هو إضافة الحشو إلى جميع الأمثلة بطول العنصر الأطول عندما نقوم بتجميع العناصر معًا - وهي تقنية نشير إليها باسم *الحشو الديناميكي*.

### الحشو الديناميكي [[dynamic-padding]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
الدالة المسؤولة عن تجميع العينات داخل دفعة تسمى *دالة تجميع*. إنها حجة يمكنك تمريرها عند بناء `DataLoader`، والافتراضية هي دالة ستقوم فقط بتحويل عيناتك إلى تنسورات PyTorch ودمجها (بشكل متكرر إذا كانت عناصرك قوائم أو أزواجًا أو قواميسًا). لن يكون ذلك ممكنًا في حالتنا لأن المدخلات التي لدينا لن تكون جميعها بنفس الحجم. لقد أخرنا الحشو عن عمد، لتطبيقه فقط حسب الحاجة على كل دفعة وتجنب وجود مدخلات طويلة جدًا مع الكثير من الحشو. سيؤدي ذلك إلى تسريع التدريب إلى حد ما، ولكن لاحظ أنه إذا كنت تتدرب على وحدة معالجة تسريع (TPU) فقد يسبب ذلك مشاكل - تفضل وحدات معالجة تسريع الأشكال الثابتة، حتى عندما يتطلب ذلك حشوًا إضافيًا.

{:else}

الدالة المسؤولة عن تجميع العينات داخل دفعة تسمى *دالة تجميع*. الدالة الافتراضية للتجميع هي دالة ستقوم فقط بتحويل عيناتك إلى `tf.Tensor` ودمجها (بشكل متكرر إذا كانت عناصرك قوائم أو أزواجًا أو قواميسًا). لن يكون ذلك ممكنًا في حالتنا لأن المدخلات التي لدينا لن تكون جميعها بنفس الحجم. لقد أخرنا الحشو عن عمد، لتطبيقه فقط حسب الحاجة على كل دفعة وتجنب وجود مدخلات طويلة جدًا مع الكثير من الحشو. سيؤدي ذلك إلى تسريع التدريب إلى حد ما، ولكن لاحظ أنه إذا كنت تتدرب على وحدة معالجة تسريع (TPU) فقد يسبب ذلك مشاكل - تفضل وحدات معالجة تسريع الأشكال الثابتة، حتى عندما يتطلب ذلك حشوًا إضافيًا.

{/if}

لفعل ذلك عمليًا، يجب علينا تحديد دالة تجميع ستطبق الكمية الصحيحة من الحشو على عناصر مجموعة البيانات التي نريد تجميعها معًا. لحسن الحظ، توفر لنا مكتبة 🤗 Transformers مثل هذه الدالة عبر `DataCollatorWithPadding`. تأخذ توكينز عند إنشاء مثيل لها (لمعرفة أي رمز حشو يجب استخدامه، وما إذا كان النموذج يتوقع الحشو على اليسار أو على يمين المدخلات) وستقوم بكل ما تحتاج إليه:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

لاختبار هذه الأداة الجديدة، دعنا نأخذ بعض العينات من مجموعة التدريب التي نود تجميعها معًا. هنا، نزيل الأعمدة `idx`، و`sentence1`، و`sentence2` لأنها لن تكون ضرورية وتحتوي على سلاسل (ولا يمكننا إنشاء تنسورات بسلاسل) ونلقي نظرة على أطوال كل إدخال في الدفعة:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

ليس من المستغرب، نحصل على عينات بأطوال متغيرة، من 32 إلى 67. يعني الحشو الديناميكي أنه يجب حشو العينات في هذه الدفعة جميعًا بطول 67، وهو الطول الأقصى داخل الدفعة. بدون الحشو الديناميكي، سيتعين حشو جميع العينات إلى الطول الأقصى في مجموعة البيانات بالكامل، أو الطول الأقصى الذي يمكن للنموذج قبوله. دعنا نتأكد من أن `data_collator` لدينا يقوم بحشو الدفعة بشكل ديناميكي:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

يبدو جيدًا! الآن بعد أن انتقلنا من النص الخام إلى الدفعات التي يمكن لنموذجنا التعامل معها، نحن مستعدون لضبطه بدقة!

{/if}

<Tip>

✏️ **جربها!** كرر ما قبل المعالجة على مجموعة بيانات GLUE SST-2. إنها مختلفة بعض الشيء لأنها تتكون من جمل مفردة بدلاً من أزواج، ولكن بقية ما قمنا به يجب أن يبدو متطابقًا. لتحدي أصعب، حاول كتابة دالة ما قبل المعالجة التي تعمل على أي من مهام GLUE.

</Tip>

{#if fw === 'tf'}

الآن بعد أن لدينا مجموعة البيانات ووسيلة تجميع البيانات، نحتاج إلى تجميعها معًا. يمكننا تحميل الدفعات وتجميعها يدويًا، ولكن هذا يتطلب الكثير من العمل، وربما لا يكون فعالًا أيضًا. بدلاً من ذلك، هناك طريقة بسيطة توفر حلاً فعالًا لهذه المشكلة: `to_tf_dataset()`. سيغلف هذا `tf.data.Dataset` حول مجموعة البيانات الخاصة بك، مع دالة تجميع اختيارية. `tf.data.Dataset` هو تنسيق أصلي لـ TensorFlow يمكن لـ Keras استخدامه لـ `model.fit()`، لذا فإن هذه الطريقة تحول مجموعة بيانات 🤗 فورًا إلى تنسيق جاهز للتدريب. دعنا نراها في العمل مع مجموعة البيانات الخاصة بنا!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

وهذا كل شيء! يمكننا أخذ هذه المجموعات إلى المحاضرة التالية، حيث سيكون التدريب بسيطًا جدًا بعد كل العمل الشاق في ما قبل معالجة البيانات.

{/if}