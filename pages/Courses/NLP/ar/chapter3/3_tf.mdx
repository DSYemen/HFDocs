<FrameworkSwitchCourse {fw} />

# ุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู Keras[[fine-tuning-a-model-with-keras]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
]} />

ุจุนุฏ ุงูุงูุชูุงุก ูู ุฌููุน ุฃุนูุงู ูุนุงูุฌุฉ ุงูุจูุงูุงุช ูู ุงููุณู ุงูุฃุฎูุฑุ ูู ูุชุจู ุณูู ุจุถุน ุฎุทูุงุช ูุชุฏุฑูุจ ุงููููุฐุฌ. ูุงุญุธุ ูุน ุฐููุ ุฃู ุฃูุฑ `model.fit()` ุณูุนูู ุจุจุทุก ุดุฏูุฏ ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ. ุฅุฐุง ูู ููู ูุฏูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU) ุฌุงูุฒุฉุ ููููู ุงูุญุตูู ุนูู ูุตูู ูุฌุงูู ุฅูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ุฃู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ุงููุงุจูุฉ ููุจุฑูุฌุฉ (TPUs) ุนูู [Google Colab](https://colab.research.google.com/).

ุชูุชุฑุถ ุฃูุซูุฉ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุฃุฏูุงู ุฃูู ูุฏ ููุฐุช ุจุงููุนู ุงูุฃูุซูุฉ ูู ุงููุณู ุงูุณุงุจู. ุฅููู ููุฎุต ูุตูุฑ ูุงุณุชุนุงุฏุฉ ูุง ุชุญุชุงุฌ ุฅููู:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### ุงูุชุฏุฑูุจ[[training]]

ูููุฐุฌ TensorFlow ุงููุณุชูุฑุฏ ูู ๐ค Transformers ูู ุจุงููุนู ูููุฐุฌ Keras. ุฅููู ููุฏูุฉ ูุตูุฑุฉ ุนู Keras.

<Youtube id="rnTGBy2ax1c"/>

ูุฐุง ูุนูู ุฃูู ุจูุฌุฑุฏ ุญุตูููุง ุนูู ุงูุจูุงูุงุชุ ูู ููุฒู ุณูู ุงููููู ูู ุงูุนูู ูุจุฏุก ุงูุชุฏุฑูุจ ุนูููุง.

<Youtube id="AUozVp78dhk"/>

ููุง ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ุณูุณุชุฎุฏู ูุฆุฉ `TFAutoModelForSequenceClassification`ุ ูุน ุนูุงูุชูู:

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ุณุชูุงุญุธ ุฃูู ุนูู ุนูุณ [ุงููุตู 2](/course/chapter2)ุ ุณุชุญุตู ุนูู ุชุญุฐูุฑ ุจุนุฏ ุฅูุดุงุก ูุฐุง ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง. ูุฐุง ูุฃู BERT ูู ูุชู ุชุฏุฑูุจู ูุณุจููุง ุนูู ุชุตููู ุฃุฒูุงุฌ ุงูุฌููุ ูุฐูู ุชู ุงูุชุฎูุต ูู ุฑุฃุณ ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ูุชู ุฅุฏุฑุงุฌ ุฑุฃุณ ุฌุฏูุฏ ููุงุณุจ ูุชุตููู ุงูุชุณูุณู ุจุฏูุงู ูู ุฐูู. ุชุดูุฑ ุงูุชุญุฐูุฑุงุช ุฅูู ุฃู ุจุนุถ ุงูุฃูุฒุงู ูู ุชูุณุชุฎุฏู (ุงูุชู ุชูุงุจู ุฑุฃุณ ุงูุชุฏุฑูุจ ุงููุณุจู ุงูููุณูุท) ูุฃู ุจุนุถูุง ุงูุขุฎุฑ ุชู ุชููุฆุชู ุนุดูุงุฆููุง (ุงูุชู ุชุฎุต ุงูุฑุฃุณ ุงูุฌุฏูุฏ). ููุฎุชุชู ุจุชุดุฌูุนู ุนูู ุชุฏุฑูุจ ุงููููุฐุฌุ ููู ูุง ุณูููู ุจู ุงูุขู ุจุงูุถุจุท.

ูุถุจุท ุงููููุฐุฌ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุงุ ูุฌุจ ุนูููุง ููุท `compile()` ูููุฐุฌูุง ุซู ุชูุฑูุฑ ุจูุงูุงุชูุง ุฅูู ุทุฑููุฉ `fit()`. ุณูุจุฏุฃ ูุฐุง ุนูููุฉ ุงูุถุจุท ุงูุฏููู (ุงูุชู ูุฌุจ ุฃู ุชุณุชุบุฑู ุจุถุน ุฏูุงุฆู ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช) ูุชุจูุบ ุนู ุฎุณุงุฑุฉ ุงูุชุฏุฑูุจ ุฃุซูุงุก ุงูุชูููุ ุจุงูุฅุถุงูุฉ ุฅูู ุฎุณุงุฑุฉ ุงูุชุญูู ูู ููุงูุฉ ูู ุญูุจุฉ.

<Tip>

ูุงุญุธ ุฃู ููุงุฐุฌ ๐ค Transformers ูุฏููุง ูุฏุฑุฉ ุฎุงุตุฉ ูุง ุชูุชูููุง ูุนุธู ููุงุฐุฌ Keras - ูููููุง ุงุณุชุฎุฏุงู ุฎุณุงุฑุฉ ููุงุณุจุฉ ุชููุงุฆููุง ูุงูุชู ุชุญุณุจูุง ุฏุงุฎูููุง. ุณุชุณุชุฎุฏู ูุฐู ุงูุฎุณุงุฑุฉ ุจุดูู ุงูุชุฑุงุถู ุฅุฐุง ูู ุชุญุฏุฏ ุญุฌุฉ ุงูุฎุณุงุฑุฉ ูู `compile()`. ูุงุญุธ ุฃูู ูุงุณุชุฎุฏุงู ุงูุฎุณุงุฑุฉ ุงูุฏุงุฎููุฉุ ุณุชุญุชุงุฌ ุฅูู ุชูุฑูุฑ ุนูุงูุงุชู ูุฌุฒุก ูู ุงูุฅุฏุฎุงูุ ูููุณ ูุนูุงูุฉ ูููุตูุฉุ ููู ุงูุทุฑููุฉ ุงููุนุชุงุฏุฉ ูุงุณุชุฎุฏุงู ุงูุนูุงูุงุช ูุน ููุงุฐุฌ Keras. ุณุชุฑู ุฃูุซูุฉ ุนูู ุฐูู ูู ุงูุฌุฒุก 2 ูู ุงูุฏูุฑุฉ ุงูุชุฏุฑูุจูุฉุ ุญูุซ ูููู ุฃู ูููู ุชุนุฑูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุตุญูุญุฉ ุตุนุจูุง. ุจุงููุณุจุฉ ูุชุตููู ุงูุชุณูุณูุ ูุน ุฐููุ ุชุนูู ุฏุงูุฉ ุฎุณุงุฑุฉ Keras ุงูููุงุณูุฉ ุจุดูู ุฌูุฏุ ูุฐูู ูุฐุง ูุง ุณูุณุชุฎุฏูู ููุง.

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

ูุงุญุธ ูุฎูุง ุดุงุฆุนูุง ุฌุฏูุง ููุง - ููููู ุชูุฑูุฑ ุงุณู ุงูุฎุณุงุฑุฉ ูุณูุณูุฉ ุฅูู Kerasุ ูููู ุงูุชุฑุงุถููุง ุณุชูุชุฑุถ Keras ุฃูู ูุฏ ุทุจูุช ุจุงููุนู softmax ุนูู ูุฎุฑุฌุงุชู. ููุน ุฐููุ ูุฅู ุงูุนุฏูุฏ ูู ุงูููุงุฐุฌ ุชุฎุฑุฌ ุงูููู ูุจู ุชุทุจูู softmax ุนูููุงุ ูุงูุชู ุชูุนุฑู ุฃูุถูุง ุจุงุณู *logits*. ูุญุชุงุฌ ุฅูู ุฅุฎุจุงุฑ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุจุฃู ูุฐุง ูุง ููุนูู ูููุฐุฌูุงุ ูุงูุทุฑููุฉ ุงููุญูุฏุฉ ููููุงู ุจุฐูู ูู ุงุณุชุฏุนุงุคูุง ูุจุงุดุฑุฉุ ุจุฏูุงู ูู ุงุณููุง ุจุณูุณูุฉ.

</Tip>


### ุชุญุณูู ุฃุฏุงุก ุงูุชุฏุฑูุจ[[improving-training-performance]]

<Youtube id="cpzq6ESSM5c"/>

ุฅุฐุง ุฌุฑุจุช ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุฃุนูุงูุ ูุณุชุนูู ุจุงูุชุฃููุฏุ ููููู ุณุชุฌุฏ ุฃู ุงูุฎุณุงุฑุฉ ุชูุฎูุถ ุจุจุทุก ุฃู ุจุดูู ูุชูุทุน ููุท. ุงูุณุจุจ ุงูุฑุฆูุณู ูู *ูุนุฏู ุงูุชุนูู*. ููุง ูู ุงูุญุงู ูุน ุงูุฎุณุงุฑุฉุ ุนูุฏูุง ููุฑุฑ ุงุณู ูุญุณู ุฅูู Keras ูุณูุณูุฉุ ุชููู Keras ุจุชููุฆุฉ ุฐูู ุงููุญุณู ุจููู ุงูุชุฑุงุถูุฉ ูุฌููุน ุงููุนููุงุชุ ุจูุง ูู ุฐูู ูุนุฏู ุงูุชุนูู. ููุน ุฐููุ ูู ุฎูุงู ุงูุชุฌุฑุจุฉ ุงูุทูููุฉุ ูุนูู ุฃู ููุงุฐุฌ ุงููุญูู ุชุณุชููุฏ ูู ูุนุฏู ุชุนูู ุฃูู ุจูุซูุฑ ูู ุงูุงูุชุฑุงุถู ูู Adamุ ููู 1e-3ุ ูุงูุฐู ูููุชุจ ุฃูุถูุง ูู 10 ุฅูู ุงูููุฉ -3ุ ุฃู 0.001. 5e-5 (0.00005)ุ ููู ุฃูู ุจุนุดุฑูู ูุฑุฉุ ููุทุฉ ุจุฏุงูุฉ ุฃูุถู ุจูุซูุฑ.

ุจุงูุฅุถุงูุฉ ุฅูู ุฎูุถ ูุนุฏู ุงูุชุนููุ ูุฏููุง ุฎุฏุนุฉ ุซุงููุฉ ูู ุฌุนุจุชูุง: ูููููุง ุฃู ูุฎูุถ ูุนุฏู ุงูุชุนูู ุจุจุทุก ุนูู ูุฏุงุฑ ุงูุชุฏุฑูุจ. ูู ุงูุฃุฏุจูุงุชุ ุณุชุฑู ูู ุจุนุถ ุงูุฃุญูุงู ุงูุฅุดุงุฑุฉ ุฅูู ูุฐุง ุนูู ุฃูู *ุชุฏููุฑ* ุฃู *ุชูููู* ูุนุฏู ุงูุชุนูู. ูู Kerasุ ุฃูุถู ุทุฑููุฉ ููููุงู ุจุฐูู ูู ุงุณุชุฎุฏุงู *ูุฎุทุท ูุนุฏู ุงูุชุนูู*. ูุงุญุฏ ุฌูุฏ ููุงุณุชุฎุฏุงู ูู `PolynomialDecay` - ุนูู ุงูุฑุบู ูู ุงูุงุณูุ ูุน ุงูุฅุนุฏุงุฏุงุช ุงูุงูุชุฑุงุถูุฉุ ูุฅูู ูููู ุจุจุณุงุทุฉ ูุนุฏู ุงูุชุนูู ุจุดูู ุฎุทู ูู ุงููููุฉ ุงูุฃูููุฉ ุฅูู ุงููููุฉ ุงูููุงุฆูุฉ ุนูู ูุฏุงุฑ ุงูุชุฏุฑูุจุ ููู ุจุงูุถุจุท ูุง ูุฑูุฏู. ููุน ุฐููุ ููู ูุณุชุฎุฏู ูุฎุทุทูุง ุจุดูู ุตุญูุญุ ูุญุชุงุฌ ุฅูู ุฅุฎุจุงุฑู ุจูุฏุฉ ุงูุชุฏุฑูุจ. ูุญุณุจ ุฐูู ุนูู ุฃูู `num_train_steps` ุฃุฏูุงู.

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ ูู ุนุฏุฏ ุงูุนููุงุช ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ููุณูููุง ุนูู ุญุฌู ุงูุฏูุนุฉ ุซู ูุถุฑูุจูุง
# ุจุนุฏุฏ ุงูุญูุจุงุช ุงูุฅุฌูุงูู. ูุงุญุธ ุฃู tf_train_dataset ููุง ูู tf.data.Dataset ูุฌูุนุ
# ูููุณ ูุฌููุนุฉ ุจูุงูุงุช Hugging Face ุงูุฃุตููุฉุ ูุฐุง ูุฅู len() ุงูุฎุงุตุฉ ุจู ูู ุจุงููุนู num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

ุชุญุชูู ููุชุจุฉ ๐ค Transformers ุฃูุถูุง ุนูู ุฏุงูุฉ `create_optimizer()` ุณุชูุดุฆ ูุญุณููุง `AdamW` ูุน ุงูุฎูุงุถ ูุนุฏู ุงูุชุนูู. ูุฐุง ุงุฎุชุตุงุฑ ููุงุณุจ ุณุชุฑุงู ุจุงูุชูุตูู ูู ุงูุฃูุณุงู ุงููุณุชูุจููุฉ ูู ุงูุฏูุฑุฉ ุงูุชุฏุฑูุจูุฉ.

</Tip>

ุงูุขู ูุฏููุง ูุญุณููุง ุงูุฌุฏูุฏ ุจุงููุงููุ ููููููุง ุชุฌุฑุจุฉ ุงูุชุฏุฑูุจ ุจู. ุฃููุงูุ ุฏุนูุง ูุนูุฏ ุชุญููู ุงููููุฐุฌุ ูุฅุนุงุฏุฉ ุชุนููู ุงูุชุบููุฑุงุช ุนูู ุงูุฃูุฒุงู ูู ุนูููุฉ ุงูุชุฏุฑูุจ ุงูุชู ูููุง ุจูุง ููุชูุ ุซู ูููููุง ุชุฌููุนู ุจุงููุญุณู ุงูุฌุฏูุฏ:

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

ุงูุขูุ ูุชูุงุณุจ ูุฑุฉ ุฃุฎุฑู:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

๐ก ุฅุฐุง ุฃุฑุฏุช ุชุญููู ูููุฐุฌู ุชููุงุฆููุง ุฅูู Hub ุฃุซูุงุก ุงูุชุฏุฑูุจุ ููููู ุชูุฑูุฑ `PushToHubCallback` ูู ุทุฑููุฉ `model.fit()`. ุณูุชุนูู ุงููุฒูุฏ ุนู ูุฐุง ูู [ุงููุตู 4](/course/chapter4/3)

</Tip>

### ุชูุจุคุงุช ุงููููุฐุฌ[[model-predictions]]

<Youtube id="nx10eh4CoOs"/>


ุชุฏุฑูุจ ุงููููุฐุฌ ููุดุงูุฏุฉ ุงูุฎูุงุถ ุงูุฎุณุงุฑุฉ ุฃูุฑ ุฑุงุฆุนุ ูููู ูุงุฐุง ูู ุฃุฑุฏูุง ุงูุญุตูู ุนูู ูุฎุฑุฌุงุช ูู ุงููููุฐุฌ ุงููุฏุฑุจุ ุฅูุง ูุญุณุงุจ ุจุนุถ ุงูููุงููุณ ุฃู ูุงุณุชุฎุฏุงู ุงููููุฐุฌ ูู ุงูุฅูุชุงุฌุ ููููุงู ุจุฐููุ ูููููุง ุจุจุณุงุทุฉ ุงุณุชุฎุฏุงู ุทุฑููุฉ `predict()`. ุณูุนูุฏ ูุฐุง *logits* ูู ุฑุฃุณ ุฅุฎุฑุงุฌ ุงููููุฐุฌุ ูุงุญุฏ ููู ูุฆุฉ.

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

ูููููุง ุชุญููู ูุฐู ุงูููุบุงุฑูุชูุงุช ุฅูู ุชูุจุคุงุช ูุฆุฉ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู `argmax` ูุฅูุฌุงุฏ ุงูููุบุงุฑูุชู ุงูุฃุนููุ ูุงูุฐู ููุงุจู ุงููุฆุฉ ุงูุฃูุซุฑ ุงุญุชูุงููุง:

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

ุงูุขูุ ุฏุนูุง ูุณุชุฎุฏู ุชูู `preds` ูุญุณุงุจ ุจุนุถ ุงูููุงููุณ! ูููููุง ุชุญููู ุงูููุงููุณ ุงููุฑุชุจุทุฉ ุจูุฌููุนุฉ ุจูุงูุงุช MRPC ุจุณูููุฉ ููุง ุญููููุง ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุฐู ุงููุฑุฉ ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `evaluate.load()`. ุงููุงุฆู ุงููุฑุชุฌุน ูู ุทุฑููุฉ `compute()` ูููููุง ุงุณุชุฎุฏุงููุง ูุญุณุงุจ ุงูููุงููุณ:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ูุฏ ุชุฎุชูู ุงููุชุงุฆุฌ ุงูุฏูููุฉ ุงูุชู ุชุญุตู ุนูููุงุ ุญูุซ ูุฏ ูุคุฏู ุงูุชููุฆุฉ ุงูุนุดูุงุฆูุฉ ูุฑุฃุณ ุงููููุฐุฌ ุฅูู ุชุบููุฑ ุงูููุงููุณ ุงูุชู ุญูููุง. ููุงุ ูููููุง ุฃู ูุฑู ุฃู ูููุฐุฌูุง ูุฏูู ุฏูุฉ ุชุจูุบ 85.78% ุนูู ูุฌููุนุฉ ุงูุชุญูู ููููุงุณ F1 ูุจูุบ 89.97. ูุฐุงู ููุง ุงููููุงุณุงู ุงููุณุชุฎุฏูุชุงู ูุชูููู ุงููุชุงุฆุฌ ุนูู ูุฌููุนุฉ ุจูุงูุงุช MRPC ููููุงุณ GLUE. ุงูุฌุฏูู ูู [ูุฑูุฉ BERT](https://arxiv.org/pdf/1810.04805.pdf) ุฃุจูุบ ุนู ูููุงุณ F1 ูุจูุบ 88.9 ูููููุฐุฌ ุงูุฃุณุงุณู. ูุงู ุฐูู ูู ุงููููุฐุฌ `uncased` ุจูููุง ูุณุชุฎุฏู ุญุงูููุง ุงููููุฐุฌ `cased`ุ ููุง ููุณุฑ ุงููุชูุฌุฉ ุงูุฃูุถู.

ูุฐุง ูุฎุชุชู ุงูููุฏูุฉ ูุถุจุท ุงูุฏูุฉ ุจุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช Keras. ุณูุชู ุฅุนุทุงุก ูุซุงู ุนูู ุงูููุงู ุจุฐูู ููุนุธู ููุงู NLP ุงูุดุงุฆุนุฉ ูู [ุงููุตู 7](/course/chapter7). ุฅุฐุง ููุช ุชุฑุบุจ ูู ุตูู ููุงุฑุงุชู ุนูู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช Kerasุ ุญุงูู ุถุจุท ุฏูุฉ ูููุฐุฌ ุนูู ูุฌููุนุฉ ุจูุงูุงุช GLUE SST-2ุ ุจุงุณุชุฎุฏุงู ูุนุงูุฌุฉ ุงูุจูุงูุงุช ุงูุชู ููุช ุจูุง ูู ุงููุณู 2.