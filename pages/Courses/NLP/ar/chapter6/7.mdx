# ุชุฌุฒูุก ุฃุญุงุฏู ุงูุฑููุฒ [[unigram-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
]} />

ูุณุชุฎุฏู ุฎูุงุฑุฒููุฉ Unigram ุจุดูู ูุชูุฑุฑ ูู SentencePieceุ ููู ุฎูุงุฑุฒููุฉ ุชุฌุฒูุก ุงูุฑููุฒ ุงููุณุชุฎุฏูุฉ ูู ููุงุฐุฌ ูุซู AlBERT ูT5 ูmBART ูBig Bird ูXLNet.

<Youtube id="TGZfZVuF9Yc"/>

<Tip>

๐ก ูุบุทู ูุฐุง ุงููุณู Unigram ุจุนููุ ููุตู ุฅูู ุญุฏ ุนุฑุถ ุงูุชูููุฐ ุงููุงูู. ููููู ุชุฎุทู ูุฐุง ุงููุณู ูุงูุงูุชูุงู ุฅูู ุงูููุงูุฉ ุฅุฐุง ููุช ุชุฑูุฏ ููุท ูุธุฑุฉ ุนุงูุฉ ุนู ุฎูุงุฑุฒููุฉ ุชุฌุฒูุก ุงูุฑููุฒ.

</Tip>

## ุฎูุงุฑุฒููุฉ ุงูุชุฏุฑูุจ [[training-algorithm]]

ููุงุฑูุฉ ุจุฎูุงุฑุฒููุชู BPE ูWordPieceุ ุชุนูู ุฎูุงุฑุฒููุฉ Unigram ูู ุงูุงุชุฌุงู ุงููุนุงูุณ: ููู ุชุจุฏุฃ ูู ููุฑุฏุงุช ูุจูุฑุฉ ูุชุฒูู ุงูุฑููุฒ ูููุง ุญุชู ุชุตู ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุทููุจ. ููุงู ุนุฏุฉ ุฎูุงุฑุงุช ูุจูุงุก ุชูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ: ูููููุง ุฃู ูุฃุฎุฐ ุฃูุซุฑ ุงูุณูุงุณู ุงููุฑุนูุฉ ุดููุนูุง ูู ุงููููุงุช ุงููุฌุฒุฃุฉ ูุณุจููุงุ ุนูู ุณุจูู ุงููุซุงูุ ุฃู ุชุทุจูู BPE ุนูู ุงููุฌููุนุฉ ุงูุฃูููุฉ ูุน ุญุฌู ููุฑุฏุงุช ูุจูุฑ.

ูู ูู ุฎุทูุฉ ูู ุงูุชุฏุฑูุจุ ุชุญุณุจ ุฎูุงุฑุฒููุฉ Unigram ุฎุณุงุฑุฉ ุนูู ุงููุฌููุนุฉ ุงููุนุทุงุฉ ุจุงููุธุฑ ุฅูู ุงูููุฑุฏุงุช ุงูุญุงููุฉ. ุจุนุฏ ุฐููุ ุจุงููุณุจุฉ ููู ุฑูุฒ ูู ุงูููุฑุฏุงุชุ ุชุญุณุจ ุงูุฎูุงุฑุฒููุฉ ููุฏุงุฑ ุงูุฒูุงุฏุฉ ูู ุงูุฎุณุงุฑุฉ ุงูุฅุฌูุงููุฉ ุฅุฐุง ุชู ุฅุฒุงูุฉ ุงูุฑูุฒุ ูุชุจุญุซ ุนู ุงูุฑููุฒ ุงูุชู ุณุชุฒูุฏ ุงูุฎุณุงุฑุฉ ุงูุฅุฌูุงููุฉ ุฃูู ูุง ูููู. ูุฐู ุงูุฑููุฒ ููุง ุชุฃุซูุฑ ุฃูู ุนูู ุงูุฎุณุงุฑุฉ ุงูุฅุฌูุงููุฉ ุนูู ุงููุฌููุนุฉุ ูุฐุง ููู ูู ุงููุงูุน "ุฃูู ุงุญุชูุงุฌูุง" ููู ุฃูุถู ุงููุฑุดุญูู ููุฅุฒุงูุฉ.

ูุฐู ุงูุนูููุฉ ููููุฉ ููุบุงูุฉุ ูุฐุง ูุง ูุฒูู ุงูุฑูุฒ ุงููุฑุฏู ุงููุฑุชุจุท ุจุฃูู ุฒูุงุฏุฉ ูู ุงูุฎุณุงุฑุฉุ ูููููุง ูุฒูู \\(p\\) (\\(p\\) ูู ูุฑุท ูุนููุฉ ููููู ุงูุชุญูู ูููุงุ ุนุงุฏุฉ 10 ุฃู 20) ูู ุงููุฆุฉ ูู ุงูุฑููุฒ ุงููุฑุชุจุทุฉ ุจุฃูู ุฒูุงุฏุฉ ูู ุงูุฎุณุงุฑุฉ. ุซู ุชุชูุฑุฑ ูุฐู ุงูุนูููุฉ ุญุชู ุชุตู ุงูููุฑุฏุงุช ุฅูู ุงูุญุฌู ุงููุทููุจ.

ูุงุญุธ ุฃููุง ูุง ูุฒูู ุงูุฃุญุฑู ุงูุฃุณุงุณูุฉ ุฃุจุฏูุงุ ููุชุฃูุฏ ูู ุฅููุงููุฉ ุชุฌุฒูุก ุฃู ูููุฉ.

ุงูุขูุ ูุฐุง ูุง ูุฒุงู ุบุงูุถูุง ุจุนุถ ุงูุดูุก: ุงูุฌุฒุก ุงูุฑุฆูุณู ูู ุงูุฎูุงุฑุฒููุฉ ูู ุญุณุงุจ ุฎุณุงุฑุฉ ุนูู ุงููุฌููุนุฉ ููุนุฑูุฉ ููู ุชุชุบูุฑ ุนูุฏูุง ูุฒูู ุจุนุถ ุงูุฑููุฒ ูู ุงูููุฑุฏุงุชุ ููููุง ูู ูุดุฑุญ ููููุฉ ุงูููุงู ุจุฐูู ุจุนุฏ. ุชุนุชูุฏ ูุฐู ุงูุฎุทูุฉ ุนูู ุฎูุงุฑุฒููุฉ ุชุฌุฒูุก ุงูุฑููุฒ ููููุฐุฌ Unigramุ ูุฐุง ุณูุบูุต ูู ูุฐุง ุงูููุถูุน ูู ุงูุฎุทูุฉ ุงูุชุงููุฉ.

ุณูุนูุฏ ุงุณุชุฎุฏุงู ุงููุฌููุนุฉ ูู ุงูุฃูุซูุฉ ุงูุณุงุจูุฉ:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ูุจุงููุณุจุฉ ููุฐุง ุงููุซุงูุ ุณูุฃุฎุฐ ุฌููุน ุงูุณูุงุณู ุงููุฑุนูุฉ ุงูุตุงุฑูุฉ ููููุฑุฏุงุช ุงูุฃูููุฉ:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## ุฎูุงุฑุฒููุฉ ุชุฌุฒูุก ุงูุฑููุฒ [[tokenization-algorithm]]

ูููุฐุฌ Unigram ูู ููุน ูู ููุงุฐุฌ ุงููุบุฉ ุงูุชู ุชุนุชุจุฑ ูู ุฑูุฒ ูุณุชูููุง ุนู ุงูุฑููุฒ ุงูุชู ุชุณุจูู. ุฅูู ุฃุจุณุท ูููุฐุฌ ููุบุฉุ ุจูุนูู ุฃู ุงุญุชูุงู ุงูุฑูุฒ X ุจุงููุธุฑ ุฅูู ุงูุณูุงู ุงูุณุงุจู ูู ุจุจุณุงุทุฉ ุงุญุชูุงู ุงูุฑูุฒ X. ูุฐุงุ ุฅุฐุง ุงุณุชุฎุฏููุง ูููุฐุฌ ูุบุฉ Unigram ูุชูููุฏ ูุตุ ูุณูููู ุฏุงุฆููุง ุจุงูุชูุจุค ุจุงูุฑูุฒ ุงูุฃูุซุฑ ุดููุนูุง.

ุงุญุชูุงู ุฑูุฒ ูุนูู ูู ุชูุฑุงุฑู (ุนุฏุฏ ุงููุฑุงุช ุงูุชู ูุฌุฏู ูููุง) ูู ุงููุฌููุนุฉ ุงูุฃุตููุฉุ ููุณูููุง ุนูู ูุฌููุน ุชูุฑุงุฑุงุช ุฌููุน ุงูุฑููุฒ ูู ุงูููุฑุฏุงุช (ููุชุฃูุฏ ูู ุฃู ุงูุงุญุชูุงูุงุช ุชุณุงูู 1). ุนูู ุณุจูู ุงููุซุงูุ `"ug"` ููุฌูุฏ ูู `"hug"` ู`"pug"` ู`"hugs"`ุ ูุฐุง ูุฅู ุชูุฑุงุฑู ูู 20 ูู ูุฌููุนุชูุง.

ููุง ุชูุฑุงุฑุงุช ุฌููุน ุงูุฑููุฒ ุงููุฑุนูุฉ ุงูููููุฉ ูู ุงูููุฑุฏุงุช:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

ูุฐุงุ ูุฅู ูุฌููุน ุฌููุน ุงูุชูุฑุงุฑุงุช ูู 210ุ ูุงุญุชูุงู ุงูุฑูุฒ ุงููุฑุนู `"ug"` ูู 20/210.

<Tip>

โ๏ธ **ุงูุขู ุฏูุฑู!** ุงูุชุจ ุงูููุฏ ูุญุณุงุจ ุงูุชูุฑุงุฑุงุช ุฃุนูุงู ูุชุฃูุฏ ูู ุฃู ุงููุชุงุฆุฌ ุงููุนุฑูุถุฉ ุตุญูุญุฉุ ููุฐูู ุงููุฌููุน ุงูููู.

</Tip>

ุงูุขูุ ูุชุฌุฒูุก ูููุฉ ูุนููุฉุ ููุธุฑ ุฅูู ุฌููุน ุงูุชุฌุฒูุฆุงุช ุงูููููุฉ ุฅูู ุฑููุฒ ููุญุณุจ ุงุญุชูุงู ูู ูููุง ููููุง ููููุฐุฌ Unigram. ูุธุฑูุง ูุฃู ุฌููุน ุงูุฑููุฒ ุชุนุชุจุฑ ูุณุชููุฉุ ูุฅู ูุฐุง ุงูุงุญุชูุงู ูู ุจุจุณุงุทุฉ ุญุงุตู ุถุฑุจ ุงุญุชูุงู ูู ุฑูุฒ. ุนูู ุณุจูู ุงููุซุงูุ ูุฅู ุชุฌุฒูุก ุงูุฑููุฒ `["p", "u", "g"]` ูููููุฉ `"pug"` ูู ุงูุงุญุชูุงู:

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

ูุจุงูููุงุฑูุฉุ ูุฅู ุชุฌุฒูุก ุงูุฑููุฒ `["pu", "g"]` ูู ุงูุงุญุชูุงู:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

ูุฐุง ูุฅู ูุฐุง ุงูุงุญุชูุงู ุฃูุจุฑ ุจูุซูุฑ. ุจุดูู ุนุงูุ ุณุชููู ุงูุชุฌุฒูุฆุงุช ุงูุชู ุชุญุชูู ุนูู ุฃูู ุนุฏุฏ ูููู ูู ุงูุฑููุฒ ููุง ุฃุนูู ุงุญุชูุงู (ุจุณุจุจ ุงููุณูุฉ ุนูู 210 ุงููุชูุฑุฑุฉ ููู ุฑูุฒ)ุ ููู ูุง ูุชูุงูู ูุน ูุง ูุฑูุฏู ุจุฏููููุง: ุชูุณูู ูููุฉ ุฅูู ุฃูู ุนุฏุฏ ูููู ูู ุงูุฑููุฒ.

ุชุฌุฒูุก ุงููููุฉ ุจุงุณุชุฎุฏุงู ูููุฐุฌ Unigram ูู ุงูุชุฌุฒูุก ุฐู ุงูุงุญุชูุงู ุงูุฃุนูู. ูู ูุซุงู ุงููููุฉ `"pug"`ุ ุฅููู ุงูุงุญุชูุงูุงุช ุงูุชู ุณูุญุตู ุนูููุง ููู ุชุฌุฒูุก ูููู:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

ูุฐุงุ ุณุชููู ุงููููุฉ `"pug"` ูุฌุฒุฃุฉ ุฅูู `["p", "ug"]` ุฃู `["pu", "g"]``ุ ุงุนุชูุงุฏูุง ุนูู ุฃู ูู ูุฐู ุงูุชุฌุฒูุฆุงุช ูุชู ููุงุฌูุชู ุฃููุงู (ูุงุญุธ ุฃูู ูู ูุฌููุนุฉ ุฃูุจุฑุ ุณุชููู ุญุงูุงุช ุงููุณุงูุงุฉ ูุซู ูุฐู ูุงุฏุฑุฉ).

ูู ูุฐู ุงูุญุงูุฉุ ูุงู ูู ุงูุณูู ุงูุนุซูุฑ ุนูู ุฌููุน ุงูุชุฌุฒูุฆุงุช ุงูููููุฉ ูุญุณุงุจ ุงุญุชูุงูุงุชูุงุ ูููู ุจุดูู ุนุงู ุณูููู ุงูุฃูุฑ ุฃุตุนุจ ูููููุง. ููุงู ุฎูุงุฑุฒููุฉ ููุงุณูููุฉ ูุณุชุฎุฏูุฉ ููุฐุงุ ุชุณูู ุฎูุงุฑุฒููุฉ ููุชุฑุจู. ุจุดูู ุฃุณุงุณูุ ูููููุง ุจูุงุก ุฑุณู ุจูุงูู ูููุดู ุนู ุงูุชุฌุฒูุฆุงุช ุงูููููุฉ ููููุฉ ูุนููุฉ ุจุงูููู ุจุฃู ููุงู ูุฑุนูุง ูู ุงูุญุฑู _a_ ุฅูู ุงูุญุฑู _b_ ุฅุฐุง ูุงู ุงูุฑูุฒ ุงููุฑุนู ูู _a_ ุฅูู _b_ ููุฌูุฏูุง ูู ุงูููุฑุฏุงุชุ ููุนุฒู ุฅูู ุฐูู ุงููุฑุน ุงุญุชูุงู ุงูุฑูุฒ ุงููุฑุนู.

ูุนุซูุฑ ุนูู ุงููุณุงุฑ ูู ุฐูู ุงูุฑุณู ุงูุจูุงูู ุงูุฐู ุณูููู ูู ุฃูุถู ูุชูุฌุฉุ ุชุญุฏุฏ ุฎูุงุฑุฒููุฉ ููุชุฑุจูุ ููู ููุถุน ูู ุงููููุฉุ ุงูุชุฌุฒูุก ุฐู ุฃูุถู ูุชูุฌุฉ ุงูุฐู ููุชูู ูู ุฐูู ุงูููุถุน. ูุธุฑูุง ูุฃููุง ููุชูู ูู ุงูุจุฏุงูุฉ ุฅูู ุงูููุงูุฉุ ูููู ุงูุนุซูุฑ ุนูู ุฃูุถู ูุชูุฌุฉ ุนู ุทุฑูู ุงูุญููุฉ ุนุจุฑ ุฌููุน ุงูุฑููุฒ ุงููุฑุนูุฉ ุงูุชู ุชูุชูู ูู ุงูููุถุน ุงูุญุงูู ุซู ุงุณุชุฎุฏุงู ุฃูุถู ูุชูุฌุฉ ุชุฌุฒูุก ูู ุงูููุถุน ุงูุฐู ูุจุฏุฃ ููู ูุฐุง ุงูุฑูุฒ ุงููุฑุนู. ุจุนุฏ ุฐููุ ูุฌุจ ุนูููุง ููุท ูู ุงููุณุงุฑ ุงููุชุฎุฐ ูููุตูู ุฅูู ุงูููุงูุฉ.

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ูุซุงู ุจุงุณุชุฎุฏุงู ููุฑุฏุงุชูุง ูุงููููุฉ `"unhug"`. ุจุงููุณุจุฉ ููู ููุถุนุ ุงูุฑููุฒ ุงููุฑุนูุฉ ุฐุงุช ุฃูุถู ุงููุชุงุฆุฌ ุงูุชู ุชูุชูู ููุงู ูู ุงูุชุงููุฉ:

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

ูุฐุง ุณุชููู ุงููููุฉ `"unhug"` ูุฌุฒุฃุฉ ุฅูู `["un", "hug"]`.

<Tip>

โ๏ธ **ุงูุขู ุฏูุฑู!** ุญุฏุฏ ุชุฌุฒูุก ุงููููุฉ `"huggun"`ุ ููุชูุฌุชูุง.

</Tip>

## ุงูุนูุฏุฉ ุฅูู ุงูุชุฏุฑูุจ [[back-to-training]]

ุงูุขู ุจุนุฏ ุฃู ุฑุฃููุง ููููุฉ ุนูู ุชุฌุฒูุก ุงูุฑููุฒุ ูููููุง ุงูุบูุต ูููููุง ูู ุงูุฎุณุงุฑุฉ ุงููุณุชุฎุฏูุฉ ุฃุซูุงุก ุงูุชุฏุฑูุจ. ูู ุฃู ูุฑุญูุฉ ูุนููุฉุ ูุชู ุญุณุงุจ ูุฐู ุงูุฎุณุงุฑุฉ ุนู ุทุฑูู ุชุฌุฒูุก ูู ูููุฉ ูู ุงููุฌููุนุฉุ ุจุงุณุชุฎุฏุงู ุงูููุฑุฏุงุช ุงูุญุงููุฉ ููููุฐุฌ Unigram ุงููุญุฏุฏ ุจูุงุณุทุฉ ุชูุฑุงุฑุงุช ูู ุฑูุฒ ูู ุงููุฌููุนุฉ (ููุง ุฑุฃููุง ูู ูุจู).

ููู ูููุฉ ูู ุงููุฌููุนุฉ ูุชูุฌุฉุ ูุงูุฎุณุงุฑุฉ ูู ุงูููุบุงุฑูุชู ุงูุนูุณู ุงูุณูุจู ูุชูู ุงููุชุงุฆุฌ -- ุฃู ูุฌููุน ุฌููุน ุงููููุงุช ูู ุงููุฌููุนุฉ ูู ุฌููุน `-log(P(word))`.

ุฏุนูุง ูุนูุฏ ุฅูู ูุซุงููุง ูุน ุงููุฌููุนุฉ ุงูุชุงููุฉ:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ุชุฌุฒูุก ูู ูููุฉ ูุน ูุชุงุฆุฌูุง ูู:

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

ูุฐุง ูุฅู ุงูุฎุณุงุฑุฉ ูู:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```
ุงูุขู ูุญุชุงุฌ ุฅูู ุญุณุงุจ ููู ูุคุซุฑ ุฅุฒุงูุฉ ูู ุฑูุฒ ุนูู ุงูุฎุณุงุฑุฉ. ูุฐุง ุฃูุฑ ููู ููุนูุง ูุงุ ูุฐูู ุณูููู ุจุฐูู ููุท ูุฑููุฒูู ููุง ูุณูุญูุธ ุงูุนูููุฉ ุงููุงููุฉ ุนูุฏูุง ูููู ูุฏููุง ููุฏ ููุณุงุนุฏุชูุง. ูู ูุฐู ุงูุญุงูุฉ (ุงููุญุฏุฏุฉ ุฌุฏูุง)ุ ูุงู ูุฏููุง ุฑูุฒุงู ูุชูุงูุฆุงู ูุฌููุน ุงููููุงุช: ููุง ุฑุฃููุง ุณุงุจููุงุ ุนูู ุณุจูู ุงููุซุงูุ ูููู ุฃู ูููู `"pug"` ููุณููุง ุฅูู ุฑููุฒ ["p", "ug"] ุจููุณ ุงููุชูุฌุฉ. ูุจุงูุชุงููุ ูุฅู ุฅุฒุงูุฉ ุงูุฑูุฒ `"pu"` ูู ุงูููุฑุฏุงุช ุณุชุนุทู ููุณ ุงูุฎุณุงุฑุฉ ุจุงูุถุจุท.

ูู ูุงุญูุฉ ุฃุฎุฑูุ ูุฅู ุฅุฒุงูุฉ `"hug"` ุณุชุฌุนู ุงูุฎุณุงุฑุฉ ุฃุณูุฃุ ูุฃู ุชูุณูู ุงููููุฉ `"hug"` ู `"hugs"` ุณูุตุจุญ:

```
"hug": ["hu", "g"] (ุงููุชูุฌุฉ 0.006802)
"hugs": ["hu", "gs"] (ุงููุชูุฌุฉ 0.001701)
```

ุณุชุคุฏู ูุฐู ุงูุชุบููุฑุงุช ุฅูู ุงุฑุชูุงุน ุงูุฎุณุงุฑุฉ ุจููุฏุงุฑ:

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

ูุฐููุ ูู ุงููุญุชูู ุฃู ูุชู ุฅุฒุงูุฉ ุงูุฑูุฒ `"pu"` ูู ุงูููุฑุฏุงุชุ ูููู ููุณ ุงูุฑูุฒ `"hug"`.

## ุชูููุฐ Unigram[[implementing-unigram]]

ุงูุขู ุฏุนููุง ูููุฐ ูู ูุง ุฑุฃููุงู ุญุชู ุงูุขู ูู ุงูููุฏ. ูุซู BPE ูWordPieceุ ูุฐู ููุณุช ุนูููุฉ ุชูููุฐ ูุนุงูุฉ ูุฎูุงุฑุฒููุฉ Unigram (ุนูู ุงูุนูุณ ุชูุงููุง)ุ ูููููุง ูุฌุจ ุฃู ุชุณุงุนุฏู ุนูู ููููุง ุจุดูู ุฃูุถู ูููููุง.

ุณูุณุชุฎุฏู ููุณ ุงููุฌููุนุฉ ูู ุงููุตูุต ููุง ูู ุงููุซุงู ุงูุณุงุจู:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

ูุฐู ุงููุฑุฉุ ุณูุณุชุฎุฏู `xlnet-base-cased` ููููุฐุฌ ูุฏููุง:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

ูุซู BPE ูWordPieceุ ูุจุฏุฃ ุจุญุณุงุจ ุนุฏุฏ ูุฑุงุช ุธููุฑ ูู ูููุฉ ูู ุงููุฌููุนุฉ:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

ุจุนุฏ ุฐููุ ูุญุชุงุฌ ุฅูู ุชููุฆุฉ ููุฑุฏุงุชูุง ุฅูู ุญุฌู ุฃูุจุฑ ูู ุญุฌู ุงูููุฑุฏุงุช ุงูุฐู ุณูุฑูุฏู ูู ุงูููุงูุฉ. ูุฌุจ ุฃู ูุฏุฑุฌ ุฌููุน ุงูุฃุญุฑู ุงูุฃุณุงุณูุฉ (ูุฅูุง ูู ูุชููู ูู ุชูุณูู ูู ูููุฉ)ุ ูููู ุจุงููุณุจุฉ ููุณูุงุณู ุงููุฑุนูุฉ ุงูุฃูุจุฑุ ุณูุญุชูุธ ููุท ุจุงูุฃูุซุฑ ุดููุนูุงุ ูุฐูู ูููู ุจุชุฑุชูุจูุง ุญุณุจ ุงูุชุฑุฏุฏ:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Loop through the subwords of length at least 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Sort subwords by frequency
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('โt', 7), ('is', 5), ('er', 5), ('โa', 5), ('โto', 4), ('to', 4), ('en', 4), ('โT', 3), ('โTh', 3), ('โThi', 3)]
```

ูููู ุจุฌูุน ุงูุฃุญุฑู ูุน ุฃูุถู ุงูุณูุงุณู ุงููุฑุนูุฉ ูููุตูู ุฅูู ููุฑุฏุงุช ุฃูููุฉ ุจุญุฌู 300:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>

๐ก ูุณุชุฎุฏู SentencePiece ุฎูุงุฑุฒููุฉ ุฃูุซุฑ ููุงุกุฉ ุชุณูู Enhanced Suffix Array (ESA) ูุฅูุดุงุก ุงูููุฑุฏุงุช ุงูุฃูููุฉ.

</Tip>

ุจุนุฏ ุฐููุ ูุญุณุจ ูุฌููุน ุฌููุน ุงูุชุฑุฏุฏุงุชุ ูุชุญููู ุงูุชุฑุฏุฏุงุช ุฅูู ุงุญุชูุงูุงุช. ุจุงููุณุจุฉ ููููุฐุฌูุงุ ุณูุฎุฒู ููุบุงุฑูุชูุงุช ุงูุงุญุชูุงูุงุชุ ูุฃู ุฅุถุงูุฉ ุงูููุบุงุฑูุชูุงุช ุฃูุซุฑ ุงุณุชูุฑุงุฑูุง ูู ุงููุงุญูุฉ ุงูุนุฏุฏูุฉ ูู ุถุฑุจ ุงูุฃุฑูุงู ุงูุตุบูุฑุฉุ ููุฐุง ุณูุจุณุท ุญุณุงุจ ุฎุณุงุฑุฉ ุงููููุฐุฌ:

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

ุงูุขู ุงููุธููุฉ ุงูุฑุฆูุณูุฉ ูู ุงูุชู ุชูุณู ุงููููุงุช ุจุงุณุชุฎุฏุงู ุฎูุงุฑุฒููุฉ Viterbi. ููุง ุฑุฃููุง ุณุงุจููุงุ ุชููู ูุฐู ุงูุฎูุงุฑุฒููุฉ ุจุญุณุงุจ ุฃูุถู ุชูุณูู ููู ุณูุณูุฉ ูุฑุนูุฉ ูู ุงููููุฉุ ูุงูุชู ุณูุฎุฒููุง ูู ูุชุบูุฑ ูุณูู `best_segmentations`. ุณูุฎุฒู ูุงููุณูุง ูุงุญุฏูุง ููู ููุถุน ูู ุงููููุฉ (ูู 0 ุฅูู ุทูููุง ุงูุฅุฌูุงูู)ุ ูุน ููุชุงุญูู: ูุคุดุฑ ุจุฏุงูุฉ ุงูุฑูุฒ ุงูุฃุฎูุฑ ูู ุฃูุถู ุชูุณููุ ููุชูุฌุฉ ุฃูุถู ุชูุณูู. ูุน ูุคุดุฑ ุจุฏุงูุฉ ุงูุฑูุฒ ุงูุฃุฎูุฑุ ุณูุชููู ูู ุงุณุชุฑุฌุงุน ุงูุชูุณูู ุงููุงูู ุจูุฌุฑุฏ ุฃู ูุชู ููุก ุงููุงุฆูุฉ ุจุงููุงูู.

ูุชู ููุก ุงููุงุฆูุฉ ุจุงุณุชุฎุฏุงู ุญููุชูู ููุท: ุงูุญููุฉ ุงูุฑุฆูุณูุฉ ุชุฏูุฑ ุญูู ูู ููุถุน ุจุฏุงูุฉุ ูุงูุญููุฉ ุงูุซุงููุฉ ุชุฌุฑุจ ุฌููุน ุงูุณูุงุณู ุงููุฑุนูุฉ ุงูุชู ุชุจุฏุฃ ุจููุถุน ุงูุจุฏุงูุฉ ูุฐุง. ุฅุฐุง ูุงูุช ุงูุณูุณูุฉ ุงููุฑุนูุฉ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุณูุญุตู ุนูู ุชูุณูู ุฌุฏูุฏ ูููููุฉ ุญุชู ููุถุน ุงูููุงูุฉ ูุฐุงุ ูุงูุฐู ููุงุฑูู ุจูุง ูู ููุฌูุฏ ูู `best_segmentations`.

ุจูุฌุฑุฏ ุงูุชูุงุก ุงูุญููุฉ ุงูุฑุฆูุณูุฉุ ูุจุฏุฃ ุจุจุณุงุทุฉ ูู ุงูููุงูุฉ ูููุชูู ูู ููุถุน ุจุฏุงูุฉ ุฅูู ุขุฎุฑุ ููููู ุจุชุณุฌูู ุงูุฑููุฒ ุฃุซูุงุก ุงูุชูููุ ุญุชู ูุตู ุฅูู ุจุฏุงูุฉ ุงููููุฉ:

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # This should be properly filled by the previous steps of the loop
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # If we have found a better segmentation ending at end_idx, we update
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # We did not find a tokenization of the word -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

ูููููุง ุจุงููุนู ุชุฌุฑุจุฉ ูููุฐุฌูุง ุงูุฃููู ุนูู ุจุนุถ ุงููููุงุช:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

ุงูุขู ูู ุงูุณูู ุญุณุงุจ ุฎุณุงุฑุฉ ุงููููุฐุฌ ุนูู ุงููุฌููุนุฉ!

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

ูููููุง ุงูุชุญูู ูู ุนููู ุนูู ุงููููุฐุฌ ุงูุฐู ูุฏููุง:

```python
compute_loss(model)
```

```python out
413.10377642940875
```

ุญุณุงุจ ุงููุชุงุฆุฌ ููู ุฑูุฒ ููุณ ุตุนุจูุง ุฃูุถูุงุ ูุญุชุงุฌ ููุท ุฅูู ุญุณุงุจ ุงูุฎุณุงุฑุฉ ูููููุฐุฌ ุงูุฐู ูุญุตู ุนููู ุนู ุทุฑูู ุญุฐู ูู ุฑูุฒ:

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # We always keep tokens of length 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

ูููููุง ุชุฌุฑุจุชู ุนูู ุฑูุฒ ูุนูู:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

ุจูุง ุฃู `"ll"` ูุณุชุฎุฏู ูู ุชูุณูู ุงููููุฉ `"Hopefully"`ุ ููู ุงููุญุชูู ุฃู ูุฌุนููุง ุฅุฒุงูุชู ูุณุชุฎุฏู ุงูุฑูุฒ `"l"` ูุฑุชูู ุจุฏูุงู ูู ุฐููุ ูุชููุน ุฃูู ุณูููู ูู ุฎุณุงุฑุฉ ุฅูุฌุงุจูุฉ. `"his"` ูุณุชุฎุฏู ููุท ุฏุงุฎู ุงููููุฉ `"This"`ุ ูุงูุชู ูุชู ุชูุณูููุง ููุง ููุ ูุฐูู ูุชููุน ุฃู ุชููู ุฎุณุงุฑุชู ุตูุฑูุฉ. ูููุง ุงููุชุงุฆุฌ:

```python out
6.376412403623874
0.0
```

<Tip>

๐ก ูุฐุง ุงูุฃุณููุจ ุบูุฑ ูุนุงู ููุบุงูุฉุ ูุฐุง ูุณุชุฎุฏู SentencePiece ุชูุฑูุจูุง ูููุฏุงู ุงููููุฐุฌ ุจุฏูู ุงูุฑูุฒ X: ุจุฏูุงู ูู ุงูุจุฏุก ูู ุงูุตูุฑุ ููู ูุณุชุจุฏู ุงูุฑูุฒ X ุจุชุฌุฒุฆุชู ูู ุงูููุฑุฏุงุช ุงููุชุจููุฉ. ุจูุฐู ุงูุทุฑููุฉุ ูููู ุญุณุงุจ ุฌููุน ุงูุฏุฑุฌุงุช ูุฑุฉ ูุงุญุฏุฉ ูู ููุณ ููุช ููุฏุงู ุงููููุฐุฌ.

</Tip>

ูุน ูุฌูุฏ ูู ูุฐุงุ ูุฅู ุขุฎุฑ ุดูุก ูุญุชุงุฌ ุฅูู ูุนูู ูู ุฅุถุงูุฉ ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุณุชุฎุฏููุง ุงููููุฐุฌ ุฅูู ุงูููุฑุฏุงุชุ ุซู ููุฑุฑ ุงูุนูููุฉ ุญุชู ูููู ุจุชูููู ุนุฏุฏ ูุงูู ูู ุงูุฑููุฒ ูู ุงูููุฑุฏุงุช ูููุตูู ุฅูู ุงูุญุฌู ุงููุทููุจ:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # ุฅุฒุงูุฉ percent_to_remove ูู ุงูุฑููุฒ ุฐุงุช ุงูุฏุฑุฌุงุช ุงูุฃุฏูู.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

ุซูุ ูุชูุณูู ุงููุต ุฅูู ุฑููุฒุ ูุญุชุงุฌ ููุท ุฅูู ุชุทุจูู ูุง ูุจู ุงูุชูุณูู ุฅูู ุฑููุฒ ุซู ุงุณุชุฎุฏุงู ุฏุงูุชูุง `encode_word()`:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['โThis', 'โis', 'โthe', 'โHugging', 'โFace', 'โ', 'c', 'ou', 'r', 's', 'e', '.']
```

ูุฐุง ูู ุดูุก ุนู Unigram! ูุฃูู ุฃู ุชุดุนุฑ ุงูุขู ุฃูู ุฎุจูุฑ ูู ูู ูุง ูุชุนูู ุจุชูุณูู ุงููุต ุฅูู ุฑููุฒ. ูู ุงููุณู ุงูุชุงููุ ุณูุบูุต ูู ุงููุจูุงุช ุงูุฃุณุงุณูุฉ ูููุชุจุฉ ๐ค Tokenizersุ ูุณููุถุญ ูู ููู ููููู ุงุณุชุฎุฏุงููุง ูุจูุงุก ุชูุณูู ุงููุต ุฅูู ุฑููุฒ ุงูุฎุงุต ุจู.