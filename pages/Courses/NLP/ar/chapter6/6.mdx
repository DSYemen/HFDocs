# ุชุฌุฒุฆุฉ WordPiece [[wordpiece-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
]} />

WordPiece ูู ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒุฆุฉ ุงูุชู ุทูุฑุชูุง ุฌูุฌู ูุชุฏุฑูุจ BERT ุงููุณุจู. ููุฏ ุชู ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงูู ููุฐ ุฐูู ุงูุญูู ูู ุงูุนุฏูุฏ ูู ููุงุฐุฌ ุงููุญูู ุงููุงุฆูุฉ ุนูู BERTุ ูุซู DistilBERT ูMobileBERT ูFunnel Transformers ูMPNET. ุฅูู ูุดุงุจู ุฌุฏูุง ูู BPE ูู ุญูุซ ุงูุชุฏุฑูุจุ ูููู ุงูุชุฌุฒุฆุฉ ุงููุนููุฉ ุชุชู ุจุดูู ูุฎุชูู.

<Youtube id="qpv6ms_t_1A"/>

<Tip>

๐ก ูุบุทู ูุฐุง ุงููุณู WordPiece ุจุนููุ ุญูุซ ูุตู ุฅูู ุญุฏ ุนุฑุถ ุงูุชูููุฐ ุงููุงูู. ููููู ุชุฎุทู ูุฐุง ุงููุณู ูุงูุงูุชูุงู ุฅูู ุงูููุงูุฉ ุฅุฐุง ููุช ุชุฑูุฏ ููุท ูุธุฑุฉ ุนุงูุฉ ุนูู ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒุฆุฉ.

</Tip>

## ุฎูุงุฑุฒููุฉ ุงูุชุฏุฑูุจ [[training-algorithm]]

<Tip warning={true}>

โ๏ธ ูู ุชูู ุฌูุฌู ุจุฅุชุงุญุฉ ุฎูุงุฑุฒููุฉ ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจู WordPieceุ ูุฐูู ูุง ููู ูู ุฃูุถู ุชุฎููู ูุฏููุง ุจูุงุกู ุนูู ุงูุฃุฏุจูุงุช ุงูููุดูุฑุฉ. ูุฏ ูุง ูููู ุฏููููุง ุจูุณุจุฉ 100%.

</Tip>

ูุซู BPEุ ุชุจุฏุฃ WordPiece ูู ููุฑุฏุงุช ุตุบูุฑุฉ ุชุดูู ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุณุชุฎุฏููุง ุงููููุฐุฌ ูุงูุฃุจุฌุฏูุฉ ุงูุฃูููุฉ. ูุธุฑูุง ูุฃูู ูุญุฏุฏ ุงููููุงุช ุงููุฑุนูุฉ ุนู ุทุฑูู ุฅุถุงูุฉ ุจุงุฏุฆุฉ (ูุซู `##` ูู BERT)ุ ูุชู ุชูุณูู ูู ูููุฉ ูู ุงูุจุฏุงูุฉ ุนู ุทุฑูู ุฅุถุงูุฉ ุชูู ุงูุจุงุฏุฆุฉ ุฅูู ุฌููุน ุงูุฃุญุฑู ุฏุงุฎู ุงููููุฉ. ูุฐููุ ุนูู ุณุจูู ุงููุซุงูุ ูุชู ุชูุณูู ุงููููุฉ "word" ุนูู ุงููุญู ุงูุชุงูู:

```
w ##o ##r ##d
```

ูุจุงูุชุงููุ ุชุญุชูู ุงูุฃุจุฌุฏูุฉ ุงูุฃูููุฉ ุนูู ุฌููุน ุงูุฃุญุฑู ุงูููุฌูุฏุฉ ูู ุจุฏุงูุฉ ุงููููุฉ ูุงูุฃุญุฑู ุงูููุฌูุฏุฉ ุฏุงุฎู ุงููููุฉ ูุณุจููุฉ ุจุจุงุฏุฆุฉ WordPiece.

ุซูุ ูุฑุฉ ุฃุฎุฑู ูุซู BPEุ ุชุชุนูู WordPiece ููุงุนุฏ ุงูุฏูุฌ. ุงููุฑู ุงูุฑุฆูุณู ูู ุทุฑููุฉ ุงุฎุชูุงุฑ ุงูุฒูุฌ ุงููุฑุงุฏ ุฏูุฌู. ุจุฏูุงู ูู ุงุฎุชูุงุฑ ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุงุ ุชุญุณุจ WordPiece ุฏุฑุฌุฉ ููู ุฒูุฌุ ุจุงุณุชุฎุฏุงู ุงูุตูุบุฉ ุงูุชุงููุฉ:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

ูู ุฎูุงู ูุณูุฉ ุชูุฑุงุฑ ุงูุฒูุฌ ุนูู ุญุงุตู ุถุฑุจ ุชูุฑุงุฑุงุช ูู ุฌุฒุก ูููุ ุชุนุทู ุงูุฎูุงุฑุฒููุฉ ุงูุฃููููุฉ ูุฏูุฌ ุงูุฃุฒูุงุฌ ุญูุซ ุชููู ุงูุฃุฌุฒุงุก ุงููุฑุฏูุฉ ุฃูู ุชูุฑุงุฑูุง ูู ุงูููุฑุฏุงุช. ุนูู ุณุจูู ุงููุซุงูุ ูู ุชููู ุจุงูุถุฑูุฑุฉ ุจุฏูุฌ `("un", "##able")` ุญุชู ุฅุฐุง ูุงู ูุฐุง ุงูุฒูุฌ ูุชูุฑุฑูุง ุฌุฏูุง ูู ุงูููุฑุฏุงุชุ ูุฃู ููุง ุงูุฒูุฌูู `"un"` ู`"##able"` ูู ุงููุญุชูู ุฃู ูุธูุฑุง ูู ุงูุนุฏูุฏ ูู ุงููููุงุช ุงูุฃุฎุฑู ููููุง ุชูุฑุงุฑ ุนุงูู. ุนูู ุงููููุถ ูู ุฐููุ ูู ุงููุญุชูู ุฃู ูุชู ุฏูุฌ ุฒูุฌ ูุซู `("hu", "##gging")` ุจุดูู ุฃุณุฑุน (ุนูู ุงูุชุฑุงุถ ุฃู ุงููููุฉ "hugging" ุชุธูุฑ ูุซูุฑูุง ูู ุงูููุฑุฏุงุช) ุญูุซ ูู ุงููุญุชูู ุฃู ูููู ุชูุฑุงุฑ ูู ูู `"hu"` ู`"##gging"` ุฃูู ุจุดูู ูุฑุฏู.

ุฏุนูุง ููุธุฑ ุฅูู ููุณ ุงูููุฑุฏุงุช ุงูุชู ุงุณุชุฎุฏููุงูุง ูู ูุซุงู ุชุฏุฑูุจ BPE:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ุณุชููู ุงูุชูุณููุงุช ููุง ุนูู ุงููุญู ุงูุชุงูู:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

ูุฐูู ุณุชููู ุงูููุฑุฏุงุช ุงูุฃูููุฉ ูู `["b", "h", "p", "##g", "##n", "##s", "##u"]` (ุฅุฐุง ุชุฌุงูููุง ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุขู). ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุง ูู `("##u", "##g")` (ููุฌูุฏ 20 ูุฑุฉ)ุ ูููู ุงูุชูุฑุงุฑ ุงููุฑุฏู ูู `"##u"` ูุฑุชูุน ุฌุฏูุงุ ูุฐูู ููุณุช ุฏุฑุฌุชู ูู ุงูุฃุนูู (ููู 1 / 36). ุฌููุน ุงูุฃุฒูุงุฌ ุงูุชู ุชุญุชูู ุนูู `"##u"` ูุฏููุง ููุณ ุงูุฏุฑุฌุฉ (1 / 36)ุ ูุฐูู ุฃูุถู ุฏุฑุฌุฉ ูู ููุฒูุฌ `("##g", "##s")` -- ุงููุญูุฏ ุจุฏูู `"##u"` -- ุนูุฏ 1 / 20ุ ููุงุนุฏุฉ ุงูุฏูุฌ ุงูุฃููู ุงูููุชุณุจุฉ ูู `("##g", "##s") -> ("##gs")`.

ูุงุญุธ ุฃูู ุนูุฏ ุงูุฏูุฌุ ูุฒูู `##` ุจูู ุงูุฑูุฒููุ ูุฐูู ูุถูู `"##gs"` ุฅูู ุงูููุฑุฏุงุช ููุทุจู ุงูุฏูุฌ ูู ูููุงุช ุงููุฌููุนุฉ:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

ูู ูุฐู ุงููุฑุญูุฉุ `"##u"` ููุฌูุฏ ูู ุฌููุน ุงูุฃุฒูุงุฌ ุงูููููุฉุ ูุฐูู ุชูุชูู ุฌููุนูุง ุจููุณ ุงูุฏุฑุฌุฉ. ูููุชุฑุถ ุฃูู ูู ูุฐู ุงูุญุงูุฉุ ูุชู ุฏูุฌ ุงูุฒูุฌ ุงูุฃููุ ูุฐูู `("h", "##u") -> "hu"`. ูุฐุง ูุฃุฎุฐูุง ุฅูู:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ุซู ุชููู ุงูุฏุฑุฌุฉ ุงูุชุงููุฉ ุงูุฃูุถู ูุดุชุฑูุฉ ุจูู `("hu", "##g")` ู`("hu", "##gs")` (ุจุฏุฑุฌุฉ 1/15ุ ููุงุฑูุฉ ุจู 1/21 ูุฌููุน ุงูุฃุฒูุงุฌ ุงูุฃุฎุฑู)ุ ูุฐูู ูุชู ุฏูุฌ ุงูุฒูุฌ ุงูุฃูู ุฐู ุงูุฏุฑุฌุฉ ุงูุฃูุจุฑ:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ููุณุชูุฑ ุนูู ูุฐุง ุงููุญู ุญุชู ูุตู ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุทููุจ.

<Tip>

โ๏ธ **ุงูุขู ุฏูุฑู!** ูุง ูู ูุงุนุฏุฉ ุงูุฏูุฌ ุงูุชุงููุฉุ

</Tip>

## ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒุฆุฉ [[tokenization-algorithm]]

ุชุฎุชูู ุงูุชุฌุฒุฆุฉ ูู WordPiece ูBPE ูู ุฃู WordPiece ูุญูุธ ุงูููุฑุฏุงุช ุงูููุงุฆูุฉ ููุทุ ูููุณ ููุงุนุฏ ุงูุฏูุฌ ุงูููุชุณุจุฉ. ุจุฏุกูุง ูู ุงููููุฉ ุงููุฑุงุฏ ุชุฌุฒุฆุชูุงุ ูุฌุฏ WordPiece ุฃุทูู ูููุฉ ูุฑุนูุฉ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ุซู ููุณู ุนูููุง. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ุงุณุชุฎุฏููุง ุงูููุฑุฏุงุช ุงูููุชุณุจุฉ ูู ุงููุซุงู ุฃุนูุงูุ ุจุงููุณุจุฉ ูููููุฉ "hugs" ูุฅู ุฃุทูู ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุงูุจุฏุงูุฉ ูุชููู ุฏุงุฎู ุงูููุฑุฏุงุช ูู "hug"ุ ูุฐูู ููุณู ุนูุฏูุง ููุญุตู ุนูู `["hug", "##s"]`. ุซู ูุณุชูุฑ ูุน `"##s"`ุ ูุงูุชู ุชููู ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุฐูู ุชููู ุชุฌุฒุฆุฉ ุงููููุฉ "hugs" ูู `["hug", "##s"]`.

ูุน BPEุ ููุง ุณูุทุจู ููุงุนุฏ ุงูุฏูุฌ ุงูููุชุณุจุฉ ุจุงูุชุฑุชูุจ ููููู ุจุชุฌุฒุฆุฉ ุงููููุฉ ุนูู ุฃููุง `["hu", "##gs"]`ุ ูุฐูู ูููู ุงูุชุฑููุฒ ูุฎุชูููุง.

ููุซุงู ุขุฎุฑุ ุฏุนูุง ูุฑู ููู ุณุชุชู ุชุฌุฒุฆุฉ ุงููููุฉ "bugs". ุชููู ุงููููุฉ "b" ูู ุฃุทูู ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุจุฏุงูุฉ ุงููููุฉ ูุชููู ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุฐูู ููุณู ุนูุฏูุง ููุญุตู ุนูู `["b", "##ugs"]`. ุซู ุชููู ุงููููุฉ "##u" ูู ุฃุทูู ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุจุฏุงูุฉ ุงููููุฉ "##ugs" ูุชููู ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุฐูู ููุณู ุนูุฏูุง ููุญุตู ุนูู `["b", "##u, "##gs"]`. ูุฃุฎูุฑูุงุ ุชููู ุงููููุฉ "##gs" ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุฐูู ุชููู ูุฐู ุงููุงุฆูุฉ ุงูุฃุฎูุฑุฉ ูู ุชุฌุฒุฆุฉ ุงููููุฉ "bugs".

ุนูุฏูุง ุชุตู ุงูุชุฌุฒุฆุฉ ุฅูู ูุฑุญูุฉ ูุง ูููู ูู ุงููููู ูููุง ุงูุนุซูุฑ ุนูู ูููุฉ ูุฑุนูุฉ ูู ุงูููุฑุฏุงุชุ ูุชู ุชุฌุฒุฆุฉ ุงููููุฉ ุจุงููุงูู ุนูู ุฃููุง ูุฌูููุฉ -- ูุฐููุ ุนูู ุณุจูู ุงููุซุงูุ ุณุชุชู ุชุฌุฒุฆุฉ ุงููููุฉ "mug" ุนูู ุฃููุง `["[UNK]"]`ุ ููุฐูู ุงููููุฉ "bum" (ุญุชู ูู ูููููุง ุงูุจุฏุก ุจู "b" ู"##u"ุ ูุฅู "##m" ุบูุฑ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุณุชููู ุงูุชุฌุฒุฆุฉ ุงููุงุชุฌุฉ ูู ููุท `["[UNK]"]`ุ ูููุณ `["b", "##u", "[UNK]"]`). ูุฐุง ูุฎุชูู ุนู BPEุ ุงูุฐู ูุงู ุณูุตูู ุงูุฃุญุฑู ุงููุฑุฏูุฉ ุบูุฑ ุงูููุฌูุฏุฉ ูู ุงูููุฑุฏุงุช ุนูู ุฃููุง ูุฌูููุฉ ููุท.

<Tip>

โ๏ธ **ุงูุขู ุฏูุฑู!** ููู ุณุชุชู ุชุฌุฒุฆุฉ ุงููููุฉ "pugs"ุ

</Tip>

## ุชูููุฐ WordPiece [[implementing-wordpiece]]

ุงูุขู ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุชูููุฐ ุฎูุงุฑุฒููุฉ WordPiece. ูุซู BPEุ ูุฐุง ูุฌุฑุฏ ูุซุงู ุชุนููููุ ููู ุชุชููู ูู ุงุณุชุฎุฏุงูู ุนูู ูุฌููุนุฉ ุจูุงูุงุช ูุจูุฑุฉ.

ุณูุณุชุฎุฏู ููุณ ูุฌููุนุฉ ุงูุจูุงูุงุช ููุง ูู ูุซุงู BPE:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

ุฃููุงูุ ูุญุชุงุฌ ุฅูู ุชุฌุฒุฆุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุณุจููุง ุฅูู ูููุงุช. ูุธุฑูุง ูุฃููุง ููุฑุฑ ูุฌุฒุฆ WordPiece (ูุซู BERT)ุ ูุณูุณุชุฎุฏู ุงููุฌุฒุฆ `bert-base-cased` ููุชุฌุฒุฆุฉ ุงููุณุจูุฉ:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ุซู ูุญุณุจ ุชูุฑุงุฑุงุช ูู ูููุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฃุซูุงุก ุงูุชุฌุฒุฆุฉ ุงููุณุจูุฉ:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```
```python out
defaultdict(
    int, {'ูุฐุง': 3, 'ูู': 2, 'ุงู': 1, 'Hugging': 1, 'Face': 1, 'ุฏูุฑุฉ': 1, '.': 4, 'ูุตู': 1, 'ุนู': 1,
    'tokenization': 1, 'ูุณู': 1, 'ูุธูุฑ': 1, 'ุนุฏุฉ': 1, 'tokenizer': 1, 'ุฎูุงุฑุฒููุงุช': 1, 'ูุฃูู': 1,
    ',': 1, 'ุฃูุช': 1, 'ุณูู': 1, 'ุชููู': 1, 'ูุงุฏุฑุงู': 1, 'ุนูู': 1, 'ููู': 1, 'ููู': 1, 'ูู': 1, 'ูู': 1,
    'ูุฏุฑุจูู': 1, 'ู': 1, 'ุชูููุฏ': 1, 'ุฑููุฒ': 1})
```

ููุง ุฑุฃููุง ุณุงุจูุงูุ ุงูุฃุจุฌุฏูุฉ ูู ูุฌููุนุฉ ูุฑูุฏุฉ ููููุฉ ูู ุฌููุน ุงูุญุฑูู ุงูุฃููู ูููููุงุชุ ูุฌููุน ุงูุญุฑูู ุงูุฃุฎุฑู ุงูุชู ุชุธูุฑ ูู ุงููููุงุช ูุณุจููุฉ ุจู `##`:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

ูุถูู ุฃูุถุงู ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุณุชุฎุฏููุง ุงููููุฐุฌ ูู ุจุฏุงูุฉ ุงูููุฑุฏุงุช. ูู ุญุงูุฉ BERTุ ูู ุงููุงุฆูุฉ `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

ุจุนุฏ ุฐููุ ูุญุชุงุฌ ุฅูู ุชูุณูู ูู ูููุฉุ ูุน ุฌููุน ุงูุญุฑูู ุงูุชู ููุณุช ุงูุฃููู ูุณุจููุฉ ุจู `##`:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

ุงูุขูุ ุจุนุฏ ุฃู ุฃุตุจุญูุง ุฌุงูุฒูู ููุชุฏุฑูุจุ ุฏุนูุง ููุชุจ ุฏุงูุฉ ุชููู ุจุญุณุงุจ ูุชูุฌุฉ ูู ุฒูุฌ. ุณูุญุชุงุฌ ุฅูู ุงุณุชุฎุฏุงู ูุฐุง ูู ูู ุฎุทูุฉ ูู ุงูุชุฏุฑูุจ:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุฌุฒุก ูู ูุฐุง ุงููุงููุณ ุจุนุฏ ุงูุชูุณููุงุช ุงูุฃูููุฉ:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

ุงูุขูุ ุฅูุฌุงุฏ ุงูุฒูุฌ ุฐู ุงููุชูุฌุฉ ุงูุฃูุถู ูุง ูุญุชุงุฌ ุณูู ุญููุฉ ุณุฑูุนุฉ:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

ูุฐููุ ุฃูู ุฏูุฌ ูุชุนููู ูู `('a', '##b') -> 'ab'`ุ ููุถูู `'ab'` ุฅูู ุงูููุฑุฏุงุช:

```python
vocab.append("ab")
```

ูููุงุตูุฉ ุฐููุ ูุญุชุงุฌ ุฅูู ุชุทุจูู ูุฐุง ุงูุฏูุฌ ูู ูุงููุณ `splits`. ุฏุนูุง ููุชุจ ุฏุงูุฉ ุฃุฎุฑู ููุฐุง:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

ููููููุง ุฃู ูููู ูุธุฑุฉ ุนูู ูุชูุฌุฉ ุงูุฏูุฌ ุงูุฃูู:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

ุงูุขู ูุฏููุง ูู ูุง ูุญุชุงุฌู ููุญููุฉ ุญุชู ูุชุนูู ุฌููุน ุนูููุงุช ุงูุฏูุฌ ุงูุชู ูุฑูุฏ. ุฏุนูุง ููุฏู ุฅูู ุญุฌู ููุฑุฏุงุช 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

ุจุนุฏ ุฐููุ ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ุงูููุฑุฏุงุช ุงููููุฏุฉ:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

ููุง ูุฑูุ ููุงุฑูุฉ ุจู BPEุ ูุฐุง ุงููุญูู ุงููุบูู ูุชุนูู ุฃุฌุฒุงุก ูู ุงููููุงุช ูุฑููุฒ ุฃุณุฑุน ููููุงู.

<Tip>

๐ก ุงุณุชุฎุฏุงู `train_new_from_iterator()` ุนูู ููุณ ุงููุต ูู ูุคุฏู ุฅูู ููุณ ุงูููุฑุฏุงุช ุจุงูุถุจุท. ูุฐุง ูุฃู ููุชุจุฉ ๐ค Tokenizers ูุง ุชููุฐ WordPiece ููุชุฏุฑูุจ (ูุธุฑูุง ูุฃููุง ูุณูุง ูุชุฃูุฏูู ุชูุงููุง ูู ุฏุงุฎููุชูุง)ุ ูููููุง ุชุณุชุฎุฏู BPE ุจุฏูุงู ูู ุฐูู.

</Tip>

ูุชุญููู ูุต ุฌุฏูุฏุ ูููู ุจุชุญูููู ูุณุจููุงุ ุซู ููุณููุ ุซู ูุทุจู ุฎูุงุฑุฒููุฉ ุงูุชุญููู ุนูู ูู ูููุฉ. ุฃู ุฃููุง ูุจุญุซ ุนู ุฃูุจุฑ ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุจุฏุงูุฉ ุงููููุฉ ุงูุฃููู ูููุณููุงุ ุซู ููุฑุฑ ุงูุนูููุฉ ุนูู ุงูุฌุฒุก ุงูุซุงููุ ูููุฐุง ุจุงููุณุจุฉ ูุจููุฉ ุชูู ุงููููุฉ ูุงููููุงุช ุงูุชุงููุฉ ูู ุงููุต:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

ุฏุนูุง ูุฌุฑุจู ุนูู ูููุฉ ูุงุญุฏุฉ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุฃุฎุฑู ููุณุช ูุฐูู:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

ุงูุขูุ ุฏุนูุง ููุชุจ ุฏุงูุฉ ุชููู ุจุชุญููู ูุต:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

ูููููุง ุชุฌุฑุจุชูุง ุนูู ุฃู ูุต:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

ูุฐุง ูู ุดูุก ุจุงููุณุจุฉ ูุฎูุงุฑุฒููุฉ WordPiece! ุงูุขู ุฏุนูุง ูููู ูุธุฑุฉ ุนูู Unigram.