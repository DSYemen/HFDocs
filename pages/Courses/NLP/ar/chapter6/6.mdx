## WordPiece tokenization 

WordPiece ูู ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒุฆุฉ ุงูุชู ุทูุฑุชูุง Google ูุชูููุฏ ุงูุทุฑูู ูุจุฑูุงูุฌ BERT. ููุฏ ุฃุนูุฏ ุงุณุชุฎุฏุงูู ููุฐ ุฐูู ุงูุญูู ูู ุนุฏุฏ ูููู ูู ููุงุฐุฌ ุงููุญูู ุงููุณุชูุฏุฉ ุฅูู BERTุ ูุซู DistilBERT ูMobileBERT ูFunnel Transformers ูMPNET. ุฅูู ูุดุงุจู ุฌุฏูุง ูู BPE ูู ุญูุซ ุงูุชุฏุฑูุจุ ูููู ุชุฌุฒุฆุฉ ุงููููุงุช ุงููุนููุฉ ุชุชู ุจุดูู ูุฎุชูู. 

<Youtube id="qpv6ms_t_1A"/> 

<Tip> 

๐ก ูุบุทู ูุฐุง ุงููุณู WordPiece ุจุดูู ูุชุนููุ ููุตู ุฅูู ุญุฏ ุฅุธูุงุฑ ุงูุชูููุฐ ุงููุงูู. ููููู ุชุฎุทู ูุง ุชุจูู ุฅุฐุง ููุช ุชุฑูุฏ ููุท ูุธุฑุฉ ุนุงูุฉ ุนูู ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒุฆุฉ. 

</Tip> 

## ุฎูุงุฑุฒููุฉ ุงูุชุฏุฑูุจ 

<Tip warning={true}> 

โ๏ธ ูู ุชูุชุญ Google ูุทูููุง ูุตุฏุฑ ุชูููุฐูุง ูุฎูุงุฑุฒููุฉ ุงูุชุฏุฑูุจ ูู WordPieceุ ูุฐุง ูุฅู ูุง ููู ูู ุฃูุถู ุชุฎููู ูุฏููุง ุจูุงุกู ุนูู ุงูุฃุฏุจูุงุช ุงูููุดูุฑุฉ. ูุฏ ูุง ูููู ุฏููููุง ุจูุณุจุฉ 100%. 

</Tip> 

ูุซู BPEุ ูุจุฏุฃ WordPiece ูู ููุฑุฏุงุช ุตุบูุฑุฉ ุจูุง ูู ุฐูู ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุณุชุฎุฏููุง ุงููููุฐุฌ ูุงูุฃุจุฌุฏูุฉ ุงูุฃูููุฉ. ูุธุฑูุง ูุฃูู ูุญุฏุฏ ุงููููุงุช ุงููุฑุนูุฉ ุนู ุทุฑูู ุฅุถุงูุฉ ุจุงุฏุฆุฉ (ูุซู `##` ูู BERT)ุ ูุชู ุชูุณูู ูู ูููุฉ ูู ุงูุจุฏุงูุฉ ุนู ุทุฑูู ุฅุถุงูุฉ ุชูู ุงูุจุงุฏุฆุฉ ุฅูู ุฌููุน ุงูุฃุญุฑู ุฏุงุฎู ุงููููุฉ. ูุฐููุ ุนูู ุณุจูู ุงููุซุงูุ ูุชู ุชูุณูู "ูููุฉ" ุนูู ุงููุญู ุงูุชุงูู: 

``` 
w ##o ##r ##d
``` 

ูุจุงูุชุงููุ ุชุญุชูู ุงูุฃุจุฌุฏูุฉ ุงูุฃูููุฉ ุนูู ุฌููุน ุงูุฃุญุฑู ุงูููุฌูุฏุฉ ูู ุจุฏุงูุฉ ุงููููุฉ ูุงูุฃุญุฑู ุงูููุฌูุฏุฉ ุฏุงุฎู ุงููููุฉ ุงูุชู ุชุณุจููุง ุจุงุฏุฆุฉ WordPiece. 

ุซูุ ูุซู BPEุ ูุชุนูู WordPiece ููุงุนุฏ ุงูุฏูุฌ. ุงููุฑู ุงูุฑุฆูุณู ูู ุทุฑููุฉ ุงุฎุชูุงุฑ ุงูุฒูุฌ ุงูุฐู ุณูุชู ุฏูุฌู. ุจุฏูุงู ูู ุชุญุฏูุฏ ุฃูุซุฑ ุงูุฃุฒูุงุฌ ุชูุฑุงุฑูุงุ ูุญุณุจ WordPiece ุฏุฑุฌุฉ ููู ุฒูุฌุ ุจุงุณุชุฎุฏุงู ุงูุตูุบุฉ ุงูุชุงููุฉ: 

$$\ mathrm {score} = (\ mathrm {freq_ {of_pair}}) / (\ mathrm {freq_ {of_first_element}} \ times \ mathrm {freq_ {of_second_element}})$$ 

ูู ุฎูุงู ูุณูุฉ ุชูุฑุงุฑ ุงูุฒูุฌ ุนูู ุญุงุตู ุถุฑุจ ุชูุฑุงุฑุงุช ูู ุฌุฒุก ูููุ ุชุนุทู ุงูุฎูุงุฑุฒููุฉ ุงูุฃููููุฉ ูุฏูุฌ ุงูุฃุฒูุงุฌ ุญูุซ ุชููู ุงูุฃุฌุฒุงุก ุงููุฑุฏูุฉ ุฃูู ุชูุฑุงุฑูุง ูู ุงูููุฑุฏุงุช. ุนูู ุณุจูู ุงููุซุงูุ ูู ูููู ุจุงูุถุฑูุฑุฉ ุจุฏูุฌ `("un"ุ "##able")` ุญุชู ุฅุฐุง ุธูุฑ ูุฐุง ุงูุฒูุฌ ุจุดูู ูุชูุฑุฑ ุฌุฏูุง ูู ุงูููุฑุฏุงุชุ ูุฃู ููุง ุงูุฒูุฌูู "un" ู"##able" ุณูุธูุฑ ุนูู ุงูุฃุฑุฌุญ ูู ุงูุนุฏูุฏ ูู ุงููููุงุช ุงูุฃุฎุฑู ูุณูููู ูููุง ุชูุฑุงุฑ ุนุงูู. ุนูู ุงููููุถ ูู ุฐููุ ูู ุงููุญุชูู ุฃู ูุชู ุฏูุฌ ุฒูุฌ ูุซู `("hu"ุ "##gging")` ุจุดูู ุฃุณุฑุน (ุจุงูุชุฑุงุถ ุฃู ูููุฉ "hugging" ุชุธูุฑ ุบุงูุจูุง ูู ุงูููุฑุฏุงุช) ูุธุฑูุง ูุฃู "hu" ู"##gging" ูู ุงููุญุชูู ุฃู ููููุง ุฃูู ุชูุฑุงุฑูุง ุจุดูู ูุฑุฏู. 

ููููู ูุธุฑุฉ ุนูู ููุณ ุงูููุฑุฏุงุช ุงูุชู ุงุณุชุฎุฏููุงูุง ูู ูุซุงู ุงูุชุฏุฑูุจ BPE: 

``` 
("hug"ุ 10)ุ ("pug"ุ 5)ุ ("pun"ุ 12)ุ ("bun"ุ 4)ุ ("hugs"ุ 5)
``` 

ุณุชููู ุงูุงููุณุงูุงุช ููุง: 

``` 
("h" "##u" "##g"ุ 10)ุ ("p" "##u" "##g"ุ 5)ุ ("p" "##u" "##n"ุ 12)ุ ("b" "##u" "##n"ุ 4)ุ ("h" "##u" "##g" "##s"ุ 5)
``` 

ูุฐูู ุณุชููู ุงูููุฑุฏุงุช ุงูุฃูููุฉ `["b"ุ "h"ุ "p"ุ "##g"ุ "##n"ุ "##s"ุ "##u"]` (ุฅุฐุง ุชุฌุงูููุง ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุขู). ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุง ูู `("##u"ุ "##g")` (ูุธูุฑ 20 ูุฑุฉ)ุ ูููู ุงูุชุฑุฏุฏ ุงููุฑุฏู ูู `"##u"` ูุฑุชูุน ุฌุฏูุงุ ูุฐุง ูุฅู ุฏุฑุฌุชู ููุณุช ุงูุฃุนูู (1 / 36). ุฌููุน ุงูุฃุฒูุงุฌ ูุน `"##u"` ููุง ูู ุงููุงูุน ููุณ ุงูุฏุฑุฌุฉ (1 / 36)ุ ูุฐุง ูุฅู ุฃูุถู ุฏุฑุฌุฉ ุชุฐูุจ ุฅูู ุงูุฒูุฌ `("##g"ุ "##s")` - ุงููุญูุฏ ุจุฏูู `"##u"` - ูู 1/20ุ ูุงููุงุนุฏุฉ ุงูุฃููู ุงูุชู ูุชู ุชุนูููุง ูู `("##g"ุ "##s") -> ("##gs")`. 

ูุงุญุธ ุฃูู ุนูุฏ ุงูุฏูุฌุ ูููู ุจุฅุฒุงูุฉ `##` ุจูู ุงูุฑูุฒููุ ูุฐุง ูุฅููุง ูุถูู `"##gs"` ุฅูู ุงูููุฑุฏุงุช ููุทุจู ุงูุฏูุฌ ูู ูููุงุช ุงููุฌููุนุฉ: 

``` 
ููุฑุฏุงุช: ["b"ุ "h"ุ "p"ุ "##g"ุ "##n"ุ "##s"ุ "##u"ุ "##gs"]
ุงููุฌููุนุฉ: ("h" "##u" "##g"ุ 10)ุ ("p" "##u" "##g"ุ 5)ุ ("p" "##u" "##n"ุ 12)ุ ("b" "##u" "##n"ุ 4)ุ ("h" "##u" "##gs"ุ 5)
``` 

ูู ูุฐู ุงููุฑุญูุฉุ ูุธูุฑ `"##u"` ูู ุฌููุน ุงูุฃุฒูุงุฌ ุงููุญุชููุฉุ ูุฐุง ูุฅููุง ุฌููุนูุง ุชูุชูู ุจููุณ ุงูุฏุฑุฌุฉ. ุฏุนูุง ูููู ุฃูู ูู ูุฐู ุงูุญุงูุฉุ ูุชู ุฏูุฌ ุงูุฒูุฌ ุงูุฃููุ ูุฐุง `("h"ุ "##u") -> "hu"`. ูุฐุง ูุฃุฎุฐูุง ุฅูู: 

``` 
ููุฑุฏุงุช: ["b"ุ "h"ุ "p"ุ "##g"ุ "##n"ุ "##s"ุ "##u"ุ "##gs"ุ "hu"]
ุงููุฌููุนุฉ: ("hu" "##g"ุ 10)ุ ("p" "##u" "##g"ุ 5)ุ ("p" "##u" "##n"ุ 12)ุ ("b" "##u" "##n"ุ 4)ุ ("hu" "##gs"ุ 5)
``` 

ุซู ูุชู ูุดุงุฑูุฉ ุงูุฏุฑุฌุฉ ุงูุชุงููุฉ ูู ูุจู `("hu"ุ "##g")` ู`("hu"ุ "##gs")` (ูุน 1/15ุ ููุงุฑูุฉ ุจู 1/21 ูุฌููุน ุงูุฃุฒูุงุฌ ุงูุฃุฎุฑู)ุ ูุฐุง ูุฅู ุฃูู ุฒูุฌ ูู ุงูุฏุฑุฌุฉ ุงูุฃูุจุฑ ูู ุงูุฐู ูุชู ุฏูุฌู: 

``` 
ููุฑุฏุงุช: ["b"ุ "h"ุ "p"ุ "##g"ุ "##n"ุ "##s"ุ "##u"ุ "##gs"ุ "hu"ุ "hug"]
ุงููุฌููุนุฉ: ("hug"ุ 10)ุ ("p" "##u" "##g"ุ 5)ุ ("p" "##u" "##n"ุ 12)ุ ("b" "##u" "##n"ุ 4)ุ ("hu" "##gs"ุ 5)
``` 

ููุญู ููุงุตู ูุซู ูุฐุง ุญุชู ูุตู ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุทููุจ. 

<Tip> 

โ๏ธ **ุงูุขู ุฏูุฑู!** ูุง ูู ูุงุนุฏุฉ ุงูุฏูุฌ ุงูุชุงููุฉุ 

</Tip> 

## ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒุฆุฉ 

ุชุฎุชูู ุงูุชุฌุฒุฆุฉ ูู WordPiece ูBPE ูู ุฃู WordPiece ูุญูุธ ููุท ุงูููุฑุฏุงุช ุงูููุงุฆูุฉุ ูููุณ ููุงุนุฏ ุงูุฏูุฌ ุงูุชู ุชู ุชุนูููุง. ุจุฏุกูุง ูู ุงููููุฉ ุงูุชู ุณูุชู ุชุฌุฒุฆุชูุงุ ูุฌุฏ WordPiece ุฃุทูู ูููุฉ ูุฑุนูุฉ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ุซู ูููู ุจุงูุชูุณูู ุนูููุง. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ุงุณุชุฎุฏููุง ุงูููุฑุฏุงุช ุงูุชู ุชู ุชุนูููุง ูู ุงููุซุงู ุฃุนูุงูุ ูุจุงููุณุจุฉ ููููุฉ "ุงุญุชุถุงู" ุฃุทูู ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุงูุจุฏุงูุฉ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุช ูู "ุงุญุชุถุงู"ุ ูุฐุง ูููู ุจุงูุชูุณูู ููุงู ููุญุตู ุนูู `["ุงุญุชุถุงู"ุ "##s"]`. ุซู ููุงุตู ูุน `"##s"`ุ ูุงูุชู ูู ูู ุงูููุฑุฏุงุชุ ูุฐุง ูุฅู ุชุฌุฒุฆุฉ ูููุฉ "ุงุญุชุถุงู" ูู `["ุงุญุชุถุงู"ุ "##s"]`. 

ูุน BPEุ ูููุง ูุฏ ุทุจููุง ุนูููุงุช ุงูุฏูุฌ ุงูุชู ุชุนูููุงูุง ุจุงูุชุฑุชูุจ ููููุง ุจุชุฌุฒุฆุฉ ูุฐุง ุนูู ุฃูู `["hu"ุ "##gs"]`ุ ูุฐุง ูุฅู ุงูุชุฑููุฒ ูุฎุชูู. 

ูููุซุงู ุขุฎุฑุ ุฏุนูุง ูุฑู ููู ุชุชู ุชุฌุฒุฆุฉ ูููุฉ "bugs". "b" ูู ุฃุทูู ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุจุฏุงูุฉ ุงููููุฉ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุฐุง ูููู ุจุงูุชูุณูู ููุงู ููุญุตู ุนูู `["b"ุ "##ugs"]`. ุซู `"##u"` ูู ุฃุทูู ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุจุฏุงูุฉ "##ugs" ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุชุ ูุฐุง ูููู ุจุงูุชูุณูู ููุงู ููุญุตู ุนูู `["b"ุ "##u"ุ "##gs"]`. ุฃุฎูุฑูุงุ `"##gs"` ููุฌูุฏ ูู ุงูููุฑุฏุงุชุ ูุฐุง ูุฅู ูุฐู ุงููุงุฆูุฉ ุงูุฃุฎูุฑุฉ ูู ุชุฌุฒุฆุฉ ูููุฉ "bugs". 

ุนูุฏูุง ุชุตู ุงูุชุฌุฒุฆุฉ ุฅูู ูุฑุญูุฉ ูุง ูููู ูููุง ุงูุนุซูุฑ ุนูู ูููุฉ ูุฑุนูุฉ ูู ุงูููุฑุฏุงุชุ ูุชู ุชุฌุฒุฆุฉ ุงููููุฉ ุจุฃููููุง ุนูู ุฃููุง ุบูุฑ ูุนุฑููุฉ - ูุฐุงุ ุนูู ุณุจูู ุงููุซุงูุ ูุชู ุชุฌุฒุฆุฉ ูููุฉ "mug" ุนูู ุฃููุง `["[UNK]"]`ุ ููุง ูู ุงูุญุงู ูุน ูููุฉ "bum" (ุญุชู ุฅุฐุง ูุงู ุจุฅููุงููุง ุงูุจุฏุก ุจู "b" ู"##u"ุ ูุฅู "##m" ุบูุฑ ููุฌูุฏ ูู ุงูููุฑุฏุงุชุ ูุณุชููู ุงูุชุฌุฒุฆุฉ ุงููุงุชุฌุฉ ุจุจุณุงุทุฉ `["[UNK]"]`ุ ูููุณ `["b"ุ "##u"ุ "[UNK]"]`). ูุฐุง ุงุฎุชูุงู ุขุฎุฑ ุนู BPEุ ูุงูุฐู ุณูุตูู ุงูุฃุญุฑู ุงููุฑุฏูุฉ ุบูุฑ ุงูููุฌูุฏุฉ ูู ุงูููุฑุฏุงุช ููุท ุนูู ุฃููุง ุบูุฑ ูุนุฑููุฉ. 

<Tip> 

โ๏ธ **ุงูุขู ุฏูุฑู!** ููู ุณูุชู ุชุฌุฒุฆุฉ ูููุฉ "pugs"ุ 

</Tip>
## ุชุทุจูู WordPiece

ุงูุขูุ ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุชุทุจูู ูุฎูุงุฑุฒููุฉ WordPiece. ูุซู BPEุ ูุฐุง ููุท ูุฃุบุฑุงุถ ุชุนููููุฉุ ููู ุชุชููู ูู ุงุณุชุฎุฏุงูู ุนูู ูุฌููุนุฉ ุจูุงูุงุช ูุจูุฑุฉ.

ุณูุณุชุฎุฏู ููุณ ูุฌููุนุฉ ุงูุจูุงูุงุช ููุง ูู ูุซุงู BPE:

```python
corpus = [
"This is the Hugging Face Course.",
"This chapter is about tokenization.",
"This section shows several tokenizer algorithms.",
"Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

ุฃููุงูุ ูุญุชุงุฌ ุฅูู ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุณุจููุง ุฅูู ูููุงุช. ูุธุฑูุง ูุฃููุง ููุฑุฑ ุนูููุฉ ุชูููููุฒุฑ WordPiece (ูุซู BERT)ุ ูุณูุณุชุฎุฏู ุชูููููุฒุฑ "bert-base-cased" ููุชูุณูู ุงููุณุจู:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ุจุนุฏ ุฐููุ ูุญุณุจ ุชูุฑุงุฑุงุช ูู ูููุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฃุซูุงุก ุงูุชูุณูู ุงููุณุจู:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
new_words = [word for word, offset in words_with_offsets]
for word in new_words:
word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

ููุง ุฑุฃููุง ุณุงุจููุงุ ุงูุฃุจุฌุฏูุฉ ูู ุงููุฌููุนุฉ ุงููุฑูุฏุฉ ุงูููููุฉ ูู ุฌููุน ุงูุฃุญุฑู ุงูุฃููู ูู ุงููููุงุชุ ูุฌููุน ุงูุฃุญุฑู ุงูุฃุฎุฑู ุงูุชู ุชุธูุฑ ูู ุงููููุงุช ุงูุชู ุชุณุจููุง "##":

```python
alphabet = []
for word in word_freqs.keys():
if word[0] not in alphabet:
alphabet.append(word[0])
for letter in word[1:]:
if f"##{letter}" not in alphabet:
alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
'##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
'w', 'y']
```

ูุถูู ุฃูุถูุง ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุณุชุฎุฏููุง ุงููููุฐุฌ ูู ุจุฏุงูุฉ ูุฐุง ุงููุงููุณ. ูู ุญุงูุฉ BERTุ ุชููู ุงููุงุฆูุฉ ููุง ููู: "["[PAD]"ุ "[UNK]"ุ "[CLS]"ุ "[SEP]"ุ "[MASK]"]:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

ุจุนุฏ ุฐููุ ูุญุชุงุฌ ุฅูู ุชูุณูู ูู ูููุฉุ ูุน ุฅุถุงูุฉ "##" ูุจู ุฌููุน ุงูุฃุญุฑู ูุง ุนุฏุง ุงูุญุฑู ุงูุฃูู:

```python
splits = {
word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
for word in word_freqs.keys()
}
```

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญูุง ุฌุงูุฒูู ููุชุฏุฑูุจุ ุฏุนููุง ููุชุจ ุฏุงูุฉ ุชููู ุจุญุณุงุจ ุฏุฑุฌุฉ ูู ุฒูุฌ. ุณูุญุชุงุฌ ุฅูู ุงุณุชุฎุฏุงู ูุฐุง ูู ูู ุฎุทูุฉ ูู ุงูุชุฏุฑูุจ:

```python
def compute_pair_scores(splits):
letter_freqs = defaultdict(int)
pair_freqs = defaultdict(int)
for word, freq in word_freqs.items():
split = splits[word]
if len(split) == 1:
letter_freqs[split[0]] += freq
continue
for i in range(len(split) - 1):
pair = (split[i], split[i + 1])
letter_freqs[split[i]] += freq
pair_freqs[pair] += freq
letter_freqs[split[-1]] += freq

scores = {
pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
for pair, freq in pair_freqs.items()
}
return scores
```

ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุฌุฒุก ูู ูุฐุง ุงููุงููุณ ุจุนุฏ ุงูุงููุณุงูุงุช ุงูุฃูููุฉ:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
print(f"{key}: {pair_scores[key]}")
if i >= 5:
break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

ุงูุขูุ ูุง ูุณุชุบุฑู ุงูุนุซูุฑ ุนูู ุงูุฒูุฌ ุฐู ุงูุฏุฑุฌุฉ ุงูุฃุนูู ุณูู ุญููุฉ ุณุฑูุนุฉ:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
if max_score is None or max_score < score:
best_pair = pair
max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

ูุฐุง ูุฅู ุฃูู ุฏูุฌ ูุชุนููู ูู "('a', '##b') -> 'ab'"ุ ููุถูู "ab'" ุฅูู ุงููุงููุณ:

```python
vocab.append("ab")
```

ูููุงุตูุฉ ุฐููุ ูุญุชุงุฌ ุฅูู ุชุทุจูู ูุฐุง ุงูุฏูุฌ ูู ูุงููุณ "splits". ุฏุนููุง ููุชุจ ุฏุงูุฉ ุฃุฎุฑู ููุฐุง ุงูุบุฑุถ:

```python
def merge_pair(a, b, splits):
for word in word_freqs:
split = splits[word]
if len(split) == 1:
continue
i = 0
while i < len(split) - 1:
if split[i] == a and split[i + 1] == b:
merge = a + b[2:] if b.startswith("##") else a + b
split = split[:i] + [merge] + split[i + 2 :]
else:
i += 1
splits[word] = split
return splits
```

ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ูุชูุฌุฉ ุงูุฏูุฌ ุงูุฃูู:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญ ูุฏููุง ูู ูุง ูุญุชุงุฌ ุฅูููุ ูููููุง ุงูุงุณุชูุฑุงุฑ ูู ุงูุญููุฉ ุญุชู ูุชุนูู ุฌููุน ุนูููุงุช ุงูุฏูุฌ ุงูุชู ูุฑูุฏ. ุฏุนููุง ููุฏู ุฅูู ุญุฌู ูุงููุณ ูุจูุบ 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
scores = compute_pair_scores(splits)
best_pair, max_score = "", None
for pair, score in scores.items():
if max_score is None or max_score < score:
best_pair = pair
max_score = score
splits = merge_pair(*best_pair, splits)
new_token = (
best_pair[0] + best_pair[1][2:]
if best_pair[1].startswith("##")
else best_pair[0] + best_pair[1]
)
vocab.append(new_token)
```

ุจุนุฏ ุฐููุ ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ุงููุงููุณ ุงููุงุชุฌ:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
'##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
'##ut']
```

ููุง ูุฑูุ ููุงุฑูุฉ ุจู BPEุ ูุชุนูู ูุฐุง ุงูุชููููุฒุฑ ุฃุฌุฒุงุก ุงููููุงุช ูุฑููุฒ ุจุดูู ุฃุณุฑุน ููููุงู.

<Tip>
๐ก ุงุณุชุฎุฏุงู "train_new_from_iterator()" ุนูู ููุณ ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ูุคุฏู ุฅูู ููุณ ุงููุงููุณ ุจุงูุถุจุท. ููุฑุฌุน ุฐูู ุฅูู ุฃู ููุชุจุฉ ๐ค Tokenizers ูุง ุชููุฐ WordPiece ููุชุฏุฑูุจ (ูุธุฑูุง ูุฃููุง ูุณูุง ูุชุฃูุฏูู ุชูุงููุง ูู ุชูุงุตูููุง ุงูุฏุงุฎููุฉ)ุ ูููููุง ุชุณุชุฎุฏู BPE ุจุฏูุงู ูู ุฐูู.
</Tip>

ูุชูููููุฒ ูุต ุฌุฏูุฏุ ูููู ุจุชูุณููู ูุณุจููุงุ ุซู ููุณููุ ุซู ูุทุจู ุฎูุงุฑุฒููุฉ ุงูุชููููุฒ ุนูู ูู ูููุฉ. ููุฐุง ูุนูู ุฃููุง ูุจุญุซ ุนู ุฃูุจุฑ ูููุฉ ูุฑุนูุฉ ุชุจุฏุฃ ูู ุจุฏุงูุฉ ุงููููุฉ ุงูุฃููู ูููุณููุงุ ุซู ููุฑุฑ ุงูุนูููุฉ ุนูู ุงูุฌุฒุก ุงูุซุงููุ ูููุฐุง ูุจููุฉ ุชูู ุงููููุฉ ูุงููููุงุช ุงูุชุงููุฉ ูู ุงููุต:

```python
def encode_word(word):
tokens = []
while len(word) > 0:
i = len(word)
while i > 0 and word[:i] not in vocab:
i -= 1
if i == 0:
return ["[UNK]"]
tokens.append(word[:i])
word = word[i:]
if len(word) > 0:
word = f"##{word}"
return tokens
```

ุฏุนููุง ูุฎุชุจุฑูุง ุนูู ูููุฉ ููุฌูุฏุฉ ูู ุงููุงููุณุ ูุฃุฎุฑู ุบูุฑ ููุฌูุฏุฉ:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

ุงูุขูุ ุฏุนููุง ููุชุจ ุฏุงูุฉ ุชููู ุจุชูููููุฒ ูุต:

```python
def tokenize(text):
pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
pre_tokenized_text = [word for word, offset in pre_tokenize_result]
encoded_words = [encode_word(word) for word in pre_tokenized_text]
return sum(encoded_words, [])
```

ูููููุง ุชุฌุฑุจุชูุง ุนูู ุฃู ูุต:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
'##e', '[UNK]']
```

ูุฐุง ูู ุดูุก ุนู ุฎูุงุฑุฒููุฉ WordPiece! ุงูุขู ุฏุนููุง ูููู ูุธุฑุฉ ุนูู Unigram.