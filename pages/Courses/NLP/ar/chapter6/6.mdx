# تجزئة WordPiece [[wordpiece-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
]} />

WordPiece هو خوارزمية التجزئة التي طورتها جوجل لتدريب BERT المسبق. وقد تم إعادة استخدامه منذ ذلك الحين في العديد من نماذج المحول القائمة على BERT، مثل DistilBERT وMobileBERT وFunnel Transformers وMPNET. إنه مشابه جدًا لـ BPE من حيث التدريب، ولكن التجزئة الفعلية تتم بشكل مختلف.

<Youtube id="qpv6ms_t_1A"/>

<Tip>

💡 يغطي هذا القسم WordPiece بعمق، حيث يصل إلى حد عرض التنفيذ الكامل. يمكنك تخطي هذا القسم والانتقال إلى النهاية إذا كنت تريد فقط نظرة عامة على خوارزمية التجزئة.

</Tip>

## خوارزمية التدريب [[training-algorithm]]

<Tip warning={true}>

⚠️ لم تقم جوجل بإتاحة خوارزمية التدريب الخاصة بـ WordPiece، لذلك ما يلي هو أفضل تخمين لدينا بناءً على الأدبيات المنشورة. قد لا يكون دقيقًا بنسبة 100%.

</Tip>

مثل BPE، تبدأ WordPiece من مفردات صغيرة تشمل الرموز الخاصة التي يستخدمها النموذج والأبجدية الأولية. نظرًا لأنه يحدد الكلمات الفرعية عن طريق إضافة بادئة (مثل `##` لـ BERT)، يتم تقسيم كل كلمة في البداية عن طريق إضافة تلك البادئة إلى جميع الأحرف داخل الكلمة. لذلك، على سبيل المثال، يتم تقسيم الكلمة "word" على النحو التالي:

```
w ##o ##r ##d
```

وبالتالي، تحتوي الأبجدية الأولية على جميع الأحرف الموجودة في بداية الكلمة والأحرف الموجودة داخل الكلمة مسبوقة ببادئة WordPiece.

ثم، مرة أخرى مثل BPE، تتعلم WordPiece قواعد الدمج. الفرق الرئيسي هو طريقة اختيار الزوج المراد دمجه. بدلاً من اختيار الزوج الأكثر تكرارًا، تحسب WordPiece درجة لكل زوج، باستخدام الصيغة التالية:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

من خلال قسمة تكرار الزوج على حاصل ضرب تكرارات كل جزء منه، تعطي الخوارزمية الأولوية لدمج الأزواج حيث تكون الأجزاء الفردية أقل تكرارًا في المفردات. على سبيل المثال، لن تقوم بالضرورة بدمج `("un", "##able")` حتى إذا كان هذا الزوج متكررًا جدًا في المفردات، لأن كلا الزوجين `"un"` و`"##able"` من المحتمل أن يظهرا في العديد من الكلمات الأخرى ولهما تكرار عالٍ. على النقيض من ذلك، من المحتمل أن يتم دمج زوج مثل `("hu", "##gging")` بشكل أسرع (على افتراض أن الكلمة "hugging" تظهر كثيرًا في المفردات) حيث من المحتمل أن يكون تكرار كل من `"hu"` و`"##gging"` أقل بشكل فردي.

دعنا ننظر إلى نفس المفردات التي استخدمناها في مثال تدريب BPE:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ستكون التقسيمات هنا على النحو التالي:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

لذلك ستكون المفردات الأولية هي `["b", "h", "p", "##g", "##n", "##s", "##u"]` (إذا تجاهلنا الرموز الخاصة الآن). الزوج الأكثر تكرارًا هو `("##u", "##g")` (موجود 20 مرة)، ولكن التكرار الفردي لـ `"##u"` مرتفع جدًا، لذلك ليست درجته هي الأعلى (فهي 1 / 36). جميع الأزواج التي تحتوي على `"##u"` لديها نفس الدرجة (1 / 36)، لذلك أفضل درجة هي للزوج `("##g", "##s")` -- الوحيد بدون `"##u"` -- عند 1 / 20، وقاعدة الدمج الأولى المكتسبة هي `("##g", "##s") -> ("##gs")`.

لاحظ أنه عند الدمج، نزيل `##` بين الرمزين، لذلك نضيف `"##gs"` إلى المفردات ونطبق الدمج في كلمات المجموعة:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

في هذه المرحلة، `"##u"` موجود في جميع الأزواج الممكنة، لذلك تنتهي جميعها بنفس الدرجة. لنفترض أنه في هذه الحالة، يتم دمج الزوج الأول، لذلك `("h", "##u") -> "hu"`. هذا يأخذنا إلى:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ثم تكون الدرجة التالية الأفضل مشتركة بين `("hu", "##g")` و`("hu", "##gs")` (بدرجة 1/15، مقارنة بـ 1/21 لجميع الأزواج الأخرى)، لذلك يتم دمج الزوج الأول ذو الدرجة الأكبر:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ونستمر على هذا النحو حتى نصل إلى حجم المفردات المطلوب.

<Tip>

✏️ **الآن دورك!** ما هي قاعدة الدمج التالية؟

</Tip>

## خوارزمية التجزئة [[tokenization-algorithm]]

تختلف التجزئة في WordPiece وBPE في أن WordPiece يحفظ المفردات النهائية فقط، وليس قواعد الدمج المكتسبة. بدءًا من الكلمة المراد تجزئتها، يجد WordPiece أطول كلمة فرعية موجودة في المفردات، ثم يقسم عليها. على سبيل المثال، إذا استخدمنا المفردات المكتسبة في المثال أعلاه، بالنسبة للكلمة "hugs" فإن أطول كلمة فرعية تبدأ من البداية وتكون داخل المفردات هي "hug"، لذلك نقسم عندها ونحصل على `["hug", "##s"]`. ثم نستمر مع `"##s"`، والتي تكون موجودة في المفردات، لذلك تكون تجزئة الكلمة "hugs" هي `["hug", "##s"]`.

مع BPE، كنا سنطبق قواعد الدمج المكتسبة بالترتيب ونقوم بتجزئة الكلمة على أنها `["hu", "##gs"]`، لذلك يكون الترميز مختلفًا.

كمثال آخر، دعنا نرى كيف ستتم تجزئة الكلمة "bugs". تكون الكلمة "b" هي أطول كلمة فرعية تبدأ من بداية الكلمة وتكون موجودة في المفردات، لذلك نقسم عندها ونحصل على `["b", "##ugs"]`. ثم تكون الكلمة "##u" هي أطول كلمة فرعية تبدأ من بداية الكلمة "##ugs" وتكون موجودة في المفردات، لذلك نقسم عندها ونحصل على `["b", "##u, "##gs"]`. وأخيرًا، تكون الكلمة "##gs" موجودة في المفردات، لذلك تكون هذه القائمة الأخيرة هي تجزئة الكلمة "bugs".

عندما تصل التجزئة إلى مرحلة لا يكون من الممكن فيها العثور على كلمة فرعية في المفردات، يتم تجزئة الكلمة بالكامل على أنها مجهولة -- لذلك، على سبيل المثال، ستتم تجزئة الكلمة "mug" على أنها `["[UNK]"]`، وكذلك الكلمة "bum" (حتى لو يمكننا البدء بـ "b" و"##u"، فإن "##m" غير موجودة في المفردات، وستكون التجزئة الناتجة هي فقط `["[UNK]"]`، وليس `["b", "##u", "[UNK]"]`). هذا يختلف عن BPE، الذي كان سيصنف الأحرف الفردية غير الموجودة في المفردات على أنها مجهولة فقط.

<Tip>

✏️ **الآن دورك!** كيف ستتم تجزئة الكلمة "pugs"؟

</Tip>

## تنفيذ WordPiece [[implementing-wordpiece]]

الآن دعنا نلقي نظرة على تنفيذ خوارزمية WordPiece. مثل BPE، هذا مجرد مثال تعليمي، ولن تتمكن من استخدامه على مجموعة بيانات كبيرة.

سنستخدم نفس مجموعة البيانات كما في مثال BPE:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

أولاً، نحتاج إلى تجزئة مجموعة البيانات مسبقًا إلى كلمات. نظرًا لأننا نكرر مجزئ WordPiece (مثل BERT)، فسنستخدم المجزئ `bert-base-cased` للتجزئة المسبقة:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ثم نحسب تكرارات كل كلمة في مجموعة البيانات أثناء التجزئة المسبقة:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```
```python out
defaultdict(
    int, {'هذا': 3, 'هو': 2, 'ال': 1, 'Hugging': 1, 'Face': 1, 'دورة': 1, '.': 4, 'فصل': 1, 'عن': 1,
    'tokenization': 1, 'قسم': 1, 'يظهر': 1, 'عدة': 1, 'tokenizer': 1, 'خوارزميات': 1, 'نأمل': 1,
    ',': 1, 'أنت': 1, 'سوف': 1, 'تكون': 1, 'قادراً': 1, 'على': 1, 'فهم': 1, 'كيف': 1, 'هم': 1, 'هم': 1,
    'مدربون': 1, 'و': 1, 'توليد': 1, 'رموز': 1})
```

كما رأينا سابقاً، الأبجدية هي مجموعة فريدة مكونة من جميع الحروف الأولى للكلمات، وجميع الحروف الأخرى التي تظهر في الكلمات مسبوقة بـ `##`:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

نضيف أيضاً الرموز الخاصة التي يستخدمها النموذج في بداية المفردات. في حالة BERT، هي القائمة `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

بعد ذلك، نحتاج إلى تقسيم كل كلمة، مع جميع الحروف التي ليست الأولى مسبوقة بـ `##`:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

الآن، بعد أن أصبحنا جاهزين للتدريب، دعنا نكتب دالة تقوم بحساب نتيجة كل زوج. سنحتاج إلى استخدام هذا في كل خطوة من التدريب:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

دعنا نلقي نظرة على جزء من هذا القاموس بعد التقسيمات الأولية:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

الآن، إيجاد الزوج ذو النتيجة الأفضل لا يحتاج سوى حلقة سريعة:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

لذلك، أول دمج نتعلمه هو `('a', '##b') -> 'ab'`، ونضيف `'ab'` إلى المفردات:

```python
vocab.append("ab")
```

لمواصلة ذلك، نحتاج إلى تطبيق هذا الدمج في قاموس `splits`. دعنا نكتب دالة أخرى لهذا:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

ويمكننا أن نلقي نظرة على نتيجة الدمج الأول:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

الآن لدينا كل ما نحتاجه للحلقة حتى نتعلم جميع عمليات الدمج التي نريد. دعنا نهدف إلى حجم مفردات 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

بعد ذلك، يمكننا إلقاء نظرة على المفردات المولدة:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

كما نرى، مقارنة بـ BPE، هذا المحلل اللغوي يتعلم أجزاء من الكلمات كرموز أسرع قليلاً.

<Tip>

💡 استخدام `train_new_from_iterator()` على نفس النص لن يؤدي إلى نفس المفردات بالضبط. هذا لأن مكتبة 🤗 Tokenizers لا تنفذ WordPiece للتدريب (نظرًا لأننا لسنا متأكدين تمامًا من داخليتها)، ولكنها تستخدم BPE بدلاً من ذلك.

</Tip>

لتحليل نص جديد، نقوم بتحليله مسبقًا، ثم نقسمه، ثم نطبق خوارزمية التحليل على كل كلمة. أي أننا نبحث عن أكبر كلمة فرعية تبدأ في بداية الكلمة الأولى ونقسمها، ثم نكرر العملية على الجزء الثاني، وهكذا بالنسبة لبقية تلك الكلمة والكلمات التالية في النص:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

دعنا نجربه على كلمة واحدة موجودة في المفردات، وأخرى ليست كذلك:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

الآن، دعنا نكتب دالة تقوم بتحليل نص:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

يمكننا تجربتها على أي نص:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

هذا كل شيء بالنسبة لخوارزمية WordPiece! الآن دعنا نلقي نظرة على Unigram.