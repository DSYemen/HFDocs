<FrameworkSwitchCourse {fw} />

# ุงูููู ุงูุฎุงุตุฉ ูููุญููุงุช ุงูุณุฑูุนุฉ [[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}

ูู ูุฐุง ุงููุณูุ ุณูููู ูุธุฑุฉ ูุงุญุตุฉ ุนูู ูุฏุฑุงุช ุงููุญููุงุช ูู ๐ค Transformers. ุญุชู ุงูุขูุ ูู ูุณุชุฎุฏููุง ุณูู ูุชุญููู ุงููุฏุฎูุงุช ุฃู ูู ุชุฑููุฒ ุงููุนุฑููุงุช (IDs) ุฅูู ูุตุ ูููู ุงููุญููุงุช - ุฎุงุตุฉ ุชูู ุงููุฏุนููุฉ ุจููุชุจุฉ ๐ค Tokenizers - ูููููุง ูุนู ุงููุซูุฑ. ูุชูุถูุญ ูุฐู ุงูููุฒุงุช ุงูุฅุถุงููุฉุ ุณูุณุชูุดู ููููุฉ ุฅุนุงุฏุฉ ุฅูุชุงุฌ ูุชุงุฆุฌ `token-classification` (ุงูุชู ุฃุทูููุง ุนูููุง ุงุณู `ner`) ูุฎุทูุท ุฃูุงุจูุจ `question-answering` ุงูุชู ูุงุฌููุงูุง ูุฃูู ูุฑุฉ ูู [ุงููุตู 1](/course/chapter1).

<Youtube id="g8quOxoqhHQ"/>

ูู ุงูููุงูุดุฉ ุงูุชุงููุฉุ ุณูููุฒ ุบุงูุจูุง ุจูู ุงููุญููุงุช "ุงูุจุทูุฆุฉ" ู"ุงูุณุฑูุนุฉ". ุงููุญููุงุช ุงูุจุทูุฆุฉ ูู ุชูู ุงูููุชูุจุฉ ุจูุบุฉ ุจุงูุซูู ุฏุงุฎู ููุชุจุฉ ๐ค Transformersุ ูู ุญูู ุฃู ุงูุฅุตุฏุงุฑุงุช ุงูุณุฑูุนุฉ ูู ุชูู ุงูุชู ุชููุฑูุง ููุชุจุฉ ๐ค Tokenizersุ ูุงูููุชูุจุฉ ุจูุบุฉ Rust. ุฅุฐุง ููุช ุชุชุฐูุฑ ุงูุฌุฏูู ูู [ุงููุตู 5](/course/chapter5/3) ุงูุฐู ุฃุจูุบ ุนู ุงููุฏุฉ ุงูุชู ุงุณุชุบุฑููุง ูุญูู ุณุฑูุน ูุขุฎุฑ ุจุทูุก ูุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ูุฑุงุฌุนุฉ ุงูุฃุฏููุฉุ ููุฌุจ ุฃู ุชููู ูุฏูู ููุฑุฉ ุนู ุณุจุจ ุชุณููุชูุง ููุง ุจุงูุณุฑูุนุฉ ูุงูุจุทูุฆุฉ:

|               | ูุญูู ุณุฑูุน | ูุญูู ุจุทูุก
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

โ๏ธ ุนูุฏ ุชุญููู ุฌููุฉ ูุงุญุฏุฉุ ูู ุชุฑู ุฏุงุฆููุง ูุฑููุง ูู ุงูุณุฑุนุฉ ุจูู ุงูุฅุตุฏุงุฑุงุช ุงูุณุฑูุนุฉ ูุงูุจุทูุฆุฉ ูู ููุณ ุงููุญูู. ูู ุงููุงูุนุ ูุฏ ูููู ุงูุฅุตุฏุงุฑ ุงูุณุฑูุน ุฃุจุทุฃ ุจุงููุนู! ูู ุชุชููู ูู ุฑุคูุฉ ุงููุฑู ุจูุถูุญ ุฅูุง ุนูุฏ ุชุญููู ุงููุซูุฑ ูู ุงููุตูุต ุจุงูุชูุงุฒู ูู ููุณ ุงูููุช.

</Tip>

## ุงูุชุฑููุฒ ุงูุฏูุนู [[batch-encoding]]

<Youtube id="3umI3tm27Vw"/>

ูุฎุฑุฌ ุงููุญูู ููุณ ูุฌุฑุฏ ูุงููุณ ุจุงูุซููุ ูุง ูุญุตู ุนููู ูู ุงููุงูุน ูู ูุงุฆู `BatchEncoding` ุฎุงุต. ุฅูู ูุฆุฉ ูุฑุนูุฉ ูู ุงููุงููุณ (ููู ุงูุณุจุจ ูู ุฃููุง ุชูููุง ูู ุงูููุฑุณุฉ ูู ุชูู ุงููุชูุฌุฉ ุฏูู ุฃู ูุดููุฉ ูู ูุจู)ุ ูููู ูุน ุทุฑู ุฅุถุงููุฉ ุชุณุชุฎุฏููุง ุงููุญููุงุช ุงูุณุฑูุนุฉ ุจุดูู ุฃุณุงุณู.

ุฅูู ุฌุงูุจ ูุฏุฑุงุช ุงูุชูุงุฒูุ ูุฅู ุงููุธููุฉ ุงูุฑุฆูุณูุฉ ูููุญููุงุช ุงูุณุฑูุนุฉ ูู ุฃููุง ุชุชุชุจุน ุฏุงุฆููุง ุงููุทุงู ุงูุฃุตูู ูููุตูุต ุงูุชู ุชุฃุชู ูููุง ุงูุฑููุฒ ุงูููุงุฆูุฉ - ููู ููุฒุฉ ูุณูููุง *ุฑุณู ุงูุฎุฑุงุฆุท ููุชุนููุถ*. ููุฐุง ุจุฏูุฑู ููุชุญ ููุฒุงุช ูุซู ุฑุณู ุฎุฑูุทุฉ ููู ูููุฉ ุฅูู ุงูุฑููุฒ ุงูุชู ุฃูุชุฌุชูุง ุฃู ุฑุณู ุฎุฑูุทุฉ ููู ุญุฑู ูู ุงููุต ุงูุฃุตูู ุฅูู ุงูุฑูุฒ ุงูุฐู ููุฌุฏ ุจุฏุงุฎููุ ูุงูุนูุณ ุตุญูุญ.

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ูุซุงู:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

ููุง ุฐูุฑูุง ุณุงุจููุงุ ูุญุตู ุนูู ูุงุฆู `BatchEncoding` ูู ูุฎุฑุฌ ุงููุญูู:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

ุจูุง ุฃู ูุฆุฉ `AutoTokenizer` ุชุฎุชุงุฑ ูุญูููุง ุณุฑูุนูุง ุจุดูู ุงูุชุฑุงุถูุ ูููููุง ุงุณุชุฎุฏุงู ุงูุทุฑู ุงูุฅุถุงููุฉ ุงูุชู ูููุฑูุง ูุฐุง ุงููุงุฆู `BatchEncoding`. ูุฏููุง ุทุฑููุชุงู ููุชุญูู ููุง ุฅุฐุง ูุงู ุงููุญูู ุงูุฎุงุต ุจูุง ุณุฑูุนูุง ุฃู ุจุทูุฆูุง. ูููููุง ุฅูุง ุงูุชุญูู ูู ุณูุฉ `is_fast` ูููุญูู:

```python
tokenizer.is_fast
```

```python out
True
```

ุฃู ุงูุชุญูู ูู ุงูุณูุฉ ููุณูุง ูู `encoding`:

```python
encoding.is_fast
```

```python out
True
```

ุฏุนูุง ูุฑู ูุง ููููููุง ุงููุญูู ุงูุณุฑูุน ูู ูุนูู. ุฃููุงูุ ูููููุง ุงููุตูู ุฅูู ุงูุฑููุฒ ุฏูู ุงูุญุงุฌุฉ ุฅูู ุชุญููู ุงููุนุฑููุงุช (IDs) ูุฑุฉ ุฃุฎุฑู ุฅูู ุฑููุฒ:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

ูู ูุฐู ุงูุญุงูุฉุ ุงูุฑูุฒ ูู ุงูููุฑุณ 5 ูู `##yl`ุ ููู ุฌุฒุก ูู ุงููููุฉ "Sylvain" ูู ุงูุฌููุฉ ุงูุฃุตููุฉ. ูููููุง ุฃูุถูุง ุงุณุชุฎุฏุงู ุทุฑููุฉ `word_ids()` ููุญุตูู ุนูู ููุฑุณ ุงููููุฉ ุงูุชู ูุฃุชู ูููุง ูู ุฑูุฒ:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

ูููููุง ุฃู ูุฑู ุฃู ุงูุฑููุฒ ุงูุฎุงุตุฉ ูููุญูู `[CLS]` ู`[SEP]` ูุชู ุชุนููููุง ุฅูู `None`ุ ุซู ูุชู ุชุนููู ูู ุฑูุฒ ุฅูู ุงููููุฉ ุงูุชู ูุฃุชู ูููุง. ูุฐุง ูููุฏ ุจุดูู ุฎุงุต ูุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ุงูุฑูุฒ ูู ุจุฏุงูุฉ ูููุฉ ุฃู ุฅุฐุง ูุงู ุฑูุฒุงู ูู ููุณ ุงููููุฉ. ูููููุง ุงูุงุนุชูุงุฏ ุนูู ุจุงุฏุฆุฉ `##` ูุฐููุ ูููููุง ุชุนูู ููุท ูููุญููุงุช ุงูุดุจููุฉ ุจBERTุ ุชุนูู ูุฐู ุงูุทุฑููุฉ ูุฃู ููุน ูู ุงููุญููุงุช ุทุงููุง ุฃููุง ุณุฑูุนุฉ. ูู ุงููุตู ุงูุชุงููุ ุณูุฑู ููู ูููููุง ุงุณุชุฎุฏุงู ูุฐู ุงููุฏุฑุฉ ูุชุทุจูู ุงูุนูุงูุงุช ุงูุชู ูุฏููุง ููู ูููุฉ ุจุดูู ุตุญูุญ ุนูู ุงูุฑููุฒ ูู ููุงู ูุซู ุงูุชุนุฑู ุนูู ุงูููุงูุงุช ุงููุณูุงุฉ (NER) ููุถุน ุงูุนูุงูุงุช ุนูู ุฃุฌุฒุงุก ุงูููุงู (POS). ูููููุง ุฃูุถูุง ุงุณุชุฎุฏุงููุง ูุญุฌุจ ุฌููุน ุงูุฑููุฒ ุงูุชู ุชุฃุชู ูู ููุณ ุงููููุฉ ูู ุงูููุฐุฌุฉ ุงููุบููุฉ ุงููููุนุฉ (ุชูููุฉ ุชุณูู _whole word masking_).

<Tip>

ููููู ูุง ูู ุงููููุฉ ูุนูุฏ. ุนูู ุณุจูู ุงููุซุงูุ ูู "I'll" (ุงุฎุชุตุงุฑ ูู "I will") ุชุนุชุจุฑ ูููุฉ ูุงุญุฏุฉ ุฃู ูููุชููุ ูู ุงููุงูุนุ ูุนุชูุฏ ุฐูู ุนูู ุงููุญูู ูุนูููุฉ ุงูุชุญููู ุงููุณุจู ุงูุชู ูุทุจููุง. ุจุนุถ ุงููุญููุงุช ุชูุณู ููุท ุนูู ุงููุณุงูุงุชุ ูุฐุง ููู ุณุชุนุชุจุฑ ูุฐุง ูููุฉ ูุงุญุฏุฉ. ูุณุชุฎุฏู ุงูุจุนุถ ุงูุขุฎุฑ ุนูุงูุงุช ุงูุชุฑููู ุจุงูุฅุถุงูุฉ ุฅูู ุงููุณุงูุงุชุ ูุฐุง ูุณูุชู ุงุนุชุจุงุฑูุง ูููุชูู.

โ๏ธ **ุฌุฑุจูุง!** ุฃูุดุฆ ูุญูููุง ูู ููุงุท ุงูุชูุชูุด `bert-base-cased` ู`roberta-base` ูุญูู "81s" ุจุงุณุชุฎุฏุงูููุง. ูุงุฐุง ุชูุงุญุธุ ูุง ูู ูุนุฑููุงุช ุงููููุงุช (word IDs)ุ

</Tip>

ูุจุงููุซูุ ููุงู ุทุฑููุฉ `sentence_ids()` ุงูุชู ูููููุง ุงุณุชุฎุฏุงููุง ูุชุนููู ุฑูุฒ ุฅูู ุงูุฌููุฉ ุงูุชู ุฌุงุก ูููุง (ุนูู ุงูุฑุบู ูู ุฃูู ูู ูุฐู ุงูุญุงูุฉุ ูููู ุฃู ุชุนุทููุง `token_type_ids` ุงูุชู ูุนูุฏูุง ุงููุญูู ููุณ ุงููุนูููุงุช).

ุฃุฎูุฑูุงุ ูููููุง ุชุนููู ุฃู ูููุฉ ุฃู ุฑูุฒ ุฅูู ุฃุญุฑู ูู ุงููุต ุงูุฃุตููุ ูุงูุนูุณ ุตุญูุญุ ุนุจุฑ ุทุฑู `word_to_chars()` ุฃู `token_to_chars()` ู`char_to_word()` ุฃู `char_to_token()`. ุนูู ุณุจูู ุงููุซุงูุ ุฃุฎุจุฑุชูุง ุทุฑููุฉ `word_ids()` ุฃู `##yl` ูู ุฌุฒุก ูู ุงููููุฉ ูู ุงูููุฑุณ 3ุ ูููู ุฃู ูููุฉ ูู ูู ุงูุฌููุฉุ ูููููุง ูุนุฑูุฉ ุฐูู ุนูู ุงููุญู ุงูุชุงูู:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

ููุง ุฐูุฑูุง ุณุงุจูุงูุ ูู ูุฐุง ูููู ุจูุถู ุญูููุฉ ุฃู ุงููุฑูุฒ ุงูุณุฑูุน ูุญุชูุธ ุจูุณุงุฑ ููุทุน ุงููุต ุงูุฐู ูุฃุชู ููู ูู ุฑูุฒ ูู ูุงุฆูุฉ ูู *ุงูุฅุฒุงุญุงุช*. ูุชูุถูุญ ุงุณุชุฎุฏุงููุงุ ุณูุนุฑุถ ูู ููููุฉ ุชูุฑุงุฑ ูุชุงุฆุฌ ุฎุท ุฃูุงุจูุจ `token-classification` ูุฏูููุง.

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ูู ุจุฅูุดุงุก ูุตู ุงูุฎุงุต ูุงูุธุฑ ุฅุฐุง ููุช ุชุณุชุทูุน ููู ุงูุฑููุฒ ุงููุฑุชุจุทุฉ ุจูุนุฑู ุงููููุฉุ ูุฃูุถุงู ููููุฉ ุงุณุชุฎุฑุงุฌ ููุงุทุน ุงูุฃุญุฑู ููููุฉ ูุงุญุฏุฉ. ูููุญุตูู ุนูู ููุงุท ุฅุถุงููุฉุ ุฌุฑุจ ุงุณุชุฎุฏุงู ุฌููุชูู ููุฏุฎูุงุช ูุงูุธุฑ ุฅุฐุง ูุงูุช ูุนุฑูุงุช ุงูุฌูู ููุทููุฉ ุจุงููุณุจุฉ ูู.

</Tip>

## ุฏุงุฎู ุฎุท ุฃูุงุจูุจ `token-classification`[[inside-the-token-classification-pipeline]]

ูู [ุงููุตู 1](/course/chapter1) ุญุตููุง ุนูู ุฃูู ุชุฐูู ูุชุทุจูู NER -- ุญูุซ ุชููู ุงููููุฉ ูู ุชุญุฏูุฏ ุฃุฌุฒุงุก ุงููุต ุงูุชู ุชุชูุงูู ูุน ุงูููุงูุงุช ูุซู ุงูุฃุดุฎุงุตุ ูุงูููุงูุนุ ุฃู ุงูููุธูุงุช -- ูุน ูุธููุฉ `pipeline()` ูู ๐ค Transformers. ุซูุ ูู [ุงููุตู 2](/course/chapter2)ุ ุฑุฃููุง ููู ูููู ุฎุท ุงูุฃูุงุจูุจ ุจุฌูุน ุงููุฑุงุญู ุงูุซูุงุซ ุงูุถุฑูุฑูุฉ ููุญุตูู ุนูู ุงูุชูุจุคุงุช ูู ูุต ุฎุงู: ุงูุชุฌุฒุฆุฉุ ูุฅุฏุฎุงู ุงููุฏุฎูุงุช ุนุจุฑ ุงููููุฐุฌุ ูุงููุนุงูุฌุฉ ุงููุงุญูุฉ. ุงูุฎุทูุชุงู ุงูุฃูููุชุงู ูู ุฎุท ุฃูุงุจูุจ `token-classification` ููุง ููุณ ุงูุฎุทูุชูู ูู ุฃู ุฎุท ุฃูุงุจูุจ ุขุฎุฑุ ููู ุงููุนุงูุฌุฉ ุงููุงุญูุฉ ุฃูุซุฑ ุชุนููุฏูุง ุจูููู -- ุฏุนููุง ูุฑู ููู!

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### ุงูุญุตูู ุนูู ุงููุชุงุฆุฌ ุงูุฃุณุงุณูุฉ ุจุงุณุชุฎุฏุงู ุฎุท ุงูุฃูุงุจูุจ[[getting-the-base-results-with-the-pipeline]]

ุฃููุงูุ ุฏุนูุง ูุญุตู ุนูู ุฎุท ุฃูุงุจูุจ ุชุตููู ุงูุฑููุฒ ุจุญูุซ ูููููุง ุงูุญุตูู ุนูู ุจุนุถ ุงููุชุงุฆุฌ ูููุงุฑูุชูุง ูุฏูููุง. ุงููููุฐุฌ ุงููุณุชุฎุฏู ุจุดูู ุงูุชุฑุงุถู ูู [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)ุ ูููู ุจุชูููุฐ NER ุนูู ุงูุฌูู:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ุญุฏุฏ ุงููููุฐุฌ ุจุดูู ุตุญูุญ ูู ุฑูุฒ ุชู ุชูููุฏู ุจูุงุณุทุฉ "Sylvain" ูุดุฎุตุ ููู ุฑูุฒ ุชู ุชูููุฏู ุจูุงุณุทุฉ "Hugging Face" ูููุธูุฉุ ูุงูุฑูุฒ "Brooklyn" ููููุน. ูููููุง ุฃูุถูุง ุฃู ูุทูุจ ูู ุฎุท ุงูุฃูุงุจูุจ ุชุฌููุน ุงูุฑููุฒ ุงูุชู ุชุชูุงูู ูุน ููุณ ุงูููุงู:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ุณูุคุฏู ุงุฎุชูุงุฑ `aggregation_strategy` ุฅูู ุชุบููุฑ ุงูุฏุฑุฌุงุช ุงููุญุณูุจุฉ ููู ููุงู ูุฌูุน. ูุน `"simple"` ุชููู ุงููุชูุฌุฉ ูู ูุชูุณุท ุฏุฑุฌุงุช ูู ุฑูุฒ ูู ุงูููุงู ุงููุนุทู: ุนูู ุณุจูู ุงููุซุงูุ ุชููู ูุชูุฌุฉ "Sylvain" ูู ูุชูุณุท ุงูุฏุฑุฌุงุช ุงูุชู ุฑุฃููุงูุง ูู ุงููุซุงู ุงูุณุงุจู ููุฑููุฒ `S`ุ `##yl`ุ `##va`ุ ู `##in`. ุงูุงุณุชุฑุงุชูุฌูุงุช ุงูุฃุฎุฑู ุงููุชุงุญุฉ ูู:

- `"first"`ุ ุญูุซ ุชููู ูุชูุฌุฉ ูู ููุงู ูู ูุชูุฌุฉ ุงูุฑูุฒ ุงูุฃูู ูุฐูู ุงูููุงู (ูุฐูู ุจุงููุณุจุฉ ูู "Sylvain" ุณุชููู 0.993828ุ ูุชูุฌุฉ ุงูุฑูุฒ `S`)
- `"max"`ุ ุญูุซ ุชููู ูุชูุฌุฉ ูู ููุงู ูู ุงููุชูุฌุฉ ุงููุตูู ููุฑููุฒ ูู ุฐูู ุงูููุงู (ูุฐูู ุจุงููุณุจุฉ ูู "Hugging Face" ุณุชููู 0.98879766ุ ูุชูุฌุฉ "Face")
- `"average"`ุ ุญูุซ ุชููู ูุชูุฌุฉ ูู ููุงู ูู ูุชูุณุท ุฏุฑุฌุงุช ุงููููุงุช ุงูููููุฉ ูุฐูู ุงูููุงู (ูุฐูู ุจุงููุณุจุฉ ูู "Sylvain" ูู ูููู ููุงู ูุฑู ุนู ุงุณุชุฑุงุชูุฌูุฉ `"simple"`ุ ููู "Hugging Face" ุณุชุญุตู ุนูู ูุชูุฌุฉ 0.9819ุ ูุชูุณุท ุฏุฑุฌุงุช "Hugging"ุ 0.975ุ ู "Face"ุ 0.98879)

ุงูุขู ุฏุนูุง ูุฑู ููููุฉ ุงูุญุตูู ุนูู ูุฐู ุงููุชุงุฆุฌ ุจุฏูู ุงุณุชุฎุฏุงู ูุธููุฉ `pipeline()`!

### ูู ุงููุฏุฎูุงุช ุฅูู ุงูุชูุจุคุงุช[[from-inputs-to-predictions]]

{#if fw === 'pt'}

ุฃููุงู ูุญุชุงุฌ ุฅูู ุชุฌุฒุฆุฉ ูุฏุฎูุงุชูุง ูุฅุฏุฎุงููุง ุนุจุฑ ุงููููุฐุฌ. ูุชู ุฐูู ุจุงูุถุจุท ููุง ูู [ุงููุตู 2](/course/chapter2)ุ ูููู ุจุชูููุฐ ุงููุฑูุฒ ูุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ูุฆุงุช `AutoXxx` ุซู ูุณุชุฎุฏููุง ุนูู ูุซุงููุง:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

ุจูุง ุฃููุง ูุณุชุฎุฏู `AutoModelForTokenClassification` ููุงุ ูุญุตู ุนูู ูุฌููุนุฉ ูุงุญุฏุฉ ูู ุงูููุบุงุฑูุชูุงุช ููู ุฑูุฒ ูู ุชุณูุณู ุงูุฅุฏุฎุงู:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

ุฃููุงู ูุญุชุงุฌ ุฅูู ุชุฌุฒุฆุฉ ูุฏุฎูุงุชูุง ูุฅุฏุฎุงููุง ุนุจุฑ ุงููููุฐุฌ. ูุชู ุฐูู ุจุงูุถุจุท ููุง ูู [ุงููุตู 2](/course/chapter2)ุ ูููู ุจุชูููุฐ ุงููุฑูุฒ ูุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ูุฆุงุช `TFAutoXxx` ุซู ูุณุชุฎุฏููุง ุนูู ูุซุงููุง:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

ุจูุง ุฃููุง ูุณุชุฎุฏู `TFAutoModelForTokenClassification` ููุงุ ูุญุตู ุนูู ูุฌููุนุฉ ูุงุญุฏุฉ ูู ุงูููุบุงุฑูุชูุงุช ููู ุฑูุฒ ูู ุชุณูุณู ุงูุฅุฏุฎุงู:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

ูุฏููุง ุฏูุนุฉ ูุงุญุฏุฉ ูุน 1 ุชุณูุณู ูู 19 ุฑูุฒ ูุงููููุฐุฌ ูุฏูู 9 ุชุณููุงุช ูุฎุชููุฉุ ูุฐูู ูููู ูุงุชุฌ ุงููููุฐุฌ ุนูู ุดูู 1 x 19 x 9. ูุซู ุฎุท ุฃูุงุจูุจ ุชุตููู ุงููุตุ ูุณุชุฎุฏู ูุธููุฉ softmax ูุชุญููู ุงูููุบุงุฑูุชูุงุช ุฅูู ุงุญุชูุงูุงุชุ ููุฃุฎุฐ argmax ููุญุตูู ุนูู ุงูุชูุจุคุงุช (ูุงุญุธ ุฃูู ูููููุง ุฃุฎุฐ argmax ุนูู ุงูููุบุงุฑูุชูุงุช ูุฃู softmax ูุง ุชุบูุฑ ุงูุชุฑุชูุจ):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

ูุญุชูู ุฎุงุตูุฉ `model.config.id2label` ุนูู ุฎุฑูุทุฉ ุงูููุงุฑุณ ุฅูู ุงูุชุณููุงุช ุงูุชู ูููููุง ุงุณุชุฎุฏุงููุง ูููู ุงูุชูุจุคุงุช:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```
ููุง ุฑุฃููุง ุณุงุจูุงูุ ููุงู 9 ุชุณููุงุช: `O` ูู ุงูุชุณููุฉ ููุฑููุฒ ุงูุชู ูุง ุชูุชูู ูุฃู ููุงู ูุณูู (ููู ุงุฎุชุตุงุฑ ูู "ุฎุงุฑุฌู")ุ ููุฏููุง ุจุนุฏ ุฐูู ุชุณููุชูู ููู ููุน ูู ุงูููุงูุงุช (ูุชููุนุ ุดุฎุตุ ููุธูุฉุ ูููุงู). ุงูุชุณููุฉ `B-XXX` ุชุดูุฑ ุฅูู ุฃู ุงูุฑูุฒ ูู ุจุฏุงูุฉ ููุงู `XXX` ูุงูุชุณููุฉ `I-XXX` ุชุดูุฑ ุฅูู ุฃู ุงูุฑูุฒ ูู ุฏุงุฎู ููุงู `XXX`. ุนูู ุณุจูู ุงููุซุงูุ ูู ุงููุซุงู ุงูุญุงููุ ูุชููุน ุฃู ูุตูู ูููุฐุฌูุง ุงูุฑูุฒ `S` ุนูู ุฃูู `B-PER` (ุจุฏุงูุฉ ููุงู ุดุฎุต) ูุงูุฑููุฒ `##yl`ุ `##va` ู `##in` ุนูู ุฃููุง `I-PER` (ุฏุงุฎู ููุงู ุดุฎุต).

ูุฏ ุชุนุชูุฏ ุฃู ุงููููุฐุฌ ูุงู ุฎุงุทุฆุงู ูู ูุฐู ุงูุญุงูุฉ ูุฃูู ุฃุนุทู ุงูุชุณููุฉ `I-PER` ูุฌููุน ูุฐู ุงูุฑููุฒ ุงูุฃุฑุจุนุฉุ ูููู ูุฐุง ููุณ ุตุญูุญุงู ุชูุงูุงู. ูู ุงููุงูุนุ ููุงู ุชูุณูููู ููุงุชูู ุงูุชุณููุชูู `B-` ู `I-`: *IOB1* ู *IOB2*. ุชูุณูู IOB2 (ุจุงูููู ุงููุฑุฏู ุฃุฏูุงู)ุ ูู ุงูุชูุณูู ุงูุฐู ูุฏููุงูุ ุจูููุง ูู ุชูุณูู IOB1 (ุจุงูููู ุงูุฃุฒุฑู)ุ ุชุณุชุฎุฏู ุงูุชุณููุงุช ุงูุชู ุชุจุฏุฃ ุจู `B-` ููุท ููุตู ููุงููู ูุชุฌุงูุฑูู ูู ููุณ ุงูููุน. ุงููููุฐุฌ ุงูุฐู ูุณุชุฎุฏูู ุชู ุถุจุทู ุนูู ูุฌููุนุฉ ุจูุงูุงุช ุจุงุณุชุฎุฏุงู ุฐูู ุงูุชูุณููุ ููู ุงูุณุจุจ ูู ุฃูู ูุนูู ุงูุชุณููุฉ `I-PER` ููุฑูุฒ `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

ูุน ูุฐุง ุงููุฎุทุทุ ูุญู ูุณุชุนุฏูู ูุชูุฑุงุฑ (ุชูุฑูุจุงู ุจุงููุงูู) ูุชุงุฆุฌ ุงูุฃูุจูุจ ุงูุฃูู -- ูููููุง ููุท ุงูุญุตูู ุนูู ุงููุชูุฌุฉ ูุงูุชุณููุฉ ููู ุฑูุฒ ูู ูุชู ุชุตูููู ุนูู ุฃูู `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

ูุฐุง ูุดุงุจู ุฌุฏุงู ููุง ูุงู ูุฏููุง ุณุงุจูุงูุ ูุน ุงุณุชุซูุงุก ูุงุญุฏ: ุงูุฃูุจูุจ ุฃุนุทุงูุง ุฃูุถุงู ูุนูููุงุช ุนู `start` ู `end` ููู ููุงู ูู ุงูุฌููุฉ ุงูุฃุตููุฉ. ููุง ูุฃุชู ุฏูุฑ ุฎุฑูุทุฉ ุงูุฅุฒุงุญุฉ ูุฏููุง. ููุญุตูู ุนูู ุงูุฅุฒุงุญุงุชุ ูู ูุง ุนูููุง ูุนูู ูู ุชุนููู `return_offsets_mapping=True` ุนูุฏูุง ูุทุจู ุงูุฑูุฒ ุนูู ูุฏุฎูุงุชูุง:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

ูู ุฒูุฌ ูู ุงููุทุงู ุงููุตู ุงูููุงุจู ููู ุฑูุฒุ ุญูุซ `(0, 0)` ูุญุฌูุฒ ููุฑููุฒ ุงูุฎุงุตุฉ. ุฑุฃููุง ุณุงุจูุงู ุฃู ุงูุฑูุฒ ูู ุงูููุฑุณ 5 ูู `##yl`ุ ูุงูุฐู ูู `(12, 14)` ูุฅุฒุงุญุงุช ููุง. ุฅุฐุง ุญุตููุง ุนูู ุงูุดุฑูุญุฉ ุงูููุงุจูุฉ ูู ูุซุงููุง:


```py
example[12:14]
```

ุณูุญุตู ุนูู ุงููุทุงู ุงููุตู ุงูุตุญูุญ ุจุฏูู `##`:

```python out
yl
```

ุจุงุณุชุฎุฏุงู ูุฐุงุ ูููููุง ุงูุขู ุฅููุงู ุงููุชุงุฆุฌ ุงูุณุงุจูุฉ:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ูุฐุง ูู ููุณ ูุง ุญุตููุง ุนููู ูู ุงูุฃูุจูุจ ุงูุฃูู!

### ุชุฌููุน ุงูููุงูุงุช [[grouping-entities]]

ุงุณุชุฎุฏุงู ุงูุฅุฒุงุญุงุช ูุชุญุฏูุฏ ููุงุชูุญ ุงูุจุฏุงูุฉ ูุงูููุงูุฉ ููู ููุงู ูููุฏุ ูููู ุชูู ุงููุนูููุงุช ููุณุช ุถุฑูุฑูุฉ ุชูุงูุงู. ุนูุฏูุง ูุฑูุฏ ุชุฌููุน ุงูููุงูุงุช ูุนุงูุ ููุน ุฐููุ ูุฅู ุงูุฅุฒุงุญุงุช ุณุชููุฑ ุนูููุง ุงููุซูุฑ ูู ุงูุฑููุฒ ุงููุนูุฏุฉ. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ุฃุฑุฏูุง ุชุฌููุน ุงูุฑููุฒ `Hu`ุ `##gging`ุ ู `Face`ุ ูููููุง ูุถุน ููุงุนุฏ ุฎุงุตุฉ ุชููู ุฃู ุงูุฃูููู ูุฌุจ ุฃู ููููุง ููุชุตููู ูุน ุฅุฒุงูุฉ `##`ุ ู `Face` ูุฌุจ ุฃู ุชุถุงู ูุน ูุณุงูุฉ ูุฃููุง ูุง ุชุจุฏุฃ ุจู `##` -- ูููู ุฐูู ุณูุนูู ููุท ููุฐุง ุงูููุน ุงููุญุฏุฏ ูู ุงูุฑููุฒ. ุณูุชุนูู ุนูููุง ูุชุงุจุฉ ูุฌููุนุฉ ุฃุฎุฑู ูู ุงูููุงุนุฏ ูุฑูุฒ SentencePiece ุฃู Byte-Pair-Encoding (ุณูุชู ููุงูุดุชูุง ูุงุญูุงู ูู ูุฐุง ุงููุตู).

ูุน ุงูุฅุฒุงุญุงุชุ ูู ุชูู ุงูุฑููุฒ ุงููุฎุตุตุฉ ุชุฎุชูู: ูููููุง ููุท ุฃุฎุฐ ุงููุทุงู ูู ุงููุต ุงูุฃุตูู ุงูุฐู ูุจุฏุฃ ุจุงูุฑูุฒ ุงูุฃูู ูููุชูู ุจุงูุฑูุฒ ุงูุฃุฎูุฑ. ูุฐููุ ูู ุญุงูุฉ ุงูุฑููุฒ `Hu`ุ `##gging`ุ ู `Face`ุ ูุฌุจ ุฃู ูุจุฏุฃ ุนูุฏ ุงูุญุฑู 33 (ุจุฏุงูุฉ `Hu`) ูููุชูู ูุจู ุงูุญุฑู 45 (ููุงูุฉ `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

ููุชุงุจุฉ ุงูุฑูุฒ ุงูุฐู ูููู ุจูุนุงูุฌุฉ ุงูุชูุจุคุงุช ุฃุซูุงุก ุชุฌููุน ุงูููุงูุงุชุ ุณูููู ุจุชุฌููุน ุงูููุงูุงุช ุงูุชู ุชููู ูุชุชุงููุฉ ููุตููุฉ ุจู `I-XXX`ุ ุจุงุณุชุซูุงุก ุงูุฃููุ ูุงูุฐู ูููู ุชุตูููู ุจู `B-XXX` ุฃู `I-XXX` (ูุฐููุ ูุชููู ุนู ุชุฌููุน ููุงู ุนูุฏูุง ูุญุตู ุนูู `O`ุ ุฃู ููุน ุฌุฏูุฏ ูู ุงูููุงูุงุชุ ุฃู `B-XXX` ุงูุฐู ูุฎุจุฑูุง ุฃู ููุงู ูู ููุณ ุงูููุน ุจุฏุฃ):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

ููุญุตู ุนูู ููุณ ุงููุชุงุฆุฌ ููุง ูู ุฃูุจูุจูุง ุงูุซุงูู!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ูุซุงู ุขุฎุฑ ููููุฉ ุชููู ูููุง ูุฐู ุงูุฅุฒุงุญุงุช ูููุฏุฉ ููุบุงูุฉ ูู ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ. ุงูุบูุต ูู ูุฐุง ุงูุฎุทุ ูุงูุฐู ุณูููู ุจู ูู ุงููุณู ุงูุชุงููุ ุณูููููุง ุฃูุถูุง ูู ุฅููุงุก ูุธุฑุฉ ุนูู ููุฒุฉ ุฃุฎูุฑุฉ ูููุณููุงุช ุงูุฑููุฒ ูู ููุชุจุฉ ๐ค Transformers: ุงูุชุนุงูู ูุน ุงูุฑููุฒ ุงููุชุฏููุฉ ุนูุฏูุง ูููู ุจุชูููุต ุฅุฏุฎุงู ุฅูู ุทูู ูุนูู.