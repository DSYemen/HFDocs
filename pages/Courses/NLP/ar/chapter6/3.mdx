# ุงูููู ุงูุฎุงุตุฉ ูููุณููุงุช ุงูุฑููุฒ ุงูุณุฑูุนุฉ

ูู ูุฐุง ุงููุณูุ ุณูููู ูุธุฑุฉ ูุงุญุตุฉ ุนูู ูุฏุฑุงุช ููุณููุงุช ุงูุฑููุฒ ูู ููุชุจุฉ ๐ค Transformers. ุญุชู ุงูุขูุ ูู ูุณุชุฎุฏููุง ุฅูุง ูุชูุณูู ุงููุฏุฎูุงุช ุฅูู ุฑููุฒ ุฃู ูู ุชุฑููุฒ ุงููุนุฑููุงุช (IDs) ูุฑุฉ ุฃุฎุฑู ุฅูู ูุตุ ูููู ูููู ูููุณููุงุช ุงูุฑููุฒ - ุฎุงุตุฉ ุชูู ุงููุฏุนููุฉ ุจููุชุจุฉ ๐ค Tokenizers - ุฃู ุชูุนู ุฃูุซุฑ ูู ุฐูู ุจูุซูุฑ. ููุชูุถูุญ ูุฐู ุงูููุฒุงุช ุงูุฅุถุงููุฉุ ุณูุณุชูุดู ููููุฉ ุฅุนุงุฏุฉ ุฅูุชุงุฌ ูุชุงุฆุฌ ุฎุทูุท ุฃูุงุจูุจ `token-classification` (ุงูุชู ุฃุทูููุง ุนูููุง ุงุณู `ner`) ู`question-answering` ุงูุชู ูุงุฌููุงูุง ูุฃูู ูุฑุฉ ูู [ุงููุตู 1](/course/chapter1).

ูู ุงูููุงูุดุฉ ุงูุชุงููุฉุ ุณูููุฒ ุบุงูุจูุง ุจูู ููุณููุงุช ุงูุฑููุฒ "ุงูุจุทูุฆุฉ" ู"ุงูุณุฑูุนุฉ". ููุณููุงุช ุงูุฑููุฒ ุงูุจุทูุฆุฉ ูู ุชูู ุงูููุชูุจุฉ ุจูุบุฉ Python ุฏุงุฎู ููุชุจุฉ ๐ค Transformersุ ูู ุญูู ุฃู ุงูุฅุตุฏุงุฑุงุช ุงูุณุฑูุนุฉ ูู ุชูู ุงูุชู ุชููุฑูุง ููุชุจุฉ ๐ค Tokenizersุ ูุงูููุชูุจุฉ ุจูุบุฉ Rust. ุฅุฐุง ููุช ุชุชุฐูุฑ ุงูุฌุฏูู ูู [ุงููุตู 5](/course/chapter5/3) ุงูุฐู ุฃุจูุบ ุนู ุงููุฏุฉ ุงูุชู ุงุณุชุบุฑููุง ููุณูู ุฑููุฒ ุณุฑูุน ูุขุฎุฑ ุจุทูุก ูุชูุณูู ูุฌููุนุฉ ุจูุงูุงุช ูุฑุงุฌุนุฉ ุงูุฏูุงุกุ ููุฌุจ ุฃู ุชููู ูุฏูู ููุฑุฉ ุนู ุณุจุจ ุชุณููุชูุง ููุง ุจุงูุณุฑูุนุฉ ูุงูุจุทูุฆุฉ:

|          | ููุณูู ุงูุฑููุฒ ุงูุณุฑูุน | ููุณูู ุงูุฑููุฒ ุงูุจุทูุก |
| :----: |:----: |:----: |
| `batched=True`  | 10.8s | 4min41s |
| `batched=False` | 59.2s | 5min3s |

|               | ููุณูู ุงูุฑููุฒ ุงูุณุฑูุน | ููุณูู ุงูุฑููุฒ ุงูุจุทูุก |
| :--------------:|:--------------:|:-------------:|
| `batched=True`  | 10.8s          | 4min41s |
| `batched=False` | 59.2s          | 5min3s |

ุชุญุฐูุฑ: โ๏ธ ุนูุฏ ุชูุณูู ุฌููุฉ ูุงุญุฏุฉุ ูู ุชูุงุญุธ ุฏุงุฆููุง ุงุฎุชูุงููุง ูู ุงูุณุฑุนุฉ ุจูู ุงูุฅุตุฏุงุฑุงุช ุงูุณุฑูุนุฉ ูุงูุจุทูุฆุฉ ูู ููุณ ููุณูู ุงูุฑููุฒ. ูู ุงููุงูุนุ ูุฏ ูููู ุงูุฅุตุฏุงุฑ ุงูุณุฑูุน ุฃุจุทุฃ ุจุงููุนู! ูู ุชุชููู ูู ุฑุคูุฉ ุงููุฑู ุจูุถูุญ ุฅูุง ุนูุฏ ุชูุณูู ุงููุซูุฑ ูู ุงููุตูุต ุจุดูู ูุชูุงุฒู ูู ููุณ ุงูููุช.

## ุงูุชุฑููุฒ ุงูุฏูุนู

ุงููุงุชุฌ ูู ููุณูู ุงูุฑููุฒ ููุณ ุนุจุงุฑุฉ ุนู ูุงููุณ Python ุจุณูุทุ ูุง ูุญุตู ุนููู ูู ุงููุงูุน ูู ูุงุฆู `BatchEncoding` ุฎุงุต. ุฅูู ูุฆุฉ ูุฑุนูุฉ ูู ูุงููุณ (ููุฐุง ูู ุงูุณุจุจ ูู ุฃููุง ุชูููุง ูู ุงูููุฑุณุฉ ูู ุชูู ุงููุชูุฌุฉ ุฏูู ุฃู ูุดููุฉ ูู ูุจู)ุ ูููู ูุน ุฃุณุงููุจ ุฅุถุงููุฉ ุชุณุชุฎุฏููุง ุจุดูู ุฃุณุงุณู ููุณููุงุช ุงูุฑููุฒ ุงูุณุฑูุนุฉ.

ุจุงูุฅุถุงูุฉ ุฅูู ูุฏุฑุงุช ุงูููุงุฒุงุฉุ ุชุชูุซู ุงููุธููุฉ ุงูุฃุณุงุณูุฉ ูููุณููุงุช ุงูุฑููุฒ ุงูุณุฑูุนุฉ ูู ุฃููุง ุชุญุชูุธ ุฏุงุฆููุง ุจุชุชุจุน ุงููุต ุงูุฃุตูู ุงูุฐู ุชุฃุชู ููู ุงูุฑููุฒ ุงูููุงุฆูุฉ - ููู ููุฒุฉ ูุณูููุง *ุฎุฑูุทุฉ ุงูุฅุฒุงุญุฉ*. ููุฐุง ุจุฏูุฑู ููุชุญ ููุฒุงุช ูุซู ุฑุณู ุฎุฑูุทุฉ ููู ูููุฉ ุฅูู ุงูุฑููุฒ ุงูุชู ุฃูุชุฌุชูุง ุฃู ุฑุณู ุฎุฑูุทุฉ ููู ุญุฑู ูู ุงููุต ุงูุฃุตูู ุฅูู ุงูุฑูุฒ ุงูุฐู ููุน ูููุ ูุงูุนูุณ ุตุญูุญ.

ููููู ูุธุฑุฉ ุนูู ูุซุงู:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

ููุง ุฐูุฑูุง ุณุงุจููุงุ ูุญุตู ุนูู ูุงุฆู `BatchEncoding` ูู ุฅุฎุฑุงุฌ ููุณูู ุงูุฑููุฒ:

```python
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

ูุธุฑูุง ูุฃู ูุฆุฉ `AutoTokenizer` ุชุฎุชุงุฑ ููุณูู ุฑููุฒ ุณุฑูุนูุง ุจุดูู ุงูุชุฑุงุถูุ ูููููุง ุงุณุชุฎุฏุงู ุงูุฃุณุงููุจ ุงูุฅุถุงููุฉ ุงูุชู ูููุฑูุง ูุงุฆู `BatchEncoding` ูุฐุง. ููุงู ุทุฑููุชุงู ููุชุญูู ููุง ุฅุฐุง ูุงู ููุณูู ุงูุฑููุฒ ุงูุฎุงุต ุจูุง ุณุฑูุนูุง ุฃู ุจุทูุฆูุง. ูููููุง ุฅูุง ุงูุชุญูู ูู ุณูุฉ `is_fast` ูู `tokenizer`:

```python
tokenizer.is_fast
```

```python
True
```

ุฃู ุงูุชุญูู ูู ุงูุณูุฉ ููุณูุง ูู `encoding`:

```python
encoding.is_fast
```

```python
True
```

ุฏุนููุง ูุฑู ูุง ููููููุง ููู ููุณูู ุงูุฑููุฒ ุงูุณุฑูุน. ุฃููุงูุ ูููููุง ุงููุตูู ุฅูู ุงูุฑููุฒ ุฏูู ุงูุญุงุฌุฉ ุฅูู ุชุญููู ุงููุนุฑููุงุช (IDs) ูุฑุฉ ุฃุฎุฑู ุฅูู ุฑููุฒ:

```py
encoding.tokens()
```

```python
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
'Brooklyn', '.', '[SEP]']
```

ูู ูุฐู ุงูุญุงูุฉุ ุงูุฑูุฒ ูู ุงูููุฑุณ 5 ูู `##yl`ุ ููู ุฌุฒุก ูู ูููุฉ "Sylvain" ูู ุงูุฌููุฉ ุงูุฃุตููุฉ. ูููููุง ุฃูุถูุง ุงุณุชุฎุฏุงู ุทุฑููุฉ `word_ids()` ููุญุตูู ุนูู ููุฑุณ ุงููููุฉ ุงูุชู ูุฃุชู ูููุง ูู ุฑูุฒ:

```py
encoding.word_ids()
```

```python
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

ูููููุง ุฃู ูุฑู ุฃู ุงูุฑููุฒ ุงูุฎุงุตุฉ ูููุณูู ุงูุฑููุฒ `[CLS]` ู`[SEP]` ูุชู ุฑุณููุง ุฅูู `None`ุ ุซู ูุชู ุฑุณู ูู ุฑูุฒ ุฅูู ุงููููุฉ ุงูุชู ูุดุฃ ูููุง. ูุฐุง ูููุฏ ุจุดูู ุฎุงุต ูุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ุงูุฑูุฒ ููุน ูู ุจุฏุงูุฉ ูููุฉ ุฃู ุฅุฐุง ูุงู ุฑูุฒุงู ูู ููุณ ุงููููุฉ. ูููููุง ุงูุงุนุชูุงุฏ ุนูู ุงูุจุงุฏุฆุฉ `##` ููุฐุง ุงูุบุฑุถุ ูููููุง ุชุนูู ููุท ูููุณููุงุช ุฑููุฒ BERT-likeุ ุชุนูู ูุฐู ุงูุทุฑููุฉ ูุฃู ููุน ูู ููุณููุงุช ุงูุฑููุฒ ุทุงููุง ุฃููุง ุณุฑูุนุฉ. ูู ุงููุตู ุงูุชุงููุ ุณูุฑู ููู ูููููุง ุงุณุชุฎุฏุงู ูุฐู ุงููุฏุฑุฉ ูุชุทุจูู ุงูุนูุงูุงุช ุงูุชู ูุฏููุง ููู ูููุฉ ุจุดูู ุตุญูุญ ุนูู ุงูุฑููุฒ ูู ููุงู ูุซู ุงูุชุนุฑู ุนูู ุงูููุงูุงุช ุงููุณูุงุฉ (NER) ููุถุน ุงูุนูุงูุงุช ุงููุญููุฉ (POS). ูููููุง ุฃูุถูุง ุงุณุชุฎุฏุงูู ูุฅุฎูุงุก ุฌููุน ุงูุฑููุฒ ุงูุชู ุชุฃุชู ูู ููุณ ุงููููุฉ ูู ุงูููุฐุฌุฉ ุงููุบููุฉ ุงููุฎููุฉ (ุชูููุฉ ุชุณูู _whole word masking_).

ุชุนุชุจุฑ ููุฑุฉ ูุง ุงููููุฉ ูุนูุฏุฉ. ุนูู ุณุจูู ุงููุซุงูุ ูู "I'll" (ุงุฎุชุตุงุฑ ูู "I will") ุชุนุชุจุฑ ูููุฉ ูุงุญุฏุฉ ุฃู ูููุชููุ ูู ุงููุงูุนุ ูุนุชูุฏ ุฐูู ุนูู ููุณูู ุงูุฑููุฒ ูุนูููุฉ ูุง ูุจู ุงูุชูุณูู ุงูุชู ูุทุจููุง. ูููู ุจุนุถ ููุณููุงุช ุงูุฑููุฒ ุจุงูุชูุณูู ููุท ุญุณุจ ุงููุณุงูุงุชุ ูุฐุง ููู ุณุชุนุชุจุฑูุง ูููุฉ ูุงุญุฏุฉ. ูุณุชุฎุฏู ุงูุจุนุถ ุงูุขุฎุฑ ุนูุงูุงุช ุงูุชุฑููู ุจุงูุฅุถุงูุฉ ุฅูู ุงููุณุงูุงุชุ ูุฐุง ูุณูุชู ุงุนุชุจุงุฑูุง ูููุชูู.

ุฌุฑุจูุง ุจููุณู: ูู ุจุฅูุดุงุก ููุณูู ุฑููุฒ ูู ููุงุท ุงูุชูุชูุด `bert-base-cased` ู`roberta-base` ููู ุจุชูุณูู "81s" ุจุงุณุชุฎุฏุงููุง. ูุงุฐุง ุชูุงุญุธุ ูุง ูู ูุนุฑููุงุช ุงููููุงุชุ

ูุจุงููุซูุ ููุงู ุทุฑููุฉ `sentence_ids()` ูููููุง ุงุณุชุฎุฏุงููุง ูุฑุณู ุฎุฑูุทุฉ ููุฑูุฒ ุฅูู ุงูุฌููุฉ ุงูุชู ุฌุงุก ูููุง (ุนูู ุงูุฑุบู ูู ุฃูู ูู ูุฐู ุงูุญุงูุฉุ ูููู ุฃู ุชุนุทููุง `token_type_ids` ุงูุชู ูุนูุฏูุง ููุณูู ุงูุฑููุฒ ููุณ ุงููุนูููุงุช).

ุฃุฎูุฑูุงุ ูููููุง ุฑุณู ุฎุฑูุทุฉ ูุฃู ูููุฉ ุฃู ุฑูุฒ ุฅูู ุงูุฃุญุฑู ูู ุงููุต ุงูุฃุตููุ ูุงูุนูุณ ุตุญูุญุ ุนุจุฑ ุทุฑู `word_to_chars()` ุฃู `token_to_chars()` ู`char_to_word()` ุฃู `char_to_token()`. ุนูู ุณุจูู ุงููุซุงูุ ุฃุฎุจุฑุชูุง ุทุฑููุฉ `word_ids()` ุฃู `##yl` ูู ุฌุฒุก ูู ุงููููุฉ ูู ุงูููุฑุณ 3ุ ูููู ุฃู ูููุฉ ูู ูู ุงูุฌููุฉุ ูููููุง ูุนุฑูุฉ ุฐูู ุจูุฐู ุงูุทุฑููุฉ:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python
Sylvain
```

ููุง ุฐูุฑูุง ุณุงุจููุงุ ูุชู ุชุดุบูู ูู ุฐูู ูู ุฎูุงู ุญูููุฉ ุฃู ููุณูู ุงูุฑููุฒ ุงูุณุฑูุน ูุญุชูุธ ุจุชุชุจุน ุงููุต ุงูุฐู ูุฃุชู ููู ูู ุฑูุฒ ูู ูุงุฆูุฉ *ุงูุฅุฒุงุญุงุช*. ูุชูุถูุญ ุงุณุชุฎุฏุงููุงุ ุณูุฑููู ุจุนุฏ ุฐูู ููููุฉ ุฅุนุงุฏุฉ ุฅูุชุงุฌ ูุชุงุฆุฌ ุฎุท ุฃูุงุจูุจ `token-classification` ูุฏูููุง.

ุฌุฑุจูุง ุจููุณู: ูู ุจุฅูุดุงุก ูุซุงู ูุตู ุฎุงุต ุจู ูุงูุธุฑ ุฅุฐุง ููุช ุชุณุชุทูุน ููู ุงูุฑููุฒ ุงููุฑุชุจุทุฉ ุจูุนุฑูู ุงููููุฉุ ููุฐูู ููููุฉ ุงุณุชุฎุฑุงุฌ ูุทุงูุงุช ุงูุฃุญุฑู ููููุฉ ูุงุญุฏุฉ. ููุญุตูู ุนูู ููุงุท ุงูููุงูุฃุฉุ ุฌุฑูุจ ุงุณุชุฎุฏุงู ุฌููุชูู ูุฅุฏุฎุงู ูุชุญูู ููุง ุฅุฐุง ูุงูุช ูุนุฑููุงุช ุงูุฌูู ููุทููุฉ ุจุงููุณุจุฉ ูู.

## ุฏุงุฎู ุฎุท ุฃูุงุจูุจ `token-classification`

ูู [ุงููุตู 1](/course/chapter1)ุ ุญุตููุง ุนูู ุฃูู ุชุฐูู ูุชุทุจูู NER - ุญูุซ ุชุชูุซู ุงููููุฉ ูู ุชุญุฏูุฏ ุงูุฃุฌุฒุงุก ูู ุงููุต ุงูุชู ุชุชูุงูู ูุน ุงูููุงูุงุช ูุซู ุงูุฃุดุฎุงุต ุฃู ุงูููุงูุน ุฃู ุงูููุธูุงุช - ุจุงุณุชุฎุฏุงู ูุธููุฉ `pipeline()` ูู ููุชุจุฉ ๐ค Transformers. ุซู ูู [ุงููุตู 2](/course/chapter2)ุ ุฑุฃููุง ููู ูููู ุฎุท ุงูุฃูุงุจูุจ ุจุชุฌููุน ุงููุฑุงุญู ุงูุซูุงุซ ุงูุถุฑูุฑูุฉ ููุญุตูู ุนูู ุงูุชูุจุคุงุช ูู ูุต ุฎุงู: ุงูุชูุณูู ุฅูู ุฑููุฒุ ูุชูุฑูุฑ ุงูุฅุฏุฎุงูุงุช ุนุจุฑ ุงููููุฐุฌุ ูุงููุนุงูุฌุฉ ุงููุงุญูุฉ. ุงูุฎุทูุชุงู ุงูุฃูููุงู ูู ุฎุท ุฃูุงุจูุจ `token-classification` ููุง ููุณ ุงูุฎุทูุชูู ุงูููุฌูุฏุชูู ูู ุฃู ุฎุท ุฃูุงุจูุจ ุขุฎุฑุ ูููู ุงููุนุงูุฌุฉ ุงููุงุญูุฉ ุฃูุซุฑ ุชุนููุฏูุง ุจุนุถ ุงูุดูุก - ุฏุนููุง ูุฑู ููู!

ุฃููุงูุ ุฏุนููุง ูุญุตู ุนูู ุฎุท ุฃูุงุจูุจ ุชุตููู ุงูุฑููุฒ ุญุชู ูุชููู ูู ุงูุญุตูู ุนูู ุจุนุถ ุงููุชุงุฆุฌ ูููุงุฑูุชูุง ูุฏูููุง. ุงููููุฐุฌ ุงููุณุชุฎุฏู ุจุดูู ุงูุชุฑุงุถู ูู [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)ุ ุฅูู ูุคุฏู NER ุนูู ุงูุฌูู:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
{'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
{'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
{'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
{'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
{'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
{'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ูุงู ุงููููุฐุฌ ุจุชุญุฏูุฏ ูู ุฑูุฒ ุชู ุฅูุดุงุคู ุจูุงุณุทุฉ "Sylvain" ุจุดูู ุตุญูุญ ุนูู ุฃูู ุดุฎุตุ ููู ุฑูุฒ ุชู ุฅูุดุงุคู ุจูุงุณุทุฉ "Hugging Face" ุนูู ุฃูู ููุธูุฉุ ูุงูุฑูุฒ "Brooklyn" ุนูู ุฃูู ูููุน. ูููููุง ุฃูุถูุง ุฃู ูุทูุจ ูู ุฎุท ุงูุฃูุงุจูุจ ุชุฌููุน ุงูุฑููุฒ ุงูุชู ุชุชูุงูู ูุน ููุณ ุงูููุงู:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
{'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
{'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ุณุชุบูุฑ ุงุณุชุฑุงุชูุฌูุฉ ุงูุชุฌููุน ุงููุญุฏุฏุฉ ุงูุฏุฑุฌุงุช ุงููุญุณูุจุฉ ููู ููุงู ูุฌูุน. ูุน `"simple"`ุ ูููู ุงูุฏุฑุฌุงุช ุจุจุณุงุทุฉ ูุชูุณุท ุงูุฏุฑุฌุงุช ููู ุฑูุฒ ูู ุงูููุงู ุงููุนุทู: ุนูู ุณุจูู ุงููุซุงูุ ุชููู ุฏุฑุฌุฉ "Sylvain" ูู ูุชูุณุท ุงูุฏุฑุฌุงุช ุงูุชู ุฑุฃููุงูุง ูู ุงููุซุงู ุงูุณุงุจู ููุฑููุฒ `S`ุ `##yl`ุ `##va`ุ ู`##in`. ุงูุงุณุชุฑุงุชูุฌูุงุช ุงูุฃุฎุฑู ุงููุชุงุญุฉ ูู:

- `"first"`ุ ุญูุซ ุชููู ุฏุฑุฌุฉ ูู ููุงู ูู ุฏุฑุฌุฉ ุงูุฑูุฒ ุงูุฃูู ูู ุฐูู ุงูููุงู (ูุฐุง ุจุงููุณุจุฉ ูู "Sylvain" ุณุชููู 0.993828ุ ุฏุฑุฌุฉ ุงูุฑูุฒ `S`)
- `"max"`ุ ุญูุซ ุชููู ุฏุฑุฌุฉ ูู ููุงู ูู ุงูุฏุฑุฌุฉ ุงููุตูู ููุฑููุฒ ูู ุฐูู ุงูููุงู (ูุฐุง ุจุงููุณุจุฉ ูู "Hugging Face" ุณุชููู 0.98879766ุ ุฏุฑุฌุฉ "Face")
- `"average"`ุ ุญูุซ ุชููู ุฏุฑุฌุฉ ูู ููุงู ูู ูุชูุณุท ุฏุฑุฌุงุช ุงููููุงุช ุงูุชู ูุชููู ูููุง ุฐูู ุงูููุงู (ูุฐุง ุจุงููุณุจุฉ ูู "Sylvain" ูู ูููู ููุงู ุงุฎุชูุงู ุนู ุงุณุชุฑุงุชูุฌูุฉ `"simple"`ุ ูููู "Hugging Face" ุณุชููู ููุง ุฏุฑุฌุฉ 0.9819ุ ูุชูุณุท ุงูุฏุฑุฌุงุช ูู "Hugging"ุ 0.975ุ ู"Face"ุ 0.98879)

ูุงูุขู ุฏุนููุง ูุฑู ููู ูุญุตู ุนูู ูุฐู ุงููุชุงุฆุฌ ุฏูู ุงุณุชุฎุฏุงู ูุธููุฉ `pipeline()`!
### ูู ุงููุฏุฎูุงุช ุฅูู ุงูุชูุจุคุงุช

ุฃููุงูุ ูุญุชุงุฌ ุฅูู ุชุญููู ูุฏุฎูุงุชูุง ุฅูู ุฑููุฒ ูุฑูุฑูุง ุนุจุฑ ุงููููุฐุฌ. ูุชู ุฐูู ุจุงูุถุจุท ููุง ูู [ุงููุตู 2](/course/chapter2)ุ ูููู ุจุชูููุฐ ูุญูู ุงูุฑููุฒ ูุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ูุฆุงุช `AutoXxx` ุซู ูุณุชุฎุฏููุง ูู ูุซุงููุง:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

ูุธุฑูุง ูุฃููุง ูุณุชุฎุฏู `AutoModelForTokenClassification` ููุงุ ูุฅููุง ูุญุตู ุนูู ูุฌููุนุฉ ูุงุญุฏุฉ ูู ุงูููุบุงุฑูุชูุงุช ููู ุฑูุฒ ูู ุชุณูุณู ุงูุฅุฏุฎุงู:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

ูุญู ูุฏููุง ุฏูุนุฉ ูุงุญุฏุฉ ุชุชููู ูู ุชุณูุณู ูุงุญุฏ ูู 19 ุฑูุฒูุงุ ููููููุฐุฌ 9 ุชุณููุงุช ูุฎุชููุฉุ ูุฐุง ูุฅู ูุงุชุฌ ุงููููุฐุฌ ูู ุดูู 1 ร 19 ร 9. ููุง ูู ุงูุญุงู ูู ุฎุท ุฃูุงุจูุจ ุชุตููู ุงููุตุ ูุณุชุฎุฏู ุฏุงูุฉ softmax ูุชุญููู ูุฐู ุงูููุบุงุฑูุชูุงุช ุฅูู ุงุญุชูุงูุงุชุ ููุฃุฎุฐ argmax ููุญุตูู ุนูู ุชูุจุคุงุช (ูุงุญุธ ุฃูู ูููููุง ุฃุฎุฐ argmax ุนูู ุงูููุบุงุฑูุชูุงุช ูุฃู softmax ูุง ุชุบูุฑ ุงูุชุฑุชูุจ):

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

ูุญุชูู atribut `model.config.id2label` ุนูู ุฎุฑูุทุฉ ุงูููุงุฑุณ ุฅูู ุงูุชุณููุงุช ุงูุชู ูููููุง ุงุณุชุฎุฏุงููุง ูููู ุงูุชูุจุคุงุช:

```py
model.config.id2label
```

```python out
{0: 'O',
1: 'B-MISC',
2: 'I-MISC',
3: 'B-PER',
4: 'I-PER',
5: 'B-ORG',
6: 'I-ORG',
7: 'B-LOC',
8: 'I-LOC'}
```

ููุง ุฑุฃููุง ุณุงุจููุงุ ููุงู 9 ุชุณููุงุช: `O` ูู ุงูุชุณููุฉ ููุฑููุฒ ุงูุชู ูุง ุชูุฌุฏ ูู ุฃู ููุงู ูุณูู (ุชูู ุนูู "ุฎุงุฑุฌ")ุ ุซู ูุฏููุง ุชุณููุชุงู ููู ููุน ูู ุงูููุงูุงุช (ูุชููุนุฉุ ุดุฎุตุ ููุธูุฉุ ููููุน). ุชุดูุฑ ุงูุชุณููุฉ `B-XXX` ุฅูู ุฃู ุงูุฑูุฒ ููุน ูู ุจุฏุงูุฉ ููุงู `XXX` ูุชุดูุฑ ุงูุชุณููุฉ `I-XXX` ุฅูู ุฃู ุงูุฑูุฒ ููุน ุฏุงุฎู ุงูููุงู `XXX`. ุนูู ุณุจูู ุงููุซุงูุ ูู ุงููุซุงู ุงูุญุงููุ ูุชููุน ุฃู ูุตูู ุงููููุฐุฌ ุงูุฑูุฒ `S` ุนูู ุฃูู `B-PER` (ุจุฏุงูุฉ ููุงู ุงูุดุฎุต) ูุงูุฑููุฒ `##yl`ุ `##va` ู`##in` ูู `I-PER` (ุฏุงุฎู ููุงู ุงูุดุฎุต).

ูุฏ ุชุนุชูุฏ ุฃู ุงููููุฐุฌ ูุงู ุฎุงุทุฆูุง ูู ูุฐู ุงูุญุงูุฉ ูุฃูู ุฃุนุทู ุงูุชุณููุฉ `I-PER` ูุฌููุน ูุฐู ุงูุฑููุฒ ุงูุฃุฑุจุนุฉุ ููู ูุฐุง ููุณ ุตุญูุญูุง ุชูุงููุง. ูู ุงููุงูุนุ ููุงู ูุณูุงู ููุฐู ุงูุชุณููุงุช `B-` ู`I-`: *IOB1* ู*IOB2*. ุชูุณูู IOB2 (ุจุงูููู ุงููุฑุฏู ุฃุฏูุงู)ุ ูู ุงูุชูุณูู ุงูุฐู ูุฏููุงู ูู ุญูู ุฃูู ูู ุชูุณูู IOB1 (ุจุงูููู ุงูุฃุฒุฑู)ุ ูุง ุชูุณุชุฎุฏู ุงูุชุณููุงุช ุงูุชู ุชุจุฏุฃ ุจู `B-` ูุทูููุง ุฅูุง ููุตู ููุงููู ูุชุฌุงูุฑูู ูู ููุณ ุงูููุน. ุชู ุถุจุท ุงููููุฐุฌ ุงูุฐู ูุณุชุฎุฏูู ุนูู ูุฌููุนุฉ ุจูุงูุงุช ุจุงุณุชุฎุฏุงู ูุฐุง ุงูุชูุณููุ ููู ุงูุณุจุจ ูู ุชุนูููู ุงูุชุณููุฉ `I-PER` ุฅูู ุงูุฑูุฒ `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 ููุงุจู IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 ููุงุจู IOB2 format"/>
</div>

ูุน ูุฐู ุงูุฎุฑูุทุฉุ ูุญู ูุณุชุนุฏูู ูุชูุฑุงุฑ (ุจุฃููููุง ุชูุฑูุจูุง) ูุชุงุฆุฌ ุฎุท ุงูุฃูุงุจูุจ ุงูุฃูู - ูููููุง ููุท ุงูุญุตูู ุนูู ุงููุชูุฌุฉ ูุงูุชุณููุฉ ููู ุฑูุฒ ูู ูุชู ุชุตูููู ุนูู ุฃูู `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
label = model.config.id2label[pred]
if label != "O":
results.append(
{"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
)

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
{'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
{'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
{'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
{'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
{'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
{'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

ูุฐุง ูุดุงุจู ุฌุฏูุง ููุง ูุงู ูุฏููุง ูู ูุจูุ ุจุงุณุชุซูุงุก ูุงุญุฏ: ูุฏู ููุง ุฎุท ุงูุฃูุงุจูุจ ุฃูุถูุง ูุนูููุงุช ุญูู `start` ู`end` ููู ููุงู ูู ุงูุฌููุฉ ุงูุฃุตููุฉ. ููุง ูุฃุชู ุฏูุฑ ุฎุฑูุทุฉ ุงูุฅุฒุงุญุฉ. ููุญุตูู ุนูู ุงูุฅุฒุงุญุงุชุ ูุง ุนูููุง ุณูู ุชุนููู `return_offsets_mapping=True` ุนูุฏ ุชุทุจูู ูุญูู ุงูุฑููุฒ ุนูู ูุฏุฎูุงุชูุง:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
(33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

ูู ุฒูุฌ ูู ุงูุชุฏุงุฏ ุงููุต ุงูููุงุจู ููู ุฑูุฒุ ุญูุซ ูุชู ุญุฌุฒ `(0ุ 0)` ููุฑููุฒ ุงูุฎุงุตุฉ. ุฑุฃููุง ุณุงุจููุง ุฃู ุงูุฑูุฒ ูู ุงูููุฑุณ 5 ูู `##yl`ุ ูุงูุฐู ูู `(12ุ 14)` ูุฅุฒุงุญุงุช ููุง. ุฅุฐุง ุญุตููุง ุนูู ุงูุดุฑูุญุฉ ุงูููุงุจูุฉ ูู ูุซุงููุง:

```py
example[12:14]
```

ูุญุตู ุนูู ุงูุชุฏุงุฏ ุงููุต ุงูุตุญูุญ ุจุฏูู `##`:

```python out
yl
```

ุจุงุณุชุฎุฏุงู ูุฐุงุ ูููููุง ุงูุขู ุงุณุชููุงู ุงููุชุงุฆุฌ ุงูุณุงุจูุฉ:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
label = model.config.id2label[pred]
if label != "O":
start, end = offsets[idx]
results.append(
{
"entity": label,
"score": probabilities[idx][pred],
"word": tokens[idx],
"start": start,
"end": end,
}
)

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
{'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
{'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
{'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
{'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
{'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
{'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ูุฐุง ูู ููุณู ูุง ุญุตููุง ุนููู ูู ุฎุท ุงูุฃูุงุจูุจ ุงูุฃูู!

### ุชุฌููุน ุงูููุงูุงุช

ุฅู ุงุณุชุฎุฏุงู ุงูุฅุฒุงุญุงุช ูุชุญุฏูุฏ ููุงุชูุญ ุงูุจุฏุงูุฉ ูุงูููุงูุฉ ููู ููุงู ุฃูุฑ ูููุฏุ ูููู ูุฐู ุงููุนูููุงุช ููุณุช ุถุฑูุฑูุฉ ุชูุงููุง. ููุน ุฐููุ ุนูุฏูุง ูุฑูุฏ ุชุฌููุน ุงูููุงูุงุช ูุนูุงุ ูุฅู ุงูุฅุฒุงุญุงุช ุณุชููุฑ ุนูููุง ุงููุซูุฑ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุบูุฑ ุงููุฑุชุจุฉ. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ุฃุฑุฏูุง ุชุฌููุน ุงูุฑููุฒ `Hu`ุ `##gging`ุ ู`Face`ุ ููููููุง ูุถุน ููุงุนุฏ ุฎุงุตุฉ ุชูุต ุนูู ุฃูู ูุฌุจ ุฅุฑูุงู ุงูุฃูููู ุฃุซูุงุก ุฅุฒุงูุฉ `##`ุ ููุฌุจ ุฅุถุงูุฉ `Face` ูุน ูุณุงูุฉ ูุฃูู ูุง ูุจุฏุฃ ุจู `##` - ูููู ุฐูู ูู ููุฌุญ ุฅูุง ููุฐุง ุงูููุน ุงููุญุฏุฏ ูู ูุญูู ุงูุฑููุฒ. ุณูุชุนูู ุนูููุง ูุชุงุจุฉ ูุฌููุนุฉ ุฃุฎุฑู ูู ุงูููุงุนุฏ ููุญูู ุฌูู ุฃู ูุญูู ุชุฑููุฒ Byte-Pair (ุณูุชู ููุงูุดุชู ูุงุญููุง ูู ูุฐุง ุงููุตู).

ูุน ุงูุฅุฒุงุญุงุชุ ุชุฎุชูู ูู ูุฐู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุงููุฎุตุตุฉ: ูููููุง ุจุจุณุงุทุฉ ุฃุฎุฐ ุงูุงูุชุฏุงุฏ ูู ุงููุต ุงูุฃุตูู ุงูุฐู ูุจุฏุฃ ุจุงูุฑูุฒ ุงูุฃูู ูููุชูู ุจุงูุฑูุฒ ุงูุฃุฎูุฑ. ูุฐููุ ูู ุญุงูุฉ ุงูุฑููุฒ `Hu`ุ `##gging`ุ ู`Face`ุ ูุฌุจ ุฃู ูุจุฏุฃ ุนูุฏ ุญุฑู 33 (ุจุฏุงูุฉ `Hu`) ูููุชูู ูุจู ุญุฑู 45 (ููุงูุฉ `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

ูููุชุงุจุฉ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุงูุชู ุชููู ุจูุนุงูุฌุฉ ุงูุชูุจุคุงุช ุจุนุฏ ุชุฌููุน ุงูููุงูุงุชุ ุณูููู ุจุชุฌููุน ุงูููุงูุงุช ุงูุชู ุชููู ูุชุชุงููุฉ ูููุณููุฉ ุจู `I-XXX`ุ ุจุงุณุชุซูุงุก ุงูุฃููุ ูุงูุฐู ูููู ุฃู ูููู ููุณูููุง ุจู `B-XXX` ุฃู `I-XXX` (ูุฐุงุ ูุชููู ุนู ุชุฌููุน ุงูููุงู ุนูุฏูุง ูุญุตู ุนูู `O`ุ ุฃู ููุน ุฌุฏูุฏ ูู ุงูููุงูุ ุฃู `B-XXX` ุงูุฐู ูุฎุจุฑูุง ุจุฃู ููุงููุง ูู ููุณ ุงูููุน ุจุฏุฃ ููุชู):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
pred = predictions[idx]
label = model.config.id2label[pred]
if label != "O":
# Remove the B- or I-
label = label[2:]
start, _ = offsets[idx]

# Grab all the tokens labeled with I-label
all_scores = []
while (
idx < len(predictions)
and model.config.id2label[predictions[idx]] == f"I-{label}"
):
all_scores.append(probabilities[idx][pred])
_, end = offsets[idx]
idx += 1

# The score is the mean of all the scores of the tokens in that grouped entity
score = np.mean(all_scores).item()
word = example[start:end]
results.append(
{
"entity_group": label,
"score": score,
"word": word,
"start": start,
"end": end,
}
)
idx += 1

print(results)
```

ููุญุตู ุนูู ููุณ ุงููุชุงุฆุฌ ููุง ูู ุงูุญุงู ูุน ุฎุท ุฃูุงุจูุจูุง ุงูุซุงูู!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
{'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
{'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ูุซุงู ุขุฎุฑ ุนูู ูููุฉ ุชููู ูููุง ูุฐู ุงูุฅุฒุงุญุงุช ูููุฏุฉ ููุบุงูุฉ ูู ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ. ุงูุบูุต ูู ุฎุท ุฃูุงุจูุจ ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉุ ูุงูุฐู ุณูููู ุจู ูู ุงููุณู ุงูุชุงููุ ุณูููููุง ุฃูุถูุง ูู ุฅููุงุก ูุธุฑุฉ ุนูู ููุฒุฉ ูุญููุงุช ุงูุฑููุฒ ุงูุฃุฎูุฑุฉ ูู ููุชุจุฉ ๐ค Transformers: ุงูุชุนุงูู ูุน ุงูุฑููุฒ ุงููุชุฏููุฉ ุนูุฏ ุงูุชุทุงุน ุฅุฏุฎุงู ุฅูู ุทูู ูุนูู.