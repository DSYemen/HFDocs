<FrameworkSwitchCourse {fw} />

# القوى الخاصة للمحللات السريعة [[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}

في هذا القسم، سنلقي نظرة فاحصة على قدرات المحللات في 🤗 Transformers. حتى الآن، لم نستخدمها سوى لتحليل المدخلات أو فك ترميز المعرّفات (IDs) إلى نص، ولكن المحللات - خاصة تلك المدعومة بمكتبة 🤗 Tokenizers - يمكنها فعل الكثير. لتوضيح هذه الميزات الإضافية، سنستكشف كيفية إعادة إنتاج نتائج `token-classification` (التي أطلقنا عليها اسم `ner`) وخطوط أنابيب `question-answering` التي واجهناها لأول مرة في [الفصل 1](/course/chapter1).

<Youtube id="g8quOxoqhHQ"/>

في المناقشة التالية، سنميز غالبًا بين المحللات "البطيئة" و"السريعة". المحللات البطيئة هي تلك المكتوبة بلغة بايثون داخل مكتبة 🤗 Transformers، في حين أن الإصدارات السريعة هي تلك التي توفرها مكتبة 🤗 Tokenizers، والمكتوبة بلغة Rust. إذا كنت تتذكر الجدول من [الفصل 5](/course/chapter5/3) الذي أبلغ عن المدة التي استغرقها محلل سريع وآخر بطيء لتحليل مجموعة بيانات مراجعة الأدوية، فيجب أن تكون لديك فكرة عن سبب تسميتنا لها بالسريعة والبطيئة:

|               | محلل سريع | محلل بطيء
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

⚠️ عند تحليل جملة واحدة، لن ترى دائمًا فرقًا في السرعة بين الإصدارات السريعة والبطيئة من نفس المحلل. في الواقع، قد يكون الإصدار السريع أبطأ بالفعل! لن تتمكن من رؤية الفرق بوضوح إلا عند تحليل الكثير من النصوص بالتوازي في نفس الوقت.

</Tip>

## الترميز الدفعي [[batch-encoding]]

<Youtube id="3umI3tm27Vw"/>

مخرج المحلل ليس مجرد قاموس بايثون؛ ما نحصل عليه في الواقع هو كائن `BatchEncoding` خاص. إنه فئة فرعية من القاموس (وهو السبب في أننا تمكنا من الفهرسة في تلك النتيجة دون أي مشكلة من قبل)، ولكن مع طرق إضافية تستخدمها المحللات السريعة بشكل أساسي.

إلى جانب قدرات التوازي، فإن الوظيفة الرئيسية للمحللات السريعة هي أنها تتتبع دائمًا النطاق الأصلي للنصوص التي تأتي منها الرموز النهائية - وهي ميزة نسميها *رسم الخرائط للتعويض*. وهذا بدوره يفتح ميزات مثل رسم خريطة لكل كلمة إلى الرموز التي أنتجتها أو رسم خريطة لكل حرف من النص الأصلي إلى الرمز الذي يوجد بداخله، والعكس صحيح.

دعنا نلقي نظرة على مثال:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

كما ذكرنا سابقًا، نحصل على كائن `BatchEncoding` في مخرج المحلل:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

بما أن فئة `AutoTokenizer` تختار محللًا سريعًا بشكل افتراضي، يمكننا استخدام الطرق الإضافية التي يوفرها هذا الكائن `BatchEncoding`. لدينا طريقتان للتحقق مما إذا كان المحلل الخاص بنا سريعًا أو بطيئًا. يمكننا إما التحقق من سمة `is_fast` للمحلل:

```python
tokenizer.is_fast
```

```python out
True
```

أو التحقق من السمة نفسها في `encoding`:

```python
encoding.is_fast
```

```python out
True
```

دعنا نرى ما يمكّننا المحلل السريع من فعله. أولاً، يمكننا الوصول إلى الرموز دون الحاجة إلى تحويل المعرّفات (IDs) مرة أخرى إلى رموز:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

في هذه الحالة، الرمز في الفهرس 5 هو `##yl`، وهو جزء من الكلمة "Sylvain" في الجملة الأصلية. يمكننا أيضًا استخدام طريقة `word_ids()` للحصول على فهرس الكلمة التي يأتي منها كل رمز:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

يمكننا أن نرى أن الرموز الخاصة للمحلل `[CLS]` و`[SEP]` يتم تعيينها إلى `None`، ثم يتم تعيين كل رمز إلى الكلمة التي يأتي منها. هذا مفيد بشكل خاص لتحديد ما إذا كان الرمز في بداية كلمة أو إذا كان رمزان في نفس الكلمة. يمكننا الاعتماد على بادئة `##` لذلك، ولكنها تعمل فقط للمحللات الشبيهة بBERT؛ تعمل هذه الطريقة لأي نوع من المحللات طالما أنها سريعة. في الفصل التالي، سنرى كيف يمكننا استخدام هذه القدرة لتطبيق العلامات التي لدينا لكل كلمة بشكل صحيح على الرموز في مهام مثل التعرف على الكيانات المسماة (NER) ووضع العلامات على أجزاء الكلام (POS). يمكننا أيضًا استخدامها لحجب جميع الرموز التي تأتي من نفس الكلمة في النمذجة اللغوية المقنعة (تقنية تسمى _whole word masking_).

<Tip>

مفهوم ما هي الكلمة معقد. على سبيل المثال، هل "I'll" (اختصار لـ "I will") تعتبر كلمة واحدة أو كلمتين؟ في الواقع، يعتمد ذلك على المحلل وعملية التحليل المسبق التي يطبقها. بعض المحللات تقسم فقط على المسافات، لذا فهي ستعتبر هذا كلمة واحدة. يستخدم البعض الآخر علامات الترقيم بالإضافة إلى المسافات، لذا فسيتم اعتبارها كلمتين.

✏️ **جربها!** أنشئ محللًا من نقاط التفتيش `bert-base-cased` و`roberta-base` وحلل "81s" باستخدامهما. ماذا تلاحظ؟ ما هي معرّفات الكلمات (word IDs)؟

</Tip>

وبالمثل، هناك طريقة `sentence_ids()` التي يمكننا استخدامها لتعيين رمز إلى الجملة التي جاء منها (على الرغم من أنه في هذه الحالة، يمكن أن تعطينا `token_type_ids` التي يعيدها المحلل نفس المعلومات).

أخيرًا، يمكننا تعيين أي كلمة أو رمز إلى أحرف في النص الأصلي، والعكس صحيح، عبر طرق `word_to_chars()` أو `token_to_chars()` و`char_to_word()` أو `char_to_token()`. على سبيل المثال، أخبرتنا طريقة `word_ids()` أن `##yl` هو جزء من الكلمة في الفهرس 3، ولكن أي كلمة هي في الجملة؟ يمكننا معرفة ذلك على النحو التالي:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

كما ذكرنا سابقاً، كل هذا ممكن بفضل حقيقة أن المرمز السريع يحتفظ بمسار مقطع النص الذي يأتي منه كل رمز في قائمة من *الإزاحات*. لتوضيح استخدامها، سنعرض لك كيفية تكرار نتائج خط أنابيب `token-classification` يدويًا.

<Tip>

✏️ **جربها!** قم بإنشاء نصك الخاص وانظر إذا كنت تستطيع فهم الرموز المرتبطة بمعرف الكلمة، وأيضاً كيفية استخراج مقاطع الأحرف لكلمة واحدة. وللحصول على نقاط إضافية، جرب استخدام جملتين كمدخلات وانظر إذا كانت معرفات الجمل منطقية بالنسبة لك.

</Tip>

## داخل خط أنابيب `token-classification`[[inside-the-token-classification-pipeline]]

في [الفصل 1](/course/chapter1) حصلنا على أول تذوق لتطبيق NER -- حيث تكون المهمة هي تحديد أجزاء النص التي تتوافق مع الكيانات مثل الأشخاص، والمواقع، أو المنظمات -- مع وظيفة `pipeline()` في 🤗 Transformers. ثم، في [الفصل 2](/course/chapter2)، رأينا كيف يقوم خط الأنابيب بجمع المراحل الثلاث الضرورية للحصول على التنبؤات من نص خام: التجزئة، وإدخال المدخلات عبر النموذج، والمعالجة اللاحقة. الخطوتان الأوليتان في خط أنابيب `token-classification` هما نفس الخطوتين في أي خط أنابيب آخر، لكن المعالجة اللاحقة أكثر تعقيدًا بقليل -- دعونا نرى كيف!

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### الحصول على النتائج الأساسية باستخدام خط الأنابيب[[getting-the-base-results-with-the-pipeline]]

أولاً، دعنا نحصل على خط أنابيب تصنيف الرموز بحيث يمكننا الحصول على بعض النتائج لمقارنتها يدويًا. النموذج المستخدم بشكل افتراضي هو [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)؛ يقوم بتنفيذ NER على الجمل:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

حدد النموذج بشكل صحيح كل رمز تم توليده بواسطة "Sylvain" كشخص، وكل رمز تم توليده بواسطة "Hugging Face" كمنظمة، والرمز "Brooklyn" كموقع. يمكننا أيضًا أن نطلب من خط الأنابيب تجميع الرموز التي تتوافق مع نفس الكيان:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

سيؤدي اختيار `aggregation_strategy` إلى تغيير الدرجات المحسوبة لكل كيان مجمع. مع `"simple"` تكون النتيجة هي متوسط درجات كل رمز في الكيان المعطى: على سبيل المثال، تكون نتيجة "Sylvain" هي متوسط الدرجات التي رأيناها في المثال السابق للرموز `S`، `##yl`، `##va`، و `##in`. الاستراتيجيات الأخرى المتاحة هي:

- `"first"`، حيث تكون نتيجة كل كيان هي نتيجة الرمز الأول لذلك الكيان (لذلك بالنسبة لـ "Sylvain" ستكون 0.993828، نتيجة الرمز `S`)
- `"max"`، حيث تكون نتيجة كل كيان هي النتيجة القصوى للرموز في ذلك الكيان (لذلك بالنسبة لـ "Hugging Face" ستكون 0.98879766، نتيجة "Face")
- `"average"`، حيث تكون نتيجة كل كيان هي متوسط درجات الكلمات المكونة لذلك الكيان (لذلك بالنسبة لـ "Sylvain" لن يكون هناك فرق عن استراتيجية `"simple"`، لكن "Hugging Face" ستحصل على نتيجة 0.9819، متوسط درجات "Hugging"، 0.975، و "Face"، 0.98879)

الآن دعنا نرى كيفية الحصول على هذه النتائج بدون استخدام وظيفة `pipeline()`!

### من المدخلات إلى التنبؤات[[from-inputs-to-predictions]]

{#if fw === 'pt'}

أولاً نحتاج إلى تجزئة مدخلاتنا وإدخالها عبر النموذج. يتم ذلك بالضبط كما في [الفصل 2](/course/chapter2)؛ نقوم بتنفيذ المرمز والنموذج باستخدام فئات `AutoXxx` ثم نستخدمها على مثالنا:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

بما أننا نستخدم `AutoModelForTokenClassification` هنا، نحصل على مجموعة واحدة من اللوغاريتمات لكل رمز في تسلسل الإدخال:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

أولاً نحتاج إلى تجزئة مدخلاتنا وإدخالها عبر النموذج. يتم ذلك بالضبط كما في [الفصل 2](/course/chapter2)؛ نقوم بتنفيذ المرمز والنموذج باستخدام فئات `TFAutoXxx` ثم نستخدمها على مثالنا:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

بما أننا نستخدم `TFAutoModelForTokenClassification` هنا، نحصل على مجموعة واحدة من اللوغاريتمات لكل رمز في تسلسل الإدخال:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

لدينا دفعة واحدة مع 1 تسلسل من 19 رمز والنموذج لديه 9 تسميات مختلفة، لذلك يكون ناتج النموذج على شكل 1 x 19 x 9. مثل خط أنابيب تصنيف النص، نستخدم وظيفة softmax لتحويل اللوغاريتمات إلى احتمالات، ونأخذ argmax للحصول على التنبؤات (لاحظ أنه يمكننا أخذ argmax على اللوغاريتمات لأن softmax لا تغير الترتيب):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

يحتوي خاصية `model.config.id2label` على خريطة الفهارس إلى التسميات التي يمكننا استخدامها لفهم التنبؤات:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```
كما رأينا سابقاً، هناك 9 تسميات: `O` هي التسمية للرموز التي لا تنتمي لأي كيان مسمى (وهي اختصار لـ "خارجي")، ولدينا بعد ذلك تسميتين لكل نوع من الكيانات (متنوع، شخص، منظمة، ومكان). التسمية `B-XXX` تشير إلى أن الرمز هو بداية كيان `XXX` والتسمية `I-XXX` تشير إلى أن الرمز هو داخل كيان `XXX`. على سبيل المثال، في المثال الحالي، نتوقع أن يصنف نموذجنا الرمز `S` على أنه `B-PER` (بداية كيان شخص) والرموز `##yl`، `##va` و `##in` على أنها `I-PER` (داخل كيان شخص).

قد تعتقد أن النموذج كان خاطئاً في هذه الحالة لأنه أعطى التسمية `I-PER` لجميع هذه الرموز الأربعة، ولكن هذا ليس صحيحاً تماماً. في الواقع، هناك تنسيقين لهاتين التسميتين `B-` و `I-`: *IOB1* و *IOB2*. تنسيق IOB2 (باللون الوردي أدناه)، هو التنسيق الذي قدمناه، بينما في تنسيق IOB1 (باللون الأزرق)، تستخدم التسميات التي تبدأ بـ `B-` فقط لفصل كيانين متجاورين من نفس النوع. النموذج الذي نستخدمه تم ضبطه على مجموعة بيانات باستخدام ذلك التنسيق، وهو السبب في أنه يعين التسمية `I-PER` للرمز `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

مع هذا المخطط، نحن مستعدون لتكرار (تقريباً بالكامل) نتائج الأنبوب الأول -- يمكننا فقط الحصول على النتيجة والتسمية لكل رمز لم يتم تصنيفه على أنه `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

هذا مشابه جداً لما كان لدينا سابقاً، مع استثناء واحد: الأنبوب أعطانا أيضاً معلومات عن `start` و `end` لكل كيان في الجملة الأصلية. هنا يأتي دور خريطة الإزاحة لدينا. للحصول على الإزاحات، كل ما علينا فعله هو تعيين `return_offsets_mapping=True` عندما نطبق الرمز على مدخلاتنا:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

كل زوج هو النطاق النصي المقابل لكل رمز، حيث `(0, 0)` محجوز للرموز الخاصة. رأينا سابقاً أن الرمز في الفهرس 5 هو `##yl`، والذي له `(12, 14)` كإزاحات هنا. إذا حصلنا على الشريحة المقابلة في مثالنا:


```py
example[12:14]
```

سنحصل على النطاق النصي الصحيح بدون `##`:

```python out
yl
```

باستخدام هذا، يمكننا الآن إكمال النتائج السابقة:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

هذا هو نفس ما حصلنا عليه من الأنبوب الأول!

### تجميع الكيانات [[grouping-entities]]

استخدام الإزاحات لتحديد مفاتيح البداية والنهاية لكل كيان مفيد، ولكن تلك المعلومات ليست ضرورية تماماً. عندما نريد تجميع الكيانات معاً، ومع ذلك، فإن الإزاحات ستوفر علينا الكثير من الرموز المعقدة. على سبيل المثال، إذا أردنا تجميع الرموز `Hu`، `##gging`، و `Face`، يمكننا وضع قواعد خاصة تقول أن الأولين يجب أن يكونا ملتصقين مع إزالة `##`، و `Face` يجب أن تضاف مع مسافة لأنها لا تبدأ بـ `##` -- ولكن ذلك سيعمل فقط لهذا النوع المحدد من الرموز. سيتعين علينا كتابة مجموعة أخرى من القواعد لرمز SentencePiece أو Byte-Pair-Encoding (سيتم مناقشتها لاحقاً في هذا الفصل).

مع الإزاحات، كل تلك الرموز المخصصة تختفي: يمكننا فقط أخذ النطاق في النص الأصلي الذي يبدأ بالرمز الأول وينتهي بالرمز الأخير. لذلك، في حالة الرموز `Hu`، `##gging`، و `Face`، يجب أن نبدأ عند الحرف 33 (بداية `Hu`) وننتهي قبل الحرف 45 (نهاية `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

لكتابة الرمز الذي يقوم بمعالجة التنبؤات أثناء تجميع الكيانات، سنقوم بتجميع الكيانات التي تكون متتالية ومصنفة بـ `I-XXX`، باستثناء الأول، والذي يمكن تصنيفه بـ `B-XXX` أو `I-XXX` (لذلك، نتوقف عن تجميع كيان عندما نحصل على `O`، أو نوع جديد من الكيانات، أو `B-XXX` الذي يخبرنا أن كيان من نفس النوع بدأ):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

ونحصل على نفس النتائج كما في أنبوبنا الثاني!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

مثال آخر لمهمة تكون فيها هذه الإزاحات مفيدة للغاية هو الإجابة على الأسئلة. الغوص في هذا الخط، والذي سنقوم به في القسم التالي، سيمكننا أيضًا من إلقاء نظرة على ميزة أخيرة لمقسّمات الرموز في مكتبة 🤗 Transformers: التعامل مع الرموز المتدفقة عندما نقوم بتقليص إدخال إلى طول معين.