# بناء محلل رموز، كتلة بكتلة [[building-a-tokenizer-block-by-block]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
]} />

كما رأينا في الأقسام السابقة، يتكون التحليل الرمزي من عدة خطوات:

- التوحيد القياسي (أي تنظيف النص الذي يعتبر ضرورياً، مثل إزالة المسافات أو علامات التشكيل، التوحيد القياسي للرموز، إلخ)
- ما قبل التحليل الرمزي (تقسيم الإدخال إلى كلمات)
- تشغيل الإدخال من خلال النموذج (باستخدام الكلمات ما قبل التحليل الرمزي لإنتاج تسلسل من الرموز)
- ما بعد المعالجة (إضافة الرموز الخاصة لمحلل الرموز، وتوليد قناع الانتباه ومعرفات نوع الرمز)

كذكرى، إليك نظرة أخرى على العملية الكلية:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="خط أنابيب التحليل الرمزي.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="خط أنابيب التحليل الرمزي.">
</div>

تم بناء مكتبة 🤗 Tokenizers لتوفير عدة خيارات لكل من تلك الخطوات، والتي يمكنك مزجها ومطابقتها معًا. في هذا القسم، سنرى كيف يمكننا بناء محلل رموز من الصفر، على عكس تدريب محلل رموز جديد من محلل قديم كما فعلنا في [القسم 2](/course/chapter6/2). بعد ذلك، ستتمكن من بناء أي نوع من محلل الرموز الذي يمكنك التفكير فيه!

<Youtube id="MR8tZm5ViWU"/>

وبشكل أكثر دقة، تم بناء المكتبة حول فئة `Tokenizer` مركزية مع كتل البناء المعاد تجميعها في وحدات فرعية:

- `normalizers` تحتوي على جميع أنواع `Normalizer` التي يمكنك استخدامها (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/normalizers)).
- `pre_tokenizers` تحتوي على جميع أنواع `PreTokenizer` التي يمكنك استخدامها (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)).
- `models` تحتوي على الأنواع المختلفة من `Model` التي يمكنك استخدامها، مثل `BPE` و`WordPiece` و`Unigram` (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/models)).
- `trainers` تحتوي على جميع الأنواع المختلفة من `Trainer` التي يمكنك استخدامها لتدريب نموذجك على مجموعة من البيانات (واحد لكل نوع من النماذج؛ القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/trainers)).
- `post_processors` تحتوي على الأنواع المختلفة من `PostProcessor` التي يمكنك استخدامها (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/post-processors)).
- `decoders` تحتوي على الأنواع المختلفة من `Decoder` التي يمكنك استخدامها لفك تشفير مخرجات التحليل الرمزي (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/components#decoders)).

يمكنك العثور على القائمة الكاملة لكتل البناء [هنا](https://huggingface.co/docs/tokenizers/components).

## الحصول على مجموعة من البيانات [[acquiring-a-corpus]]

لتدريب محلل الرموز الجديد، سنستخدم مجموعة صغيرة من النصوص (حتى تعمل الأمثلة بسرعة). الخطوات للحصول على مجموعة البيانات مماثلة لتلك التي اتخذناها في [بداية هذا الفصل](/course/chapter6/2)، ولكن هذه المرة سنستخدم مجموعة بيانات [WikiText-2](https://huggingface.co/datasets/wikitext):

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

الدالة `get_training_corpus()` هي مولد سيقوم بإنتاج دفعات من 1000 نص، والتي سنستخدمها لتدريب محلل الرموز.

🤗 يمكن أيضًا تدريب Tokenizers على ملفات النص مباشرة. إليك كيف يمكننا إنشاء ملف نصي يحتوي على جميع النصوص/الإدخالات من WikiText-2 التي يمكننا استخدامها محليًا:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

بعد ذلك، سنريكم كيفية بناء BERT وGPT-2 وXLNet محللات الرموز، كتلة بكتلة. سيعطينا ذلك مثالاً لكل من خوارزميات التحليل الرمزي الثلاثة الرئيسية: WordPiece وBPE وUnigram. لنبدأ مع BERT!

## بناء محلل رموز WordPiece من الصفر [[building-a-wordpiece-tokenizer-from-scratch]]

لبناء محلل رموز مع مكتبة 🤗 Tokenizers، نبدأ بإنشاء كائن `Tokenizer` مع `model`، ثم نحدد سمات `normalizer` و`pre_tokenizer` و`post_processor` و`decoder` إلى القيم التي نريدها.

لهذا المثال، سننشئ `Tokenizer` مع نموذج WordPiece:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

يجب علينا تحديد `unk_token` حتى يعرف النموذج ما الذي يجب إعادته عندما يواجه أحرفًا لم يرها من قبل. تشمل الحجج الأخرى التي يمكننا تعيينها هنا `vocab` لنموذجنا (سنقوم بتدريب النموذج، لذلك لا نحتاج إلى تعيين هذا) و`max_input_chars_per_word`، الذي يحدد طولًا أقصى لكل كلمة (سيتم تقسيم الكلمات الأطول من القيمة التي تم تمريرها).

الخطوة الأولى من التحليل الرمزي هي التوحيد القياسي، لذلك دعنا نبدأ بذلك. نظرًا لأن BERT يستخدم على نطاق واسع، هناك `BertNormalizer` مع الخيارات الكلاسيكية التي يمكننا تعيينها لـ BERT: `lowercase` و`strip_accents`، والتي تشرح نفسها بنفسها؛ `clean_text` لإزالة جميع أحرف التحكم واستبدال المسافات المتكررة بمسافة واحدة؛ و`handle_chinese_chars`، الذي يضع مسافات حول الأحرف الصينية. لتكرار محلل الرموز `bert-base-uncased`، يمكننا فقط تعيين هذا المحلل:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

ومع ذلك، بشكل عام، عند بناء محلل رموز جديد، لن يكون لديك حق الوصول إلى مثل هذا المحلل الرموز المفيد الذي تم تنفيذه بالفعل في مكتبة 🤗 Tokenizers - لذلك دعنا نرى كيفية إنشاء محلل الرموز BERT يدويًا. توفر المكتبة `Lowercase` محلل رموز و`StripAccents` محلل رموز، ويمكنك تكوين عدة محللات رموز باستخدام `Sequence`:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

نستخدم أيضًا `NFD` محلل رموز للرموز الموحدة، وإلا فإن `StripAccents` محلل الرموز لن يتعرف بشكل صحيح على الأحرف المشكولة ولن يزيلها.

كما رأينا من قبل، يمكننا استخدام طريقة `normalize_str()` لـ `normalizer` للتحقق من التأثير الذي يحدثه على نص معين:

```python
print(tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
```

```python out
hello how are u?
```

<Tip>

**للمزيد** إذا قمت باختبار الإصدارين السابقين من محللات الرموز هذه على سلسلة تحتوي على حرف الرموز الموحدة `u"\u0085"`، فستلاحظ بالتأكيد أن هذين المحللين ليسا متطابقين تمامًا.
لتجنب تعقيد الإصدار مع `normalizers.Sequence` كثيرًا، لم ندرج الاستبدالات العادية التي تتطلبها `BertNormalizer` عند تعيين حجة `clean_text` إلى `True` - وهو السلوك الافتراضي. ولكن لا تقلق: من الممكن الحصول على نفس التوحيد القياسي تمامًا دون استخدام `BertNormalizer` المفيد عن طريق إضافة `normalizers.Replace` إلى تسلسل المحللات.

</Tip>

الخطوة التالية هي خطوة ما قبل التحليل الرمزي. مرة أخرى، هناك `BertPreTokenizer` مسبق البناء يمكننا استخدامه:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

أو يمكننا بناؤه من الصفر:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

لاحظ أن `Whitespace` محلل الرموز المسبق يقسم على المسافات وعلى جميع الأحرف التي ليست أحرفًا أو أرقامًا أو حرف التسطير، لذلك فهو يقسم تقنيًا على المسافات وعلامات الترقيم:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```
إذا كنت تريد فقط التقسيم على المسافات البيضاء، فيجب عليك استخدام `WhitespaceSplit` كمعالج مسبق:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

مثل المعالجات، يمكنك استخدام `Sequence` لتركيب عدة معالجات مسبقة:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

الخطوة التالية في خط أنابيب التجزئة هي تشغيل المدخلات عبر النموذج. لقد حددنا نموذجنا بالفعل في التهيئة، لكننا ما زلنا بحاجة إلى تدريبه، مما سيتطلب `WordPieceTrainer`. الشيء الرئيسي الذي يجب تذكره عند إنشاء مدرب في 🤗 Tokenizers هو أنه يجب عليك تمرير جميع الرموز الخاصة التي تنوي استخدامها - وإلا فلن يقوم بإضافتها إلى المفردات، حيث أنها ليست في مجموعة التدريب:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

بالإضافة إلى تحديد `vocab_size` و `special_tokens`، يمكننا ضبط `min_frequency` (عدد المرات التي يجب أن يظهر فيها الرمز ليتم تضمينه في المفردات) أو تغيير `continuing_subword_prefix` (إذا كنا نريد استخدام شيء مختلف عن `##`).

لتدريب نموذجنا باستخدام المعالج الذي حددناه سابقًا، كل ما علينا فعله هو تنفيذ هذا الأمر:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

يمكننا أيضًا استخدام ملفات النص لتدريب معالجنا، والذي سيكون على النحو التالي (نعيد تهيئة النموذج باستخدام `WordPiece` فارغ مسبقًا):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

في كلتا الحالتين، يمكننا بعد ذلك اختبار المعالج على نص عن طريق استدعاء طريقة `encode()`:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

إن `encoding` الذي تم الحصول عليه هو `Encoding`، والذي يحتوي على جميع المخرجات الضرورية للمعالج في سماته المختلفة: `ids`، `type_ids`، `tokens`، `offsets`، `attention_mask`، `special_tokens_mask`، و `overflowing`.

الخطوة الأخيرة في خط أنابيب التجزئة هي ما بعد المعالجة. نحن بحاجة إلى إضافة الرمز `[CLS]` في البداية والرموز `[SEP]` في النهاية (أو بعد كل جملة، إذا كان لدينا زوج من الجمل). سنستخدم `TemplateProcessor` لهذا، ولكن أولاً نحتاج إلى معرفة معرفات الرموز `[CLS]` و `[SEP]` في المفردات:

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

لكتابة القالب لـ `TemplateProcessor`، يجب علينا تحديد كيفية معالجة جملة واحدة وزوج من الجمل. بالنسبة لكليهما، نكتب الرموز الخاصة التي نريد استخدامها؛ الجملة الأولى (أو الوحيدة) ممثلة بـ `$A`، بينما الجملة الثانية (إذا كانت الترميز لزوج) ممثلة بـ `$B`. بالنسبة لكل من هذه (الرموز الخاصة والجمل)، نقوم أيضًا بتحديد معرف نوع الرمز المقابل بعد علامة النقطتين.

لذلك يتم تعريف القالب الكلاسيكي لـ BERT على النحو التالي:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

لاحظ أنه يجب علينا تمرير معرفات الرموز الخاصة، بحيث يمكن للمعالج تحويلها إلى معرفاتها بشكل صحيح.

بمجرد إضافة هذا، عند العودة إلى مثالنا السابق، سنحصل على:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

وعلى زوج من الجمل، نحصل على النتيجة الصحيحة:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

لقد انتهينا تقريبًا من بناء هذا المعالج من الصفر - الخطوة الأخيرة هي تضمين فك التشفير:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

دعنا نختبرها على `encoding` السابق:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

رائع! يمكننا حفظ معالجنا في ملف JSON واحد على النحو التالي:

```python
tokenizer.save("tokenizer.json")
```

يمكننا بعد ذلك إعادة تحميل هذا الملف في كائن `Tokenizer` باستخدام طريقة `from_file()`:

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

لاستخدام هذا المعالج في 🤗 Transformers، يجب علينا تغليفه في `PreTrainedTokenizerFast`. يمكننا إما استخدام الفئة العامة أو، إذا كان معالجنا يتوافق مع نموذج موجود، استخدام تلك الفئة (هنا، `BertTokenizerFast`). إذا كنت تطبق هذا الدرس لبناء معالج جديد تمامًا، فسيتعين عليك استخدام الخيار الأول.

لتغليف المعالج في `PreTrainedTokenizerFast`، يمكننا إما تمرير المعالج الذي قمنا ببنائه كـ `tokenizer_object` أو تمرير ملف المعالج الذي قمنا بحفظه كـ `tokenizer_file`. الشيء الرئيسي الذي يجب تذكره هو أنه يجب علينا تعيين جميع الرموز الخاصة يدويًا، حيث لا يمكن لهذه الفئة الاستنتاج من كائن `tokenizer` أي رمز هو رمز القناع، الرمز `[CLS]`، إلخ:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # يمكنك التحميل من ملف المعالج، كبديل
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

إذا كنت تستخدم فئة معالج محددة (مثل `BertTokenizerFast`)، فستحتاج فقط إلى تحديد الرموز الخاصة المختلفة عن الرموز الافتراضية (هنا، لا شيء):

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

يمكنك بعد ذلك استخدام هذا المعالج مثل أي معالج آخر من 🤗 Transformers. يمكنك حفظه باستخدام طريقة `save_pretrained()`، أو تحميله إلى Hub باستخدام طريقة `push_to_hub()`.

الآن بعد أن رأينا كيفية بناء معالج WordPiece، دعنا نفعل الشيء نفسه لمعالج BPE. سنذهب بسرعة أكبر لأنك تعرف جميع الخطوات، وسنلقي الضوء فقط على الاختلافات.

## بناء معالج BPE من الصفر[[building-a-bpe-tokenizer-from-scratch]]

دعنا الآن نبني معالج GPT-2. مثل معالج BERT، نبدأ بتهيئة `Tokenizer` مع نموذج BPE:

```python
tokenizer = Tokenizer(models.BPE())
```

أيضًا مثل BERT، يمكننا تهيئة هذا النموذج بمفردات إذا كان لدينا واحدة (سنحتاج إلى تمرير `vocab` و `merges` في هذه الحالة)، ولكن بما أننا سنتدرب من الصفر، فلا نحتاج إلى القيام بذلك. نحن أيضًا لا نحتاج إلى تحديد `unk_token` لأن GPT-2 يستخدم BPE على مستوى البايت، والذي لا يتطلب ذلك.

لا يستخدم GPT-2 معالجًا طبيعيًا، لذلك نتخطى تلك الخطوة ونذهب مباشرة إلى ما قبل التجزئة:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

الخيار الذي أضفناه إلى `ByteLevel` هنا هو عدم إضافة مسافة في بداية الجملة (وهو الافتراضي في الحالات الأخرى). يمكننا إلقاء نظرة على ما قبل تجزئة نص مثال مثل السابق:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ġtest', (5, 10)), ('Ġpre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```
بعد ذلك يأتي النموذج، والذي يحتاج إلى تدريب. بالنسبة لـ GPT-2، فإن الرمز الخاص الوحيد هو رمز نهاية النص:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

كما هو الحال مع `WordPieceTrainer`، بالإضافة إلى `vocab_size` و `special_tokens`، يمكننا تحديد `min_frequency` إذا أردنا ذلك، أو إذا كان لدينا لاحقة نهاية الكلمة (مثل `</w>`)، يمكننا تعيينها باستخدام `end_of_word_suffix`.

يمكن تدريب هذا المحلل الصرفي أيضاً على ملفات النص:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

دعنا نلقي نظرة على تحليل نص عينة:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']
```

نطبق معالجة ما بعد مستوى البايت لمحلل GPT-2 الصرفي كما يلي:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

يشير خيار `trim_offsets = False` إلى المعالج اللاحق بأننا يجب أن نترك إزاحات الرموز التي تبدأ بـ 'Ġ' كما هي: بهذه الطريقة ستشير بداية الإزاحات إلى المسافة قبل الكلمة، وليس أول حرف من الكلمة (حيث أن المسافة هي جزء من الرمز من الناحية الفنية). دعنا نلقي نظرة على النتيجة مع النص الذي قمنا بتشفيره للتو، حيث `'Ġtest'` هو الرمز في الفهرس 4:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

أخيراً، نضيف فك تشفير مستوى البايت:

```python
tokenizer.decoder = decoders.ByteLevel()
```

ويمكننا التأكد من أنه يعمل بشكل صحيح:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

رائع! الآن بعد أن انتهينا، يمكننا حفظ المحلل الصرفي كما فعلنا من قبل، وتغليفه في `PreTrainedTokenizerFast` أو `GPT2TokenizerFast` إذا أردنا استخدامه في 🤗 Transformers:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

أو:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

كمثال أخير، سنريكم كيفية بناء محلل صرفي أحادي من الصفر.

## بناء محلل صرفي أحادي من الصفر [[building-a-unigram-tokenizer-from-scratch]]

دعنا الآن نبني محلل XLNet الصرفي. كما هو الحال في المحللات الصرفية السابقة، نبدأ بتهيئة `Tokenizer` مع نموذج أحادي:

```python
tokenizer = Tokenizer(models.Unigram())
```

مرة أخرى، يمكننا تهيئة هذا النموذج بمفردات إذا كان لدينا واحدة.

بالنسبة للتوحيد القياسي، يستخدم XLNet بعض الاستبدالات (التي تأتي من SentencePiece):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

يستبدل هذا الرمز <code>``</code> و <code>''</code> بـ <code>"</code> وأي تسلسل من مسافتين أو أكثر بمسافة واحدة، بالإضافة إلى إزالة التشكيل في النصوص التي سيتم تحليلها صرفياً.

المحلل الصرفي المسبق للاستخدام لأي محلل صرفي SentencePiece هو `Metaspace`:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

يمكننا إلقاء نظرة على التحليل الصرفي المسبق لنص مثال كما فعلنا من قبل:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("▁Let's", (0, 5)), ('▁test', (5, 10)), ('▁the', (10, 14)), ('▁pre-tokenizer!', (14, 29))]
```

بعد ذلك يأتي النموذج، والذي يحتاج إلى تدريب. لدى XLNet العديد من الرموز الخاصة:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

حجة مهمة جداً يجب عدم نسيانها للـ `UnigramTrainer` هي `unk_token`. يمكننا أيضاً تمرير حجج أخرى خاصة بخوارزمية Unigram، مثل `shrinking_factor` لكل خطوة نزيل فيها الرموز (القيمة الافتراضية هي 0.75) أو `max_piece_length` لتحديد الطول الأقصى لرموز معينة (القيمة الافتراضية هي 16).

يمكن تدريب هذا المحلل الصرفي أيضاً على ملفات النص:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

دعنا نلقي نظرة على تحليل نص عينة:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['▁Let', "'", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']
```

من خصائص XLNet أنه يضع الرمز `<cls>` في نهاية الجملة، مع معرف نوع 2 (لتمييزه عن الرموز الأخرى). إنه يضيف مسافات على اليسار، نتيجة لذلك. يمكننا التعامل مع جميع الرموز الخاصة ومعرفات نوع الرموز باستخدام قالب، كما هو الحال في BERT، ولكن أولاً يجب علينا الحصول على معرفات الرموز `<cls>` و `<sep>`:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

يبدو القالب كالتالي:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

ويمكننا اختبار عمله عن طريق تشفير زوج من الجمل:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['▁Let', "'", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', 
  '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

أخيراً، نضيف فك تشفير `Metaspace`:

```python
tokenizer.decoder = decoders.Metaspace()
```

وبذلك نكون قد انتهينا من هذا المحلل الصرفي! يمكننا حفظ المحلل الصرفي كما فعلنا من قبل، وتغليفه في `PreTrainedTokenizerFast` أو `XLNetTokenizerFast` إذا أردنا استخدامه في 🤗 Transformers. شيء واحد يجب ملاحظته عند استخدام `PreTrainedTokenizerFast` هو أنه بالإضافة إلى الرموز الخاصة، نحتاج إلى إخبار مكتبة 🤗 Transformers بإضافة مسافات على اليسار:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

أو كبديل:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

الآن بعد أن رأيت كيفية استخدام كتل البناء المختلفة لبناء المحللات الصرفية الموجودة، يجب أن تكون قادراً على كتابة أي محلل صرفي تريده باستخدام مكتبة 🤗 Tokenizers والقدرة على استخدامه في 🤗 Transformers.