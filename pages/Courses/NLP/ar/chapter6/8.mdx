# ุจูุงุก ูุญูู ุฑููุฒุ ูุชูุฉ ุจูุชูุฉ [[building-a-tokenizer-block-by-block]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
]} />

ููุง ุฑุฃููุง ูู ุงูุฃูุณุงู ุงูุณุงุจูุฉุ ูุชููู ุงูุชุญููู ุงูุฑูุฒู ูู ุนุฏุฉ ุฎุทูุงุช:

- ุงูุชูุญูุฏ ุงูููุงุณู (ุฃู ุชูุธูู ุงููุต ุงูุฐู ูุนุชุจุฑ ุถุฑูุฑูุงูุ ูุซู ุฅุฒุงูุฉ ุงููุณุงูุงุช ุฃู ุนูุงูุงุช ุงูุชุดูููุ ุงูุชูุญูุฏ ุงูููุงุณู ููุฑููุฒุ ุฅูุฎ)
- ูุง ูุจู ุงูุชุญููู ุงูุฑูุฒู (ุชูุณูู ุงูุฅุฏุฎุงู ุฅูู ูููุงุช)
- ุชุดุบูู ุงูุฅุฏุฎุงู ูู ุฎูุงู ุงููููุฐุฌ (ุจุงุณุชุฎุฏุงู ุงููููุงุช ูุง ูุจู ุงูุชุญููู ุงูุฑูุฒู ูุฅูุชุงุฌ ุชุณูุณู ูู ุงูุฑููุฒ)
- ูุง ุจุนุฏ ุงููุนุงูุฌุฉ (ุฅุถุงูุฉ ุงูุฑููุฒ ุงูุฎุงุตุฉ ููุญูู ุงูุฑููุฒุ ูุชูููุฏ ููุงุน ุงูุงูุชุจุงู ููุนุฑูุงุช ููุน ุงูุฑูุฒ)

ูุฐูุฑูุ ุฅููู ูุธุฑุฉ ุฃุฎุฑู ุนูู ุงูุนูููุฉ ุงููููุฉ:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="ุฎุท ุฃูุงุจูุจ ุงูุชุญููู ุงูุฑูุฒู.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="ุฎุท ุฃูุงุจูุจ ุงูุชุญููู ุงูุฑูุฒู.">
</div>

ุชู ุจูุงุก ููุชุจุฉ ๐ค Tokenizers ูุชูููุฑ ุนุฏุฉ ุฎูุงุฑุงุช ููู ูู ุชูู ุงูุฎุทูุงุชุ ูุงูุชู ููููู ูุฒุฌูุง ููุทุงุจูุชูุง ูุนูุง. ูู ูุฐุง ุงููุณูุ ุณูุฑู ููู ูููููุง ุจูุงุก ูุญูู ุฑููุฒ ูู ุงูุตูุฑุ ุนูู ุนูุณ ุชุฏุฑูุจ ูุญูู ุฑููุฒ ุฌุฏูุฏ ูู ูุญูู ูุฏูู ููุง ูุนููุง ูู [ุงููุณู 2](/course/chapter6/2). ุจุนุฏ ุฐููุ ุณุชุชููู ูู ุจูุงุก ุฃู ููุน ูู ูุญูู ุงูุฑููุฒ ุงูุฐู ููููู ุงูุชูููุฑ ููู!

<Youtube id="MR8tZm5ViWU"/>

ูุจุดูู ุฃูุซุฑ ุฏูุฉุ ุชู ุจูุงุก ุงูููุชุจุฉ ุญูู ูุฆุฉ `Tokenizer` ูุฑูุฒูุฉ ูุน ูุชู ุงูุจูุงุก ุงููุนุงุฏ ุชุฌููุนูุง ูู ูุญุฏุงุช ูุฑุนูุฉ:

- `normalizers` ุชุญุชูู ุนูู ุฌููุน ุฃููุงุน `Normalizer` ุงูุชู ููููู ุงุณุชุฎุฏุงููุง (ุงููุงุฆูุฉ ุงููุงููุฉ [ููุง](https://huggingface.co/docs/tokenizers/api/normalizers)).
- `pre_tokenizers` ุชุญุชูู ุนูู ุฌููุน ุฃููุงุน `PreTokenizer` ุงูุชู ููููู ุงุณุชุฎุฏุงููุง (ุงููุงุฆูุฉ ุงููุงููุฉ [ููุง](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)).
- `models` ุชุญุชูู ุนูู ุงูุฃููุงุน ุงููุฎุชููุฉ ูู `Model` ุงูุชู ููููู ุงุณุชุฎุฏุงููุงุ ูุซู `BPE` ู`WordPiece` ู`Unigram` (ุงููุงุฆูุฉ ุงููุงููุฉ [ููุง](https://huggingface.co/docs/tokenizers/api/models)).
- `trainers` ุชุญุชูู ุนูู ุฌููุน ุงูุฃููุงุน ุงููุฎุชููุฉ ูู `Trainer` ุงูุชู ููููู ุงุณุชุฎุฏุงููุง ูุชุฏุฑูุจ ูููุฐุฌู ุนูู ูุฌููุนุฉ ูู ุงูุจูุงูุงุช (ูุงุญุฏ ููู ููุน ูู ุงูููุงุฐุฌุ ุงููุงุฆูุฉ ุงููุงููุฉ [ููุง](https://huggingface.co/docs/tokenizers/api/trainers)).
- `post_processors` ุชุญุชูู ุนูู ุงูุฃููุงุน ุงููุฎุชููุฉ ูู `PostProcessor` ุงูุชู ููููู ุงุณุชุฎุฏุงููุง (ุงููุงุฆูุฉ ุงููุงููุฉ [ููุง](https://huggingface.co/docs/tokenizers/api/post-processors)).
- `decoders` ุชุญุชูู ุนูู ุงูุฃููุงุน ุงููุฎุชููุฉ ูู `Decoder` ุงูุชู ููููู ุงุณุชุฎุฏุงููุง ููู ุชุดููุฑ ูุฎุฑุฌุงุช ุงูุชุญููู ุงูุฑูุฒู (ุงููุงุฆูุฉ ุงููุงููุฉ [ููุง](https://huggingface.co/docs/tokenizers/components#decoders)).

ููููู ุงูุนุซูุฑ ุนูู ุงููุงุฆูุฉ ุงููุงููุฉ ููุชู ุงูุจูุงุก [ููุง](https://huggingface.co/docs/tokenizers/components).

## ุงูุญุตูู ุนูู ูุฌููุนุฉ ูู ุงูุจูุงูุงุช [[acquiring-a-corpus]]

ูุชุฏุฑูุจ ูุญูู ุงูุฑููุฒ ุงูุฌุฏูุฏุ ุณูุณุชุฎุฏู ูุฌููุนุฉ ุตุบูุฑุฉ ูู ุงููุตูุต (ุญุชู ุชุนูู ุงูุฃูุซูุฉ ุจุณุฑุนุฉ). ุงูุฎุทูุงุช ููุญุตูู ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ููุงุซูุฉ ูุชูู ุงูุชู ุงุชุฎุฐูุงูุง ูู [ุจุฏุงูุฉ ูุฐุง ุงููุตู](/course/chapter6/2)ุ ูููู ูุฐู ุงููุฑุฉ ุณูุณุชุฎุฏู ูุฌููุนุฉ ุจูุงูุงุช [WikiText-2](https://huggingface.co/datasets/wikitext):

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

ุงูุฏุงูุฉ `get_training_corpus()` ูู ูููุฏ ุณูููู ุจุฅูุชุงุฌ ุฏูุนุงุช ูู 1000 ูุตุ ูุงูุชู ุณูุณุชุฎุฏููุง ูุชุฏุฑูุจ ูุญูู ุงูุฑููุฒ.

๐ค ูููู ุฃูุถูุง ุชุฏุฑูุจ Tokenizers ุนูู ูููุงุช ุงููุต ูุจุงุดุฑุฉ. ุฅููู ููู ูููููุง ุฅูุดุงุก ููู ูุตู ูุญุชูู ุนูู ุฌููุน ุงููุตูุต/ุงูุฅุฏุฎุงูุงุช ูู WikiText-2 ุงูุชู ูููููุง ุงุณุชุฎุฏุงููุง ูุญูููุง:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

ุจุนุฏ ุฐููุ ุณูุฑููู ููููุฉ ุจูุงุก BERT ูGPT-2 ูXLNet ูุญููุงุช ุงูุฑููุฒุ ูุชูุฉ ุจูุชูุฉ. ุณูุนุทููุง ุฐูู ูุซุงูุงู ููู ูู ุฎูุงุฑุฒููุงุช ุงูุชุญููู ุงูุฑูุฒู ุงูุซูุงุซุฉ ุงูุฑุฆูุณูุฉ: WordPiece ูBPE ูUnigram. ููุจุฏุฃ ูุน BERT!

## ุจูุงุก ูุญูู ุฑููุฒ WordPiece ูู ุงูุตูุฑ [[building-a-wordpiece-tokenizer-from-scratch]]

ูุจูุงุก ูุญูู ุฑููุฒ ูุน ููุชุจุฉ ๐ค Tokenizersุ ูุจุฏุฃ ุจุฅูุดุงุก ูุงุฆู `Tokenizer` ูุน `model`ุ ุซู ูุญุฏุฏ ุณูุงุช `normalizer` ู`pre_tokenizer` ู`post_processor` ู`decoder` ุฅูู ุงูููู ุงูุชู ูุฑูุฏูุง.

ููุฐุง ุงููุซุงูุ ุณููุดุฆ `Tokenizer` ูุน ูููุฐุฌ WordPiece:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

ูุฌุจ ุนูููุง ุชุญุฏูุฏ `unk_token` ุญุชู ูุนุฑู ุงููููุฐุฌ ูุง ุงูุฐู ูุฌุจ ุฅุนุงุฏุชู ุนูุฏูุง ููุงุฌู ุฃุญุฑููุง ูู ูุฑูุง ูู ูุจู. ุชุดูู ุงูุญุฌุฌ ุงูุฃุฎุฑู ุงูุชู ูููููุง ุชุนููููุง ููุง `vocab` ููููุฐุฌูุง (ุณูููู ุจุชุฏุฑูุจ ุงููููุฐุฌุ ูุฐูู ูุง ูุญุชุงุฌ ุฅูู ุชุนููู ูุฐุง) ู`max_input_chars_per_word`ุ ุงูุฐู ูุญุฏุฏ ุทูููุง ุฃูุตู ููู ูููุฉ (ุณูุชู ุชูุณูู ุงููููุงุช ุงูุฃุทูู ูู ุงููููุฉ ุงูุชู ุชู ุชูุฑูุฑูุง).

ุงูุฎุทูุฉ ุงูุฃููู ูู ุงูุชุญููู ุงูุฑูุฒู ูู ุงูุชูุญูุฏ ุงูููุงุณูุ ูุฐูู ุฏุนูุง ูุจุฏุฃ ุจุฐูู. ูุธุฑูุง ูุฃู BERT ูุณุชุฎุฏู ุนูู ูุทุงู ูุงุณุนุ ููุงู `BertNormalizer` ูุน ุงูุฎูุงุฑุงุช ุงูููุงุณูููุฉ ุงูุชู ูููููุง ุชุนููููุง ูู BERT: `lowercase` ู`strip_accents`ุ ูุงูุชู ุชุดุฑุญ ููุณูุง ุจููุณูุงุ `clean_text` ูุฅุฒุงูุฉ ุฌููุน ุฃุญุฑู ุงูุชุญูู ูุงุณุชุจุฏุงู ุงููุณุงูุงุช ุงููุชูุฑุฑุฉ ุจูุณุงูุฉ ูุงุญุฏุฉุ ู`handle_chinese_chars`ุ ุงูุฐู ูุถุน ูุณุงูุงุช ุญูู ุงูุฃุญุฑู ุงูุตูููุฉ. ูุชูุฑุงุฑ ูุญูู ุงูุฑููุฒ `bert-base-uncased`ุ ูููููุง ููุท ุชุนููู ูุฐุง ุงููุญูู:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

ููุน ุฐููุ ุจุดูู ุนุงูุ ุนูุฏ ุจูุงุก ูุญูู ุฑููุฒ ุฌุฏูุฏุ ูู ูููู ูุฏูู ุญู ุงููุตูู ุฅูู ูุซู ูุฐุง ุงููุญูู ุงูุฑููุฒ ุงููููุฏ ุงูุฐู ุชู ุชูููุฐู ุจุงููุนู ูู ููุชุจุฉ ๐ค Tokenizers - ูุฐูู ุฏุนูุง ูุฑู ููููุฉ ุฅูุดุงุก ูุญูู ุงูุฑููุฒ BERT ูุฏูููุง. ุชููุฑ ุงูููุชุจุฉ `Lowercase` ูุญูู ุฑููุฒ ู`StripAccents` ูุญูู ุฑููุฒุ ูููููู ุชูููู ุนุฏุฉ ูุญููุงุช ุฑููุฒ ุจุงุณุชุฎุฏุงู `Sequence`:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

ูุณุชุฎุฏู ุฃูุถูุง `NFD` ูุญูู ุฑููุฒ ููุฑููุฒ ุงูููุญุฏุฉุ ูุฅูุง ูุฅู `StripAccents` ูุญูู ุงูุฑููุฒ ูู ูุชุนุฑู ุจุดูู ุตุญูุญ ุนูู ุงูุฃุญุฑู ุงููุดูููุฉ ููู ูุฒูููุง.

ููุง ุฑุฃููุง ูู ูุจูุ ูููููุง ุงุณุชุฎุฏุงู ุทุฑููุฉ `normalize_str()` ูู `normalizer` ููุชุญูู ูู ุงูุชุฃุซูุฑ ุงูุฐู ูุญุฏุซู ุนูู ูุต ูุนูู:

```python
print(tokenizer.normalizer.normalize_str("Hรฉllรฒ hรดw are รผ?"))
```

```python out
hello how are u?
```

<Tip>

**ูููุฒูุฏ** ุฅุฐุง ููุช ุจุงุฎุชุจุงุฑ ุงูุฅุตุฏุงุฑูู ุงูุณุงุจููู ูู ูุญููุงุช ุงูุฑููุฒ ูุฐู ุนูู ุณูุณูุฉ ุชุญุชูู ุนูู ุญุฑู ุงูุฑููุฒ ุงูููุญุฏุฉ `u"\u0085"`ุ ูุณุชูุงุญุธ ุจุงูุชุฃููุฏ ุฃู ูุฐูู ุงููุญูููู ููุณุง ูุชุทุงุจููู ุชูุงููุง.
ูุชุฌูุจ ุชุนููุฏ ุงูุฅุตุฏุงุฑ ูุน `normalizers.Sequence` ูุซูุฑูุงุ ูู ูุฏุฑุฌ ุงูุงุณุชุจุฏุงูุงุช ุงูุนุงุฏูุฉ ุงูุชู ุชุชุทูุจูุง `BertNormalizer` ุนูุฏ ุชุนููู ุญุฌุฉ `clean_text` ุฅูู `True` - ููู ุงูุณููู ุงูุงูุชุฑุงุถู. ูููู ูุง ุชููู: ูู ุงููููู ุงูุญุตูู ุนูู ููุณ ุงูุชูุญูุฏ ุงูููุงุณู ุชูุงููุง ุฏูู ุงุณุชุฎุฏุงู `BertNormalizer` ุงููููุฏ ุนู ุทุฑูู ุฅุถุงูุฉ `normalizers.Replace` ุฅูู ุชุณูุณู ุงููุญููุงุช.

</Tip>

ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุฎุทูุฉ ูุง ูุจู ุงูุชุญููู ุงูุฑูุฒู. ูุฑุฉ ุฃุฎุฑูุ ููุงู `BertPreTokenizer` ูุณุจู ุงูุจูุงุก ูููููุง ุงุณุชุฎุฏุงูู:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

ุฃู ูููููุง ุจูุงุคู ูู ุงูุตูุฑ:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

ูุงุญุธ ุฃู `Whitespace` ูุญูู ุงูุฑููุฒ ุงููุณุจู ููุณู ุนูู ุงููุณุงูุงุช ูุนูู ุฌููุน ุงูุฃุญุฑู ุงูุชู ููุณุช ุฃุญุฑููุง ุฃู ุฃุฑูุงููุง ุฃู ุญุฑู ุงูุชุณุทูุฑุ ูุฐูู ููู ููุณู ุชููููุง ุนูู ุงููุณุงูุงุช ูุนูุงูุงุช ุงูุชุฑููู:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```
ุฅุฐุง ููุช ุชุฑูุฏ ููุท ุงูุชูุณูู ุนูู ุงููุณุงูุงุช ุงูุจูุถุงุกุ ููุฌุจ ุนููู ุงุณุชุฎุฏุงู `WhitespaceSplit` ููุนุงูุฌ ูุณุจู:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

ูุซู ุงููุนุงูุฌุงุชุ ููููู ุงุณุชุฎุฏุงู `Sequence` ูุชุฑููุจ ุนุฏุฉ ูุนุงูุฌุงุช ูุณุจูุฉ:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุฎุท ุฃูุงุจูุจ ุงูุชุฌุฒุฆุฉ ูู ุชุดุบูู ุงููุฏุฎูุงุช ุนุจุฑ ุงููููุฐุฌ. ููุฏ ุญุฏุฏูุง ูููุฐุฌูุง ุจุงููุนู ูู ุงูุชููุฆุฉุ ููููุง ูุง ุฒููุง ุจุญุงุฌุฉ ุฅูู ุชุฏุฑูุจูุ ููุง ุณูุชุทูุจ `WordPieceTrainer`. ุงูุดูุก ุงูุฑุฆูุณู ุงูุฐู ูุฌุจ ุชุฐูุฑู ุนูุฏ ุฅูุดุงุก ูุฏุฑุจ ูู ๐ค Tokenizers ูู ุฃูู ูุฌุจ ุนููู ุชูุฑูุฑ ุฌููุน ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ุชููู ุงุณุชุฎุฏุงููุง - ูุฅูุง ููู ูููู ุจุฅุถุงูุชูุง ุฅูู ุงูููุฑุฏุงุชุ ุญูุซ ุฃููุง ููุณุช ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

ุจุงูุฅุถุงูุฉ ุฅูู ุชุญุฏูุฏ `vocab_size` ู `special_tokens`ุ ูููููุง ุถุจุท `min_frequency` (ุนุฏุฏ ุงููุฑุงุช ุงูุชู ูุฌุจ ุฃู ูุธูุฑ ูููุง ุงูุฑูุฒ ููุชู ุชุถูููู ูู ุงูููุฑุฏุงุช) ุฃู ุชุบููุฑ `continuing_subword_prefix` (ุฅุฐุง ููุง ูุฑูุฏ ุงุณุชุฎุฏุงู ุดูุก ูุฎุชูู ุนู `##`).

ูุชุฏุฑูุจ ูููุฐุฌูุง ุจุงุณุชุฎุฏุงู ุงููุนุงูุฌ ุงูุฐู ุญุฏุฏูุงู ุณุงุจููุงุ ูู ูุง ุนูููุง ูุนูู ูู ุชูููุฐ ูุฐุง ุงูุฃูุฑ:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ูููููุง ุฃูุถูุง ุงุณุชุฎุฏุงู ูููุงุช ุงููุต ูุชุฏุฑูุจ ูุนุงูุฌูุงุ ูุงูุฐู ุณูููู ุนูู ุงููุญู ุงูุชุงูู (ูุนูุฏ ุชููุฆุฉ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู `WordPiece` ูุงุฑุบ ูุณุจููุง):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

ูู ููุชุง ุงูุญุงูุชููุ ูููููุง ุจุนุฏ ุฐูู ุงุฎุชุจุงุฑ ุงููุนุงูุฌ ุนูู ูุต ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `encode()`:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

ุฅู `encoding` ุงูุฐู ุชู ุงูุญุตูู ุนููู ูู `Encoding`ุ ูุงูุฐู ูุญุชูู ุนูู ุฌููุน ุงููุฎุฑุฌุงุช ุงูุถุฑูุฑูุฉ ูููุนุงูุฌ ูู ุณูุงุชู ุงููุฎุชููุฉ: `ids`ุ `type_ids`ุ `tokens`ุ `offsets`ุ `attention_mask`ุ `special_tokens_mask`ุ ู `overflowing`.

ุงูุฎุทูุฉ ุงูุฃุฎูุฑุฉ ูู ุฎุท ุฃูุงุจูุจ ุงูุชุฌุฒุฆุฉ ูู ูุง ุจุนุฏ ุงููุนุงูุฌุฉ. ูุญู ุจุญุงุฌุฉ ุฅูู ุฅุถุงูุฉ ุงูุฑูุฒ `[CLS]` ูู ุงูุจุฏุงูุฉ ูุงูุฑููุฒ `[SEP]` ูู ุงูููุงูุฉ (ุฃู ุจุนุฏ ูู ุฌููุฉุ ุฅุฐุง ูุงู ูุฏููุง ุฒูุฌ ูู ุงูุฌูู). ุณูุณุชุฎุฏู `TemplateProcessor` ููุฐุงุ ูููู ุฃููุงู ูุญุชุงุฌ ุฅูู ูุนุฑูุฉ ูุนุฑูุงุช ุงูุฑููุฒ `[CLS]` ู `[SEP]` ูู ุงูููุฑุฏุงุช:

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

ููุชุงุจุฉ ุงููุงูุจ ูู `TemplateProcessor`ุ ูุฌุจ ุนูููุง ุชุญุฏูุฏ ููููุฉ ูุนุงูุฌุฉ ุฌููุฉ ูุงุญุฏุฉ ูุฒูุฌ ูู ุงูุฌูู. ุจุงููุณุจุฉ ููููููุงุ ููุชุจ ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุฑูุฏ ุงุณุชุฎุฏุงููุงุ ุงูุฌููุฉ ุงูุฃููู (ุฃู ุงููุญูุฏุฉ) ููุซูุฉ ุจู `$A`ุ ุจูููุง ุงูุฌููุฉ ุงูุซุงููุฉ (ุฅุฐุง ูุงูุช ุงูุชุฑููุฒ ูุฒูุฌ) ููุซูุฉ ุจู `$B`. ุจุงููุณุจุฉ ููู ูู ูุฐู (ุงูุฑููุฒ ุงูุฎุงุตุฉ ูุงูุฌูู)ุ ูููู ุฃูุถูุง ุจุชุญุฏูุฏ ูุนุฑู ููุน ุงูุฑูุฒ ุงูููุงุจู ุจุนุฏ ุนูุงูุฉ ุงูููุทุชูู.

ูุฐูู ูุชู ุชุนุฑูู ุงููุงูุจ ุงูููุงุณููู ูู BERT ุนูู ุงููุญู ุงูุชุงูู:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

ูุงุญุธ ุฃูู ูุฌุจ ุนูููุง ุชูุฑูุฑ ูุนุฑูุงุช ุงูุฑููุฒ ุงูุฎุงุตุฉุ ุจุญูุซ ูููู ูููุนุงูุฌ ุชุญููููุง ุฅูู ูุนุฑูุงุชูุง ุจุดูู ุตุญูุญ.

ุจูุฌุฑุฏ ุฅุถุงูุฉ ูุฐุงุ ุนูุฏ ุงูุนูุฏุฉ ุฅูู ูุซุงููุง ุงูุณุงุจูุ ุณูุญุตู ุนูู:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

ูุนูู ุฒูุฌ ูู ุงูุฌููุ ูุญุตู ุนูู ุงููุชูุฌุฉ ุงูุตุญูุญุฉ:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

ููุฏ ุงูุชูููุง ุชูุฑูุจูุง ูู ุจูุงุก ูุฐุง ุงููุนุงูุฌ ูู ุงูุตูุฑ - ุงูุฎุทูุฉ ุงูุฃุฎูุฑุฉ ูู ุชุถููู ูู ุงูุชุดููุฑ:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

ุฏุนูุง ูุฎุชุจุฑูุง ุนูู `encoding` ุงูุณุงุจู:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

ุฑุงุฆุน! ูููููุง ุญูุธ ูุนุงูุฌูุง ูู ููู JSON ูุงุญุฏ ุนูู ุงููุญู ุงูุชุงูู:

```python
tokenizer.save("tokenizer.json")
```

ูููููุง ุจุนุฏ ุฐูู ุฅุนุงุฏุฉ ุชุญููู ูุฐุง ุงูููู ูู ูุงุฆู `Tokenizer` ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `from_file()`:

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

ูุงุณุชุฎุฏุงู ูุฐุง ุงููุนุงูุฌ ูู ๐ค Transformersุ ูุฌุจ ุนูููุง ุชุบูููู ูู `PreTrainedTokenizerFast`. ูููููุง ุฅูุง ุงุณุชุฎุฏุงู ุงููุฆุฉ ุงูุนุงูุฉ ุฃูุ ุฅุฐุง ูุงู ูุนุงูุฌูุง ูุชูุงูู ูุน ูููุฐุฌ ููุฌูุฏุ ุงุณุชุฎุฏุงู ุชูู ุงููุฆุฉ (ููุงุ `BertTokenizerFast`). ุฅุฐุง ููุช ุชุทุจู ูุฐุง ุงูุฏุฑุณ ูุจูุงุก ูุนุงูุฌ ุฌุฏูุฏ ุชูุงููุงุ ูุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ุงูุฎูุงุฑ ุงูุฃูู.

ูุชุบููู ุงููุนุงูุฌ ูู `PreTrainedTokenizerFast`ุ ูููููุง ุฅูุง ุชูุฑูุฑ ุงููุนุงูุฌ ุงูุฐู ูููุง ุจุจูุงุฆู ูู `tokenizer_object` ุฃู ุชูุฑูุฑ ููู ุงููุนุงูุฌ ุงูุฐู ูููุง ุจุญูุธู ูู `tokenizer_file`. ุงูุดูุก ุงูุฑุฆูุณู ุงูุฐู ูุฌุจ ุชุฐูุฑู ูู ุฃูู ูุฌุจ ุนูููุง ุชุนููู ุฌููุน ุงูุฑููุฒ ุงูุฎุงุตุฉ ูุฏูููุงุ ุญูุซ ูุง ูููู ููุฐู ุงููุฆุฉ ุงูุงุณุชูุชุงุฌ ูู ูุงุฆู `tokenizer` ุฃู ุฑูุฒ ูู ุฑูุฒ ุงูููุงุนุ ุงูุฑูุฒ `[CLS]`ุ ุฅูุฎ:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # ููููู ุงูุชุญููู ูู ููู ุงููุนุงูุฌุ ูุจุฏูู
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

ุฅุฐุง ููุช ุชุณุชุฎุฏู ูุฆุฉ ูุนุงูุฌ ูุญุฏุฏุฉ (ูุซู `BertTokenizerFast`)ุ ูุณุชุญุชุงุฌ ููุท ุฅูู ุชุญุฏูุฏ ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงููุฎุชููุฉ ุนู ุงูุฑููุฒ ุงูุงูุชุฑุงุถูุฉ (ููุงุ ูุง ุดูุก):

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

ููููู ุจุนุฏ ุฐูู ุงุณุชุฎุฏุงู ูุฐุง ุงููุนุงูุฌ ูุซู ุฃู ูุนุงูุฌ ุขุฎุฑ ูู ๐ค Transformers. ููููู ุญูุธู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `save_pretrained()`ุ ุฃู ุชุญูููู ุฅูู Hub ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `push_to_hub()`.

ุงูุขู ุจุนุฏ ุฃู ุฑุฃููุง ููููุฉ ุจูุงุก ูุนุงูุฌ WordPieceุ ุฏุนูุง ููุนู ุงูุดูุก ููุณู ููุนุงูุฌ BPE. ุณูุฐูุจ ุจุณุฑุนุฉ ุฃูุจุฑ ูุฃูู ุชุนุฑู ุฌููุน ุงูุฎุทูุงุชุ ูุณูููู ุงูุถูุก ููุท ุนูู ุงูุงุฎุชูุงูุงุช.

## ุจูุงุก ูุนุงูุฌ BPE ูู ุงูุตูุฑ[[building-a-bpe-tokenizer-from-scratch]]

ุฏุนูุง ุงูุขู ูุจูู ูุนุงูุฌ GPT-2. ูุซู ูุนุงูุฌ BERTุ ูุจุฏุฃ ุจุชููุฆุฉ `Tokenizer` ูุน ูููุฐุฌ BPE:

```python
tokenizer = Tokenizer(models.BPE())
```

ุฃูุถูุง ูุซู BERTุ ูููููุง ุชููุฆุฉ ูุฐุง ุงููููุฐุฌ ุจููุฑุฏุงุช ุฅุฐุง ูุงู ูุฏููุง ูุงุญุฏุฉ (ุณูุญุชุงุฌ ุฅูู ุชูุฑูุฑ `vocab` ู `merges` ูู ูุฐู ุงูุญุงูุฉ)ุ ูููู ุจูุง ุฃููุง ุณูุชุฏุฑุจ ูู ุงูุตูุฑุ ููุง ูุญุชุงุฌ ุฅูู ุงูููุงู ุจุฐูู. ูุญู ุฃูุถูุง ูุง ูุญุชุงุฌ ุฅูู ุชุญุฏูุฏ `unk_token` ูุฃู GPT-2 ูุณุชุฎุฏู BPE ุนูู ูุณุชูู ุงูุจุงูุชุ ูุงูุฐู ูุง ูุชุทูุจ ุฐูู.

ูุง ูุณุชุฎุฏู GPT-2 ูุนุงูุฌูุง ุทุจูุนููุงุ ูุฐูู ูุชุฎุทู ุชูู ุงูุฎุทูุฉ ููุฐูุจ ูุจุงุดุฑุฉ ุฅูู ูุง ูุจู ุงูุชุฌุฒุฆุฉ:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

ุงูุฎูุงุฑ ุงูุฐู ุฃุถููุงู ุฅูู `ByteLevel` ููุง ูู ุนุฏู ุฅุถุงูุฉ ูุณุงูุฉ ูู ุจุฏุงูุฉ ุงูุฌููุฉ (ููู ุงูุงูุชุฑุงุถู ูู ุงูุญุงูุงุช ุงูุฃุฎุฑู). ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ูุง ูุจู ุชุฌุฒุฆุฉ ูุต ูุซุงู ูุซู ุงูุณุงุจู:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('ฤtest', (5, 10)), ('ฤpre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```
ุจุนุฏ ุฐูู ูุฃุชู ุงููููุฐุฌุ ูุงูุฐู ูุญุชุงุฌ ุฅูู ุชุฏุฑูุจ. ุจุงููุณุจุฉ ูู GPT-2ุ ูุฅู ุงูุฑูุฒ ุงูุฎุงุต ุงููุญูุฏ ูู ุฑูุฒ ููุงูุฉ ุงููุต:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ููุง ูู ุงูุญุงู ูุน `WordPieceTrainer`ุ ุจุงูุฅุถุงูุฉ ุฅูู `vocab_size` ู `special_tokens`ุ ูููููุง ุชุญุฏูุฏ `min_frequency` ุฅุฐุง ุฃุฑุฏูุง ุฐููุ ุฃู ุฅุฐุง ูุงู ูุฏููุง ูุงุญูุฉ ููุงูุฉ ุงููููุฉ (ูุซู `</w>`)ุ ูููููุง ุชุนููููุง ุจุงุณุชุฎุฏุงู `end_of_word_suffix`.

ูููู ุชุฏุฑูุจ ูุฐุง ุงููุญูู ุงูุตุฑูู ุฃูุถุงู ุนูู ูููุงุช ุงููุต:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุชุญููู ูุต ุนููุฉ:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'ฤtest', 'ฤthis', 'ฤto', 'ken', 'izer', '.']
```

ูุทุจู ูุนุงูุฌุฉ ูุง ุจุนุฏ ูุณุชูู ุงูุจุงูุช ููุญูู GPT-2 ุงูุตุฑูู ููุง ููู:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

ูุดูุฑ ุฎูุงุฑ `trim_offsets = False` ุฅูู ุงููุนุงูุฌ ุงููุงุญู ุจุฃููุง ูุฌุจ ุฃู ูุชุฑู ุฅุฒุงุญุงุช ุงูุฑููุฒ ุงูุชู ุชุจุฏุฃ ุจู 'ฤ' ููุง ูู: ุจูุฐู ุงูุทุฑููุฉ ุณุชุดูุฑ ุจุฏุงูุฉ ุงูุฅุฒุงุญุงุช ุฅูู ุงููุณุงูุฉ ูุจู ุงููููุฉุ ูููุณ ุฃูู ุญุฑู ูู ุงููููุฉ (ุญูุซ ุฃู ุงููุณุงูุฉ ูู ุฌุฒุก ูู ุงูุฑูุฒ ูู ุงููุงุญูุฉ ุงููููุฉ). ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุงููุชูุฌุฉ ูุน ุงููุต ุงูุฐู ูููุง ุจุชุดููุฑู ููุชูุ ุญูุซ `'ฤtest'` ูู ุงูุฑูุฒ ูู ุงูููุฑุณ 4:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

ุฃุฎูุฑุงูุ ูุถูู ูู ุชุดููุฑ ูุณุชูู ุงูุจุงูุช:

```python
tokenizer.decoder = decoders.ByteLevel()
```

ููููููุง ุงูุชุฃูุฏ ูู ุฃูู ูุนูู ุจุดูู ุตุญูุญ:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

ุฑุงุฆุน! ุงูุขู ุจุนุฏ ุฃู ุงูุชูููุงุ ูููููุง ุญูุธ ุงููุญูู ุงูุตุฑูู ููุง ูุนููุง ูู ูุจูุ ูุชุบูููู ูู `PreTrainedTokenizerFast` ุฃู `GPT2TokenizerFast` ุฅุฐุง ุฃุฑุฏูุง ุงุณุชุฎุฏุงูู ูู ๐ค Transformers:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

ุฃู:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

ููุซุงู ุฃุฎูุฑุ ุณูุฑููู ููููุฉ ุจูุงุก ูุญูู ุตุฑูู ุฃุญุงุฏู ูู ุงูุตูุฑ.

## ุจูุงุก ูุญูู ุตุฑูู ุฃุญุงุฏู ูู ุงูุตูุฑ [[building-a-unigram-tokenizer-from-scratch]]

ุฏุนูุง ุงูุขู ูุจูู ูุญูู XLNet ุงูุตุฑูู. ููุง ูู ุงูุญุงู ูู ุงููุญููุงุช ุงูุตุฑููุฉ ุงูุณุงุจูุฉุ ูุจุฏุฃ ุจุชููุฆุฉ `Tokenizer` ูุน ูููุฐุฌ ุฃุญุงุฏู:

```python
tokenizer = Tokenizer(models.Unigram())
```

ูุฑุฉ ุฃุฎุฑูุ ูููููุง ุชููุฆุฉ ูุฐุง ุงููููุฐุฌ ุจููุฑุฏุงุช ุฅุฐุง ูุงู ูุฏููุง ูุงุญุฏุฉ.

ุจุงููุณุจุฉ ููุชูุญูุฏ ุงูููุงุณูุ ูุณุชุฎุฏู XLNet ุจุนุถ ุงูุงุณุชุจุฏุงูุงุช (ุงูุชู ุชุฃุชู ูู SentencePiece):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

ูุณุชุจุฏู ูุฐุง ุงูุฑูุฒ <code>``</code> ู <code>''</code> ุจู <code>"</code> ูุฃู ุชุณูุณู ูู ูุณุงูุชูู ุฃู ุฃูุซุฑ ุจูุณุงูุฉ ูุงุญุฏุฉุ ุจุงูุฅุถุงูุฉ ุฅูู ุฅุฒุงูุฉ ุงูุชุดููู ูู ุงููุตูุต ุงูุชู ุณูุชู ุชุญููููุง ุตุฑููุงู.

ุงููุญูู ุงูุตุฑูู ุงููุณุจู ููุงุณุชุฎุฏุงู ูุฃู ูุญูู ุตุฑูู SentencePiece ูู `Metaspace`:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ุงูุชุญููู ุงูุตุฑูู ุงููุณุจู ููุต ูุซุงู ููุง ูุนููุง ูู ูุจู:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("โLet's", (0, 5)), ('โtest', (5, 10)), ('โthe', (10, 14)), ('โpre-tokenizer!', (14, 29))]
```

ุจุนุฏ ุฐูู ูุฃุชู ุงููููุฐุฌุ ูุงูุฐู ูุญุชุงุฌ ุฅูู ุชุฏุฑูุจ. ูุฏู XLNet ุงูุนุฏูุฏ ูู ุงูุฑููุฒ ุงูุฎุงุตุฉ:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ุญุฌุฉ ูููุฉ ุฌุฏุงู ูุฌุจ ุนุฏู ูุณูุงููุง ููู `UnigramTrainer` ูู `unk_token`. ูููููุง ุฃูุถุงู ุชูุฑูุฑ ุญุฌุฌ ุฃุฎุฑู ุฎุงุตุฉ ุจุฎูุงุฑุฒููุฉ Unigramุ ูุซู `shrinking_factor` ููู ุฎุทูุฉ ูุฒูู ูููุง ุงูุฑููุฒ (ุงููููุฉ ุงูุงูุชุฑุงุถูุฉ ูู 0.75) ุฃู `max_piece_length` ูุชุญุฏูุฏ ุงูุทูู ุงูุฃูุตู ูุฑููุฒ ูุนููุฉ (ุงููููุฉ ุงูุงูุชุฑุงุถูุฉ ูู 16).

ูููู ุชุฏุฑูุจ ูุฐุง ุงููุญูู ุงูุตุฑูู ุฃูุถุงู ุนูู ูููุงุช ุงููุต:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุชุญููู ูุต ุนููุฉ:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['โLet', "'", 's', 'โtest', 'โthis', 'โto', 'ken', 'izer', '.']
```

ูู ุฎุตุงุฆุต XLNet ุฃูู ูุถุน ุงูุฑูุฒ `<cls>` ูู ููุงูุฉ ุงูุฌููุฉุ ูุน ูุนุฑู ููุน 2 (ูุชูููุฒู ุนู ุงูุฑููุฒ ุงูุฃุฎุฑู). ุฅูู ูุถูู ูุณุงูุงุช ุนูู ุงููุณุงุฑุ ูุชูุฌุฉ ูุฐูู. ูููููุง ุงูุชุนุงูู ูุน ุฌููุน ุงูุฑููุฒ ุงูุฎุงุตุฉ ููุนุฑูุงุช ููุน ุงูุฑููุฒ ุจุงุณุชุฎุฏุงู ูุงูุจุ ููุง ูู ุงูุญุงู ูู BERTุ ูููู ุฃููุงู ูุฌุจ ุนูููุง ุงูุญุตูู ุนูู ูุนุฑูุงุช ุงูุฑููุฒ `<cls>` ู `<sep>`:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

ูุจุฏู ุงููุงูุจ ูุงูุชุงูู:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

ููููููุง ุงุฎุชุจุงุฑ ุนููู ุนู ุทุฑูู ุชุดููุฑ ุฒูุฌ ูู ุงูุฌูู:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['โLet', "'", 's', 'โtest', 'โthis', 'โto', 'ken', 'izer', '.', '.', '.', '<sep>', 'โ', 'on', 'โ', 'a', 'โpair', 
  'โof', 'โsentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

ุฃุฎูุฑุงูุ ูุถูู ูู ุชุดููุฑ `Metaspace`:

```python
tokenizer.decoder = decoders.Metaspace()
```

ูุจุฐูู ูููู ูุฏ ุงูุชูููุง ูู ูุฐุง ุงููุญูู ุงูุตุฑูู! ูููููุง ุญูุธ ุงููุญูู ุงูุตุฑูู ููุง ูุนููุง ูู ูุจูุ ูุชุบูููู ูู `PreTrainedTokenizerFast` ุฃู `XLNetTokenizerFast` ุฅุฐุง ุฃุฑุฏูุง ุงุณุชุฎุฏุงูู ูู ๐ค Transformers. ุดูุก ูุงุญุฏ ูุฌุจ ููุงุญุธุชู ุนูุฏ ุงุณุชุฎุฏุงู `PreTrainedTokenizerFast` ูู ุฃูู ุจุงูุฅุถุงูุฉ ุฅูู ุงูุฑููุฒ ุงูุฎุงุตุฉุ ูุญุชุงุฌ ุฅูู ุฅุฎุจุงุฑ ููุชุจุฉ ๐ค Transformers ุจุฅุถุงูุฉ ูุณุงูุงุช ุนูู ุงููุณุงุฑ:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

ุฃู ูุจุฏูู:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

ุงูุขู ุจุนุฏ ุฃู ุฑุฃูุช ููููุฉ ุงุณุชุฎุฏุงู ูุชู ุงูุจูุงุก ุงููุฎุชููุฉ ูุจูุงุก ุงููุญููุงุช ุงูุตุฑููุฉ ุงูููุฌูุฏุฉุ ูุฌุจ ุฃู ุชููู ูุงุฏุฑุงู ุนูู ูุชุงุจุฉ ุฃู ูุญูู ุตุฑูู ุชุฑูุฏู ุจุงุณุชุฎุฏุงู ููุชุจุฉ ๐ค Tokenizers ูุงููุฏุฑุฉ ุนูู ุงุณุชุฎุฏุงูู ูู ๐ค Transformers.