# التطبيع والتمهيد للتجزيء {}[[normalization-and-pre-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
]} />

قبل أن نتعمق أكثر في خوارزميات التجزيء الثلاثة الأكثر شيوعًا المستخدمة مع نماذج المحول (الترميز ثنائي البايت [BPE]، وWordPiece، وUnigram)، سنلقي أولاً نظرة على ما قبل المعالجة التي يطبقها كل مجزئ على النص. فيما يلي نظرة عامة على خطوات خط أنابيب التجزيء:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="خط أنابيب التجزيء.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="خط أنابيب التجزيء.">
</div>

قبل تقسيم النص إلى رموز فرعية (وفقًا لنموذجه)، يقوم المجزئ بخطوتين: _التطبيع_ و_التمهيد للتجزيء_.

## التطبيع {}[[normalization]]

<Youtube id="4IIC2jI9CaU"/>

تتضمن خطوة التطبيع بعض التنظيف العام، مثل إزالة المسافات غير الضرورية، والتحويل إلى أحرف صغيرة، و/أو إزالة التشكيل. إذا كنت على دراية بـ [تطبيع Unicode](http://www.unicode.org/reports/tr15/) (مثل NFC أو NFKC)، فهذا أيضًا شيء قد يطبقه المجزئ.

لدى 🤗 Transformers `tokenizer` سمة تسمى `backend_tokenizer` التي توفر الوصول إلى المجزئ الأساسي من مكتبة 🤗 Tokenizers:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

لدى سمة `normalizer` للكائن `tokenizer` طريقة `normalize_str()` التي يمكننا استخدامها لمعرفة كيفية إجراء التطبيع:

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
```

```python out
'hello how are u?'
```

في هذا المثال، نظرًا لأننا اخترنا نقطة التحقق `bert-base-uncased`، فقد طبق التطبيع التحويل إلى أحرف صغيرة وأزال التشكيل.

<Tip>

✏️ **جربه!** قم بتحميل مجزئ من نقطة التحقق `bert-base-cased` ومرر نفس المثال إليه. ما هي الاختلافات الرئيسية التي يمكنك ملاحظتها بين الإصدارات المطبقة وغير المطبقة من المجزئ؟

</Tip>

## التمهيد للتجزيء {}[[pre-tokenization]]

<Youtube id="grlLV8AIXug"/>

كما سنرى في الأقسام التالية، لا يمكن تدريب المجزئ على النص الخام وحده. بدلاً من ذلك، نحتاج أولاً إلى تقسيم النصوص إلى كيانات صغيرة، مثل الكلمات. وهنا يأتي دور خطوة التمهيد للتجزيء. كما رأينا في [الفصل 2](/course/chapter2)، يمكن لمجزئ قائم على الكلمات ببساطة تقسيم نص خام إلى كلمات على المسافات والفواصل. ستكون تلك الكلمات هي حدود الرموز الفرعية التي يمكن للمجزئ تعلمها أثناء تدريبه.

لرؤية كيف يقوم مجزئ سريع بأداء التمهيد للتجزيء، يمكننا استخدام طريقة `pre_tokenize_str()` لسمة `pre_tokenizer` للكائن `tokenizer`:

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

لاحظ كيف يحتفظ المجزئ بالفعل بتتبع الإزاحات، وهو ما يمكنه من تزويدنا بخريطة الإزاحة التي استخدمناها في القسم السابق. هنا يتجاهل المجزئ المسافتين ويستبدلهما بواحدة فقط، ولكن الإزاحة تقفز بين `are` و`you` لمراعاة ذلك.

نظرًا لأننا نستخدم مجزئ BERT، فإن التمهيد للتجزيء ينطوي على التقسيم على المسافات والفواصل. يمكن لمجزئات أخرى أن يكون لديها قواعد مختلفة لهذه الخطوة. على سبيل المثال، إذا استخدمنا مجزئ GPT-2:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

سيقسم على المسافات والفواصل أيضًا، ولكنه سيحتفظ بالمسافات ويستبدلها برمز `Ġ`، مما يمكنه من استعادة المسافات الأصلية إذا قمنا بفك رموز الرموز:

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)),
 ('?', (19, 20))]
```

لاحظ أيضًا أن هذا المجزئ، على عكس مجزئ BERT، لا يتجاهل المسافتين المزدوجتين.

كمثال أخير، دعنا نلقي نظرة على مجزئ T5، والذي يعتمد على خوارزمية SentencePiece:

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]
```

مثل مجزئ GPT-2، يحتفظ هذا المجزئ بالمسافات ويستبدلها برمز محدد (`_`)، ولكن مجزئ T5 يقسم فقط على المسافات، وليس الفواصل. لاحظ أيضًا أنه أضاف مسافة افتراضية في بداية الجملة (قبل `Hello`) وتجاهل المسافة المزدوجة بين `are` و`you`.

الآن بعد أن رأينا كيف تقوم بعض المجزئيات المختلفة بمعالجة النص، يمكننا البدء في استكشاف الخوارزميات الأساسية نفسها. سنبدأ بنظرة سريعة على خوارزمية SentencePiece واسعة التطبيق؛ ثم، على مدار الأقسام الثلاثة التالية، سنفحص كيفية عمل الخوارزميات الثلاث الرئيسية المستخدمة للتجزيء الفرعي للكلمات.

## SentencePiece {}[[sentencepiece]]

[SentencePiece](https://github.com/google/sentencepiece) هي خوارزمية تجزيء لمعالجة النص قبل المعالجة التي يمكنك استخدامها مع أي من النماذج التي سنراها في الأقسام الثلاثة التالية. تعتبر النص كتسلسل من أحرف Unicode، وتستبدل المسافات برمز خاص، `▁`. عند استخدامه بالاقتران مع خوارزمية Unigram (انظر [القسم 7](/course/chapter7/7))، فإنه لا يتطلب حتى خطوة التمهيد للتجزيء، وهو أمر مفيد للغاية للغات التي لا تستخدم حرف المسافة (مثل الصينية أو اليابانية).

تتمثل الميزة الرئيسية الأخرى لـ SentencePiece في *التجزيء القابل للعكس*: نظرًا لعدم وجود معاملة خاصة للمسافات، يتم فك رموز الرموز ببساطة عن طريق دمجها واستبدال الرموز `_` بالمسافات - وهذا يؤدي إلى النص المطبق. كما رأينا سابقًا، يزيل مجزئ BERT المسافات المتكررة، لذا فإن تجزيئه غير قابل للعكس.

## نظرة عامة على الخوارزمية {}[[algorithm-overview]]

في الأقسام التالية، سنغوص في خوارزميات التجزيء الفرعي للكلمات الثلاثة الرئيسية: BPE (الذي يستخدمه GPT-2 وغيره)، وWordPiece (الذي يستخدمه BERT على سبيل المثال)، وUnigram (الذي يستخدمه T5 وغيره). قبل أن نبدأ، إليك نظرة عامة سريعة على كيفية عمل كل منها. لا تتردد في العودة إلى هذا الجدول بعد قراءة كل من الأقسام التالية إذا لم يكن واضحًا بالنسبة لك بعد.


النموذج | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
التدريب | يبدأ من مفردات صغيرة ويتعلم قواعد دمج الرموز | يبدأ من مفردات صغيرة ويتعلم قواعد دمج الرموز | يبدأ من مفردات كبيرة ويتعلم قواعد إزالة الرموز
خطوة التدريب | يدمج الرموز المقابلة لأكثر الأزواج شيوعًا | يدمج الرموز المقابلة للزوج ذو أفضل نتيجة بناءً على تكرار الزوج، مع إعطاء الأولوية للأزواج التي يكون فيها كل رمز فردي أقل تكرارًا | يزيل جميع الرموز في المفردات التي ستقلل من الخسارة المحسوبة على كامل المجموعة
يتعلم | قواعد الدمج ومفردات | مفردات فقط | مفردات مع نتيجة لكل رمز
الترميز | يقسم الكلمة إلى أحرف ويطبق عمليات الدمج التي تم تعلمها أثناء التدريب | يجد أطول كلمة فرعية بدءًا من البداية والتي تكون في المفردات، ثم يفعل الشيء نفسه لبقية الكلمة | يجد التقسيم الأكثر احتمالًا إلى رموز، باستخدام النتائج التي تم تعلمها أثناء التدريب

الآن دعنا نغوص في BPE!