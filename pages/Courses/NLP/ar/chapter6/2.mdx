# تدريب محدد كلمات جديد من محدد كلمات قديم [[training-a-new-tokenizer-from-an-old-one]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

إذا لم يكن نموذج اللغة متاحًا باللغة التي تهتم بها، أو إذا كان نصك مختلفًا جدًا عن النص الذي تم تدريب نموذج اللغة عليه، فمن المحتمل أنك تريد إعادة تدريب النموذج من الصفر باستخدام محدد كلمات مُعدل لبياناتك. سيتطلب ذلك تدريب محدد كلمات جديد على مجموعة بياناتك. ولكن ما هو المقصود بذلك بالضبط؟ عندما نظرنا لأول مرة إلى محددات الكلمات في [الفصل 2](/course/chapter2)، رأينا أن معظم نماذج المحول تستخدم _خوارزمية تجزيئية فرعية_. لتحديد الكلمات الفرعية التي تهم وتظهر بشكل متكرر في النص، يحتاج محدد الكلمات إلى فحص جميع النصوص في النص - وهي عملية نسميها *التدريب*. تعتمد القواعد الدقيقة التي تحكم هذا التدريب على نوع محدد الكلمات المستخدم، وسنمر عبر الخوارزميات الرئيسية الثلاثة لاحقًا في هذا الفصل.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

⚠️ تدريب محدد الكلمات ليس نفس تدريب النموذج! يستخدم تدريب النموذج النزول التدريجي العشوائي لجعل الخسارة أصغر قليلاً لكل دفعة. إنه عشوائي بطبيعته (مما يعني أنه عليك تعيين بعض البذور للحصول على نفس النتائج عند إجراء نفس التدريب مرتين). تدريب محدد الكلمات هو عملية إحصائية تحاول تحديد الكلمات الفرعية الأفضل لاختيارها لنص معين، والقواعد الدقيقة المستخدمة لاختيارها تعتمد على خوارزمية التجزيئية. إنه حتمي، مما يعني أنك تحصل دائمًا على نفس النتائج عند التدريب باستخدام نفس الخوارزمية على نفس النص.

</Tip>

## تجميع نص [[assembling-a-corpus]]

هناك واجهة برمجة تطبيقات بسيطة جدًا في 🤗 Transformers يمكنك استخدامها لتدريب محدد كلمات جديد بنفس خصائص محدد كلمات موجود: `AutoTokenizer.train_new_from_iterator()`. لرؤية ذلك في العمل، لنفترض أننا نريد تدريب GPT-2 من الصفر، ولكن بلغة أخرى غير الإنجليزية. ستكون مهمتنا الأولى هي جمع الكثير من البيانات بتلك اللغة في نص تدريبي. لتقديم أمثلة سيفهمها الجميع، لن نستخدم لغة مثل الروسية أو الصينية هنا، ولكن بدلاً من ذلك لغة إنجليزية متخصصة: كود بايثون.

يمكن لمكتبة [🤗 Datasets](https://github.com/huggingface/datasets) مساعدتنا في تجميع نص من كود مصدر بايثون. سنستخدم الدالة المعتادة `load_dataset()` لتنزيل وتخزين مجموعة بيانات [CodeSearchNet](https://huggingface.co/datasets/code_search_net) مؤقتًا. تم إنشاء هذه المجموعة من البيانات لتحدي [CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) وتحتوي على ملايين الدوال من مكتبات المصادر المفتوحة على GitHub بعدة لغات برمجة. هنا، سنقوم بتحميل الجزء الخاص بلغة بايثون من هذه المجموعة من البيانات:

```py
from datasets import load_dataset

# قد يستغرق هذا التحميل بضع دقائق، لذا احصل على قهوة أو شاي أثناء الانتظار!
raw_datasets = load_dataset("code_search_net", "python")
```

يمكننا إلقاء نظرة على التقسيم التدريبي لمعرفة الأعمدة التي يمكننا الوصول إليها:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

يمكننا أن نرى أن مجموعة البيانات تفصل الوثائق التوضيحية عن الكود وتقترح تجزيئية لكليهما. هنا، سنستخدم عمود `whole_func_string` فقط لتدريب محدد الكلمات الخاص بنا. يمكننا إلقاء نظرة على مثال واحد من هذه الدوال عن طريق الفهرسة في التقسيم التدريبي:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

الذي يجب أن يطبع ما يلي:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

أول شيء نحتاج إلى فعله هو تحويل مجموعة البيانات إلى _مكرر_ لقوائم النصوص - على سبيل المثال، قائمة من قوائم النصوص. ستمكننا قوائم النصوص من جعل محدد الكلمات أسرع (التدريب على دفعات من النصوص بدلاً من معالجة النصوص الفردية واحدة تلو الأخرى)، ويجب أن يكون مكررًا إذا أردنا تجنب وجود كل شيء في الذاكرة في نفس الوقت. إذا كان نصك ضخمًا، فستود الاستفادة من حقيقة أن 🤗 Datasets لا يحمل كل شيء في ذاكرة الوصول العشوائي ولكن يخزن عناصر مجموعة البيانات على القرص.

سيعمل فعل ما يلي على إنشاء قائمة من قوائم 1000 نص لكل منها، ولكن سيحمل كل شيء في الذاكرة:

```py
# لا تقم بإلغاء تعليق السطر التالي إلا إذا كانت مجموعة بياناتك صغيرة!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

باستخدام مولد بايثون، يمكننا تجنب تحميل بايثون لأي شيء في الذاكرة حتى يكون ذلك ضروريًا بالفعل. لإنشاء مثل هذا المولد، تحتاج فقط إلى استبدال الأقواس بالأقواس:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

هذا السطر من الكود لا يسترد أي عناصر من مجموعة البيانات؛ إنه فقط ينشئ كائن يمكنك استخدامه في حلقة `for` بايثون. لن يتم تحميل النصوص إلا عند الحاجة إليها (أي عندما تكون في خطوة حلقة `for` التي تتطلبها)، وسيتم تحميل 1000 نص فقط في كل مرة. بهذه الطريقة لن تستنفد كل ذاكرتك حتى إذا كنت تعالج مجموعة بيانات ضخمة.

المشكلة مع كائن المولد هي أنه يمكن استخدامه مرة واحدة فقط. لذا، بدلاً من إعطائنا قائمة بأول 10 أرقام مرتين:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

نحصل عليها مرة واحدة ثم قائمة فارغة:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

لهذا السبب نحدد دالة تعيد مولدًا بدلاً من ذلك:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

يمكنك أيضًا تحديد مولدك داخل حلقة `for` باستخدام عبارة `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

والذي سينتج عنه نفس المولد بالضبط كما كان من قبل، ولكنه يسمح لك باستخدام منطق أكثر تعقيدًا مما يمكنك في تعبير قائمة الفهم.

## تدريب محدد كلمات جديد [[training-a-new-tokenizer]]

الآن بعد أن أصبح لدينا نصنا في شكل مكرر من دفعات النصوص، نحن مستعدون لتدريب محدد كلمات جديد. للقيام بذلك، نحتاج أولاً إلى تحميل محدد الكلمات الذي نريد إقرانه بنموذجنا (هنا، GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

على الرغم من أننا سنقوم بتدريب محدد كلمات جديد، فمن الجيد فعل ذلك لتجنب البدء من الصفر تمامًا. بهذه الطريقة، لن نضطر إلى تحديد أي شيء عن خوارزمية التجزيئية أو الرموز الخاصة التي نريد استخدامها؛ سيكون محدد كلماتنا الجديد مطابقًا تمامًا لـ GPT-2، والشيء الوحيد الذي سيتغير هو المفردات، والتي سيتم تحديدها من خلال التدريب على نصنا.

أولاً دعونا نلقي نظرة على كيفية معاملة هذا المحدد لكلمات لدالة مثال:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```python
['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ"""', 'Add', 'Ġthe', 'Ġtwo',
 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '."', '""', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
```

يحتوي هذا المحلل الرمزي على بعض الرموز الخاصة، مثل `Ġ` و`Ċ`، والتي تشير إلى المسافات والأسطر الجديدة على التوالي. كما نرى، هذا ليس فعالًا للغاية: يقوم المحلل الرمزي بإرجاع رموز فردية لكل مسافة، عندما يمكنه تجميع مستويات المسافة البادئة معًا (نظرًا لأن وجود مجموعات من أربع أو ثماني مسافات سيكون شائعًا جدًا في الكود). كما قام بتقسيم اسم الدالة بطريقة غريبة، غير معتاد على رؤية كلمات تحتوي على حرف `_`.

دعنا نقوم بتدريب محلل رمزي جديد ونرى إذا كان سيحل هذه المشكلات. لهذا، سنستخدم الطريقة `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

قد يستغرق هذا الأمر بعض الوقت إذا كان نصك كبيرًا جدًا، ولكن بالنسبة لمجموعة البيانات هذه التي تبلغ 1.6 جيجابايت من النصوص، فهو سريع للغاية (دقيقة واحدة و16 ثانية على معالج AMD Ryzen 9 3900X بمعالج 12 نواة).

لاحظ أن `AutoTokenizer.train_new_from_iterator()` يعمل فقط إذا كان المحلل الرمزي الذي تستخدمه هو محلل رمزي "سريع". كما سترى في القسم التالي، تحتوي مكتبة 🤗 Transformers على نوعين من المحللات الرمزية: بعضها مكتوب بالكامل في بايثون والبعض الآخر (السريع) مدعوم بمكتبة 🤗 Tokenizers، والتي مكتوبة بلغة [Rust](https://www.rust-lang.org) البرمجية. بايثون هي اللغة الأكثر استخدامًا في علم البيانات وتطبيقات التعلم العميق، ولكن عندما يحتاج أي شيء إلى أن يكون موازيًا ليكون سريعًا، يجب كتابته بلغة أخرى. على سبيل المثال، عمليات الضرب المصفوفة التي هي في قلب حساب النموذج مكتوبة في CUDA، وهي مكتبة C محسنة لوحدات معالجة الرسومات.

سيكون تدريب محلل رمزي جديد في بايثون بطيئًا للغاية، وهو السبب في تطويرنا لمكتبة 🤗 Tokenizers. لاحظ أنه كما لم يكن عليك تعلم لغة CUDA لكي تتمكن من تنفيذ نموذجك على دفعة من الإدخالات على وحدة معالجة الرسومات، فلن تحتاج إلى تعلم Rust لاستخدام محلل رمزي سريع. توفر مكتبة 🤗 Tokenizers روابط بايثون للعديد من الطرق التي تستدعي داخليًا قطعة من الكود في Rust؛ على سبيل المثال، لتوازي تدريب محلل رمزي الجديد الخاص بك أو، كما رأينا في [الفصل 3](/course/chapter3)، تحليل دفعة من الإدخالات.

معظم نماذج المحول لديها محلل رمزي سريع متاح (هناك بعض الاستثناءات التي يمكنك التحقق منها [هنا](https://huggingface.co/transformers/#supported-frameworks))، ويوفر واجهة برمجة التطبيقات `AutoTokenizer` دائمًا المحلل الرمزي السريع لك إذا كان متاحًا. في القسم التالي، سنلقي نظرة على بعض الميزات الخاصة الأخرى التي تمتلكها المحللات الرمزية السريعة، والتي ستكون مفيدة للغاية لمهام مثل تصنيف الرموز والإجابة على الأسئلة. قبل الغوص في ذلك، ومع ذلك، دعنا نجرب محللنا الرمزي الجديد على المثال السابق:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ"""', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`',
 'a', '`', 'Ġand', 'Ġ`', 'b', '`."""', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
```

هنا نرى مرة أخرى الرموز الخاصة `Ġ` و`Ċ` التي تشير إلى المسافات والأسطر الجديدة، ولكن يمكننا أيضًا أن نرى أن محللنا الرمزي تعلم بعض الرموز التي تخص نصًا من دوال بايثون: على سبيل المثال، هناك رمز `ĊĠĠĠ` الذي يمثل مسافة بادئة، ورمز `Ġ"""` الذي يمثل علامات الاقتباس الثلاثة التي تبدأ سلسلة التوثيق. قام المحلل الرمزي أيضًا بتقسيم اسم الدالة بشكل صحيح على `_`. هذه تمثيل مضغوط للغاية؛ بالمقارنة، باستخدام المحلل الرمزي الإنجليزي العادي على نفس المثال سيعطينا جملة أطول:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

دعنا ننظر إلى مثال آخر:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',',
 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ',
 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']
```

بالإضافة إلى الرمز المقابل لمسافة البادئة، هنا يمكننا أيضًا رؤية رمز لمسافة بادئة مزدوجة: `ĊĠĠĠĠĠĠĠ`. يتم تحليل الكلمات الخاصة في بايثون مثل `class` و`init` و`call` و`self` و`return` كرمز واحد، ويمكننا أن نرى أيضًا أنه بالإضافة إلى التقسيم على `_` و`.`، يقوم المحلل الرمزي بتقسيم الأسماء المكتوبة بطريقة CamelCase بشكل صحيح: يتم تحليل `LinearLayer` كـ `["ĠLinear", "Layer"]`.

## حفظ المحلل الرمزي [[saving-the-tokenizer]]

للتأكد من أننا يمكننا استخدامه لاحقًا، نحتاج إلى حفظ محللنا الرمزي الجديد. مثل النماذج، يتم ذلك باستخدام طريقة `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

سيؤدي هذا إلى إنشاء مجلد جديد يسمى *code-search-net-tokenizer*، والذي سيحتوي على جميع الملفات التي يحتاجها المحلل الرمزي لإعادة التحميل. إذا كنت ترغب في مشاركة هذا المحلل الرمزي مع زملائك وأصدقائك، يمكنك تحميله على Hub عن طريق تسجيل الدخول إلى حسابك. إذا كنت تعمل في دفتر ملاحظات، فهناك وظيفة ملائمة لمساعدتك في ذلك:

```python
from huggingface_hub import notebook_login

notebook_login()
```

سيتم عرض أداة يمكنك من خلالها إدخال بيانات اعتماد تسجيل الدخول إلى Hugging Face. إذا لم تكن تعمل في دفتر ملاحظات، فما عليك سوى كتابة السطر التالي في طرفيتك:

```bash
huggingface-cli login
```

بمجرد تسجيل الدخول، يمكنك دفع محللك الرمزي عن طريق تنفيذ الأمر التالي:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

سيؤدي هذا إلى إنشاء مستودع جديد في مساحة اسمك باسم `code-search-net-tokenizer`، يحتوي على ملف المحلل الرمزي. بعد ذلك، يمكنك تحميل المحلل الرمزي من أي مكان باستخدام طريقة `from_pretrained()`:

```py
# استبدل "huggingface-course" أدناه بمساحة اسمك الفعلية لاستخدام محللك الرمزي الخاص بك
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

أنت الآن جاهز لتدريب نموذج لغة من الصفر وضبطه الدقيق على المهمة التي بين يديك! سنصل إلى ذلك في [الفصل 7](/course/chapter7)، ولكن أولاً، في بقية هذا الفصل، سنلقي نظرة فاحصة على المحللات الرمزية السريعة واستكشاف ما يحدث بالفعل عند استدعاء طريقة `train_new_from_iterator()`.