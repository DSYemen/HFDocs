# ุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏ ูู ูุญุฏุฏ ุฑููุฒ ูุฏูู

ุฅุฐุง ูู ููู ูููุฐุฌ ุงููุบุฉ ูุชุงุญูุง ุจุงููุบุฉ ุงูุชู ุชูููุ ุฃู ุฅุฐุง ูุงู ูุตู ูุฎุชูููุง ุฌุฏูุง ุนู ุงููุต ุงูุฐู ุชู ุชุฏุฑูุจ ูููุฐุฌ ุงููุบุฉ ุนูููุ ููู ุงููุญุชูู ุฃูู ุชุฑูุฏ ุฅุนุงุฏุฉ ุชุฏุฑูุจ ุงููููุฐุฌ ูู ุงูุตูุฑ ุจุงุณุชุฎุฏุงู ูุญุฏุฏ ุฑููุฒ ูููู ุนูู ุจูุงูุงุชู. ุณูุชุทูุจ ุฐูู ุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏ ุนูู ูุฌููุนุฉ ุจูุงูุงุชู. ูููู ูุงุฐุง ูุนูู ุฐูู ุจุงูุถุจุทุ ุนูุฏูุง ูุธุฑูุง ูุฃูู ูุฑุฉ ุฅูู ูุญุฏุฏุงุช ุงูุฑููุฒ ูู [ุงููุตู 2] (/course/chapter2)ุ ุฑุฃููุง ุฃู ูุนุธู ููุงุฐุฌ ุงููุญููุงุช ุชุณุชุฎุฏู _ุฎูุงุฑุฒููุฉ ุชุญุฏูุฏ ุฑููุฒ ูุฑุนูุฉ_. ูุชุญุฏูุฏ ุงูุฑููุฒ ุงููุฑุนูุฉ ุฐุงุช ุงูุฃูููุฉ ูุงูุชู ุชุญุฏุซ ุจุดูู ูุชูุฑุฑ ูู ุงููุต ุงููุนููุ ูุฌุจ ุนูู ูุญุฏุฏ ุงูุฑููุฒ ุฅููุงุก ูุธุฑุฉ ูุงุญุตุฉ ุนูู ุฌููุน ุงููุตูุต ูู ุงููุต - ููู ุนูููุฉ ูุณูููุง *ุงูุชุฏุฑูุจ*. ุชุนุชูุฏ ุงูููุงุนุฏ ุงูุฏูููุฉ ุงูุชู ุชุญูู ูุฐุง ุงูุชุฏุฑูุจ ุนูู ููุน ูุญุฏุฏ ุงูุฑููุฒ ุงููุณุชุฎุฏูุ ูุณูุชูุงูู ุงูุฎูุงุฑุฒููุงุช ุงูุฑุฆูุณูุฉ ุงูุซูุงุซุฉ ูุงุญููุง ูู ูุฐุง ุงููุตู.

<Tip warning={true}>
โ๏ธ ุชุฏุฑูุจ ูุญุฏุฏ ุงูุฑููุฒ ููุณ ูุซู ุชุฏุฑูุจ ุงููููุฐุฌ! ูุณุชุฎุฏู ุชุฏุฑูุจ ุงููููุฐุฌ ุงูุงูุญุฏุงุฑ ุงูุชุฏุฑูุฌู ุงูุนุดูุงุฆู ูุฌุนู ุงูุฎุณุงุฑุฉ ุฃุตุบุฑ ููููุงู ููู ุฏูุนุฉ. ุฅูู ุนุดูุงุฆู ุจุทุจูุนุชู (ููุง ูุนูู ุฃูู ูุชุนูู ุนููู ุชุนููู ุจุนุถ ุงูุจุฐูุฑ ููุญุตูู ุนูู ููุณ ุงููุชุงุฆุฌ ุนูุฏ ุฅุฌุฑุงุก ุงูุชุฏุฑูุจ ููุณู ูุฑุชูู). ุชุฏุฑูุจ ูุญุฏุฏ ุงูุฑููุฒ ูู ุนูููุฉ ุฅุญุตุงุฆูุฉ ุชุญุงูู ุชุญุฏูุฏ ุฃูุถู ุงูุฑููุฒ ุงููุฑุนูุฉ ููุงุฎุชูุงุฑ ููุฌููุนุฉ ูุนููุฉ ูู ุงููุตูุตุ ูุชุนุชูุฏ ุงูููุงุนุฏ ุงูุฏูููุฉ ุงููุณุชุฎุฏูุฉ ูุงุฎุชูุงุฑูุง ุนูู ุฎูุงุฑุฒููุฉ ุชุญุฏูุฏ ุงูุฑููุฒ. ุฅูู ุญุชููุ ููุง ูุนูู ุฃูู ุชุญุตู ุฏุงุฆููุง ุนูู ููุณ ุงููุชุงุฆุฌ ุนูุฏ ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ููุณ ุงูุฎูุงุฑุฒููุฉ ุนูู ููุณ ุงููุต.
</Tip>

## ุชุฌููุน ูุต ูุจูุฑ

ููุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช (API) ุจุณูุทุฉ ููุบุงูุฉ ูู ๐ค Transformers ููููู ุงุณุชุฎุฏุงููุง ูุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏ ุจููุณ ุฎุตุงุฆุต ูุญุฏุฏ ุฑููุฒ ููุฌูุฏ: `AutoTokenizer.train_new_from_iterator()`. ููุดุงูุฏุฉ ุฐูู ูู ุงูุนููุ ุฏุนูุง ูููู ุฃููุง ูุฑูุฏ ุชุฏุฑูุจ GPT-2 ูู ุงูุตูุฑุ ูููู ุจูุบุฉ ุฃุฎุฑู ุบูุฑ ุงููุบุฉ ุงูุฅูุฌููุฒูุฉ. ุณุชููู ูููุชูุง ุงูุฃููู ูู ุฌูุน ุงููุซูุฑ ูู ุงูุจูุงูุงุช ุจุชูู ุงููุบุฉ ูู ูุต ุชุฏุฑูุจู. ูุชูุฏูู ุฃูุซูุฉ ูููู ููุฌููุน ููููุงุ ูู ูุณุชุฎุฏู ูุบุฉ ูุซู ุงูุฑูุณูุฉ ุฃู ุงูุตูููุฉ ููุงุ ูููู ุจุฏูุงู ูู ุฐููุ ูุบุฉ ุฅูุฌููุฒูุฉ ูุชุฎุตุตุฉ: ููุฏ Python.

ูููู ูููุชุจุฉ [๐ค Datasets](https://github.com/huggingface/datasets) ูุณุงุนุฏุชูุง ูู ุชุฌููุน ูุต ูู ููุฏ ูุตุฏุฑ Python. ุณูุณุชุฎุฏู ุงูุฏุงูุฉ ุงููุนุชุงุฏุฉ `load_dataset()` ูุชูุฒูู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููุฌููุนุฉ ุงูุจูุงูุงุช [CodeSearchNet](https://huggingface.co/datasets/code_search_net). ุชู ุฅูุดุงุก ูุฐู ุงููุฌููุนุฉ ููุชุญุฏู [CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) ูุชุญุชูู ุนูู ููุงููู ุงููุธุงุฆู ูู ููุชุจุงุช ุงููุตุงุฏุฑ ุงูููุชูุญุฉ ุนูู GitHub ุจุนุฏุฉ ูุบุงุช ุจุฑูุฌุฉ. ููุงุ ุณูููู ุจุชุญููู ุงูุฌุฒุก ุงูุฎุงุต ุจู Python ูู ูุฐู ุงููุฌููุนุฉ:

```py
from datasets import load_dataset

# ูุฏ ูุณุชุบุฑู ูุฐุง ุงูุฃูุฑ ุจุถุน ุฏูุงุฆู ููุชุญูููุ ูุฐุง ุงุญุตู ุนูู ููุฌุงู ูู ุงููููุฉ ุฃู ุงูุดุงู ุฃุซูุงุก ุงูุชุธุงุฑู!
raw_datasets = load_dataset("code_search_net", "python")
```

ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ุงูุชูุณูู ุงูุชุฏุฑูุจู ููุนุฑูุฉ ุงูุฃุนูุฏุฉ ุงูุชู ูููููุง ุงููุตูู ุฅูููุง:

```py
raw_datasets["train"]
```

```python out
Dataset({
features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language',
'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name',
'func_code_url'
],
num_rows: 412178
})
```

ูููููุง ุฃู ูุฑู ุฃู ูุฌููุนุฉ ุงูุจูุงูุงุช ุชูุตู ุงููุซุงุฆู ุงูุชูุถูุญูุฉ ุนู ุงูููุฏ ูุชูุชุฑุญ ุชุญุฏูุฏ ุฑููุฒ ููููููุง. ููุงุ ุณูุณุชุฎุฏู ุนููุฏ `whole_func_string` ููุท ูุชุฏุฑูุจ ูุญุฏุฏ ุงูุฑููุฒ ุงูุฎุงุต ุจูุง. ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ูุซุงู ุนู ุฅุญุฏู ูุฐู ุงููุธุงุฆู ุนู ุทุฑูู ุงูููุฑุณุฉ ูู ุงูุชูุณูู ุงูุชุฏุฑูุจู:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

ุงูุฐู ูุฌุจ ุฃู ูุทุจุน ูุง ููู:

```out
def handle_simple_responses(
self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
"""Accepts normal responses from the device.

Args:
timeout_ms: Timeout in milliseconds to wait for each response.
info_cb: Optional callback for text sent from the bootloader.

Returns:
OKAY packet's message.
"""
return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

ุฃูู ุดูุก ูุชุนูู ุนูููุง ูุนูู ูู ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู _iterator_ ูู ููุงุฆู ุงููุตูุต - ุนูู ุณุจูู ุงููุซุงูุ ูุงุฆูุฉ ุจููุงุฆู ุงููุตูุต. ุณุชููููุง ุงุณุชุฎุฏุงู ููุงุฆู ุงููุตูุต ูู ุฌุนู ูุญุฏุฏ ุงูุฑููุฒ ุฃุณุฑุน (ุงูุชุฏุฑูุจ ุนูู ุฏูุนุงุช ูู ุงููุตูุต ุจุฏูุงู ูู ูุนุงูุฌุฉ ุงููุตูุต ุงููุฑุฏูุฉ ูุงุญุฏูุง ุชูู ุงูุขุฎุฑ)ุ ููุฌุจ ุฃู ูููู iterator ุฅุฐุง ุฃุฑุฏูุง ุชุฌูุจ ูุฌูุฏ ูู ุดูุก ูู ุงูุฐุงูุฑุฉ ูู ููุณ ุงูููุช. ุฅุฐุง ูุงูุช ูุฌููุนุฉ ุงููุตูุต ุงูุฎุงุตุฉ ุจู ุถุฎูุฉุ ูุณุชุญุชุงุฌ ุฅูู ุงูุงุณุชูุงุฏุฉ ูู ุญูููุฉ ุฃู ๐ค Datasets ูุง ูุญูู ูู ุดูุก ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู ููููู ูุฎุฒู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุนูู ุงููุฑุต.

ุณูุชุทูุจ ุงูููุงู ุจูุง ููู ุฅูุดุงุก ูุงุฆูุฉ ุจููุงุฆู 1000 ูุต ููู ูููุงุ ููููู ุณูุญูู ูู ุดูุก ูู ุงูุฐุงูุฑุฉ:

```py
# ูุง ุชูู ุจุฅูุบุงุก ุงูุชุนููู ุนู ุงูุณุทุฑ ุงูุชุงูู ูุง ูู ุชูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุตุบูุฑุฉ!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

ุจุงุณุชุฎุฏุงู ูููุฏ Pythonุ ูููููุง ุชุฌูุจ ุชุญููู Python ูุฃู ุดูุก ูู ุงูุฐุงูุฑุฉ ุญุชู ูููู ุฐูู ุถุฑูุฑููุง. ูุฅูุดุงุก ูุซู ูุฐุง ุงููููุฏุ ุชุญุชุงุฌ ููุท ุฅูู ุงุณุชุจุฏุงู ุงูุฃููุงุณ ุจุงูููุงุตู:

```py
training_corpus = (
raw_datasets["train"][i : i + 1000]["whole_func_string"]
for i in range(0, len(raw_datasets["train"]), 1000)
)
```

ูุง ูููู ูุฐุง ุงูุณุทุฑ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุจุงุณุชุฑุฏุงุฏ ุฃู ุนูุงุตุฑ ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ููู ูููู ููุท ุจุฅูุดุงุก ูุงุฆู ููููู ุงุณุชุฎุฏุงูู ูู ุญููุฉ Python `for`. ูู ูุชู ุชุญููู ุงููุตูุต ุฅูุง ุนูุฏ ุงูุญุงุฌุฉ ุฅูููุง (ุฃู ุนูุฏ ุงููุตูู ุฅูู ุงูุฎุทูุฉ ูู ุญููุฉ `for` ุงูุชู ุชุชุทูุจูุง)ุ ููู ูุชู ุชุญููู ุณูู 1000 ูุต ูู ูู ูุฑุฉ. ุจูุฐู ุงูุทุฑููุฉุ ูู ุชุณุชููุฏ ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู ุงูุฎุงุตุฉ ุจู ุญุชู ุฅุฐุง ููุช ุชููู ุจูุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช ุถุฎูุฉ.

ุงููุดููุฉ ูุน ูุงุฆู ุงููููุฏ ูู ุฃูู ูููู ุงุณุชุฎุฏุงูู ูุฑุฉ ูุงุญุฏุฉ ููุท. ูุฐุงุ ุจุฏูุงู ูู ุฅุนุทุงุฆูุง ูุงุฆูุฉ ุจุฃูู 10 ุฃุฑูุงู ูุฑุชูู:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

ูุญุตู ุนูููุง ูุฑุฉ ูุงุญุฏุฉ ุซู ูุงุฆูุฉ ูุงุฑุบุฉ:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

ูุฐุง ูู ุงูุณุจุจ ูู ุฃููุง ูุญุฏุฏ ุฏุงูุฉ ุชุนูุฏ ูููุฏูุง ุจุฏูุงู ูู ุฐูู:

```py
def get_training_corpus():
return (
raw_datasets["train"][i : i + 1000]["whole_func_string"]
for i in range(0, len(raw_datasets["train"]), 1000)
)


training_corpus = get_training_corpus()
```

ููููู ุฃูุถูุง ุชุญุฏูุฏ ูููุฏู ุฏุงุฎู ุญููุฉ `for` ุจุงุณุชุฎุฏุงู ุนุจุงุฑุฉ `yield`:

```py
def get_training_corpus():
dataset = raw_datasets["train"]
for start_idx in range(0, len(dataset), 1000):
samples = dataset[start_idx : start_idx + 1000]
yield samples["whole_func_string"]
```

ูุงูุฐู ุณููุชุฌ ููุณ ุงููููุฏ ููุง ูู ุงูุญุงู ูุจูุ ููููู ูุณูุญ ูู ุจุงุณุชุฎุฏุงู ููุทู ุฃูุซุฑ ุชุนููุฏูุง ููุง ููููู ูู ุชุนุจูุฑ ูุงุฆูุฉ ุงูููู.

## ุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏ

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญ ูุฏููุง ูุตูุง ูู ุดูู iterator ูู ุฏูุนุงุช ุงููุตูุตุ ููุญู ูุณุชุนุฏูู ูุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏ. ููููุงู ุจุฐููุ ูุญุชุงุฌ ุฃููุงู ุฅูู ุชุญููู ูุญุฏุฏ ุงูุฑููุฒ ุงูุฐู ูุฑูุฏ ุฅูุฑุงูู ุจูููุฐุฌูุง (ููุงุ GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ุนูู ุงูุฑุบู ูู ุฃููุง ุณูููู ุจุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏุ ุฅูุง ุฃู ูุฐู ููุฑุฉ ุฌูุฏุฉ ูุชุฌูุจ ุงูุจุฏุก ูู ุงูุตูุฑ ุชูุงููุง. ุจูุฐู ุงูุทุฑููุฉุ ูู ูุถุทุฑ ุฅูู ุชุญุฏูุฏ ุฃู ุดูุก ุญูู ุฎูุงุฑุฒููุฉ ุชุญุฏูุฏ ุงูุฑููุฒ ุฃู ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุฑูุฏ ุงุณุชุฎุฏุงููุงุ ุณูููู ูุญุฏุฏ ุงูุฑููุฒ ุงูุฌุฏูุฏ ุงูุฎุงุต ุจูุง ูุทุงุจููุง ุชูุงููุง ูู GPT-2ุ ูุงูุดูุก ุงููุญูุฏ ุงูุฐู ุณูุชุบูุฑ ูู ุงูููุฑุฏุงุชุ ูุงูุชู ุณูุชู ุชุญุฏูุฏูุง ุจูุงุณุทุฉ ุงูุชุฏุฑูุจ ุนูู ูุตูุง.

ุฃููุงู ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ููููุฉ ูุนุงูุฌุฉ ูุฐุง ุงููุญุฏุฏ ูุฑููุฒ ูุซุงู ุนู ุฅุญุฏู ุงููุธุงุฆู:

```py
example = '''def add_numbers(a, b):
"""Add the two numbers `a` and `b`."""
return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ฤadd', '_', 'n', 'umbers', '(', 'a', ',', 'ฤb', '):', 'ฤ', 'ฤ', 'ฤ', 'ฤ', 'ฤ"""', 'Add', 'ฤthe', 'ฤtwo',
'ฤnumbers', 'ฤ`', 'a', '`', 'ฤand', 'ฤ`', 'b', '`', '."', '""', 'ฤ', 'ฤ', 'ฤ', 'ฤ', 'ฤreturn', 'ฤa', 'ฤ+', 'ฤb']
```

ูุญุชูู ูุฐุง ุงููุญุฏุฏ ูุฑููุฒ ุนูู ุจุนุถ ุงูุฑููุฒ ุงูุฎุงุตุฉุ ูุซู `ฤ` ู `ฤ`ุ ูุงูุชู ุชุดูุฑ ุฅูู ุงููุณุงูุงุช ูุนูุงูุงุช ุงูุฃุณุทุฑ ุงูุฌุฏูุฏุฉุ ุนูู ุงูุชูุงูู. ููุง ูุฑูุ ูุฐุง ุบูุฑ ูุนุงู ููุบุงูุฉ: ูููู ูุญุฏุฏ ุงูุฑููุฒ ุจุฅุฑุฌุงุน ุฑููุฒ ูุฑุฏูุฉ ููู ูุณุงูุฉุ ุนูุฏูุง ููููู ุชุฌููุน ูุณุชููุงุช ุงูุฅุฒุงุญุฉ (ูุธุฑูุง ูุฃู ูุฌูุฏ ูุฌููุนุงุช ูู ุฃุฑุจุนุฉ ุฃู ุซูุงููุฉ ูุณุงูุงุช ุณูููู ุดุงุฆุนูุง ุฌุฏูุง ูู ุงูููุฏ). ููุง ุฃูู ูุงู ุจุชูุณูู ุงุณู ุงููุธููุฉ ุจุทุฑููุฉ ุบุฑูุจุฉุ ุญูุซ ูู ูุนุชุฏ ุนูู ุฑุคูุฉ ูููุงุช ุชุญุชูู ุนูู ุญุฑู `_`.

ุฏุนููุง ูููู ุจุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏ ููุฑู ูุง ุฅุฐุง ูุงู ุฐูู ูุญู ูุฐู ุงููุดููุงุช. ููููุงู ุจุฐููุ ุณูุณุชุฎุฏู ุทุฑููุฉ `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

ูุฏ ูุณุชุบุฑู ูุฐุง ุงูุฃูุฑ ุจุนุถ ุงูููุช ุฅุฐุง ูุงู ูุตู ูุจูุฑูุง ุฌุฏูุงุ ููููู ุณุฑูุน ููุบุงูุฉ ุจุงููุณุจุฉ ููุฌููุนุฉ ุงูุจูุงูุงุช ูุฐู ุงูุชู ุชุจูุบ 1.6 ุฌูุฌุงุจุงูุช ูู ุงููุตูุต (ุฏูููุฉ ูุงุญุฏุฉ ู16 ุซุงููุฉ ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ AMD Ryzen 9 3900X ุฐุงุช 12 ููุงุฉ).

ูุงุญุธ ุฃู `AutoTokenizer.train_new_from_iterator()` ูุนูู ููุท ุฅุฐุง ูุงู ูุญุฏุฏ ุงูุฑููุฒ ุงูุฐู ุชุณุชุฎุฏูู ูู ูุญุฏุฏ ุฑููุฒ "ุณุฑูุน". ููุง ุณุชุฑู ูู ุงููุณู ุงูุชุงููุ ุชุญุชูู ููุชุจุฉ ๐ค Transformers ุนูู ููุนูู ูู ูุญุฏุฏุงุช ุงูุฑููุฒ: ุจุนุถูุง ููุชูุจ ุจูุบุฉ Python ููุท ูุงูุจุนุถ ุงูุขุฎุฑ (ุงูุณุฑูุน) ูุฏุนูู ูู ููุชุจุฉ ๐ค Tokenizersุ ุงูููุชูุจุฉ ุจูุบุฉ ุจุฑูุฌุฉ [Rust](https://www.rust-lang.org). Python ูู ุงููุบุฉ ุงูุฃูุซุฑ ุงุณุชุฎุฏุงููุง ูู ุนูู ุงูุจูุงูุงุช ูุชุทุจููุงุช ุงูุชุนูู ุงูุนูููุ ูููู ุนูุฏูุง ูุญุชุงุฌ ุฃู ุดูุก ุฅูู ููุงุฒุงุฉ ููุตุจุญ ุณุฑูุนูุงุ ูุฌุจ ูุชุงุจุชู ุจูุบุฉ ุฃุฎุฑู. ุนูู ุณุจูู ุงููุซุงูุ ูุชู ูุชุงุจุฉ ุนูููุงุช ุงูุถุฑุจ ุจุงููุตูููุฉ ุงูุชู ุชุดูู ุฌููุฑ ุญุณุงุจ ุงููููุฐุฌ ุจูุบุฉ C CUDA ุงููุญุณูุฉุ ูููุชุจุงุช GPU.

ุณูููู ุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุฌุฏูุฏ ุชูุงููุง ูู Python ุจุทูุฆูุง ููุบุงูุฉุ ููุฐุง ูู ุงูุณุจุจ ูู ุชุทููุฑ ููุชุจุฉ ๐ค Tokenizers. ูุงุญุธ ุฃูู ุชูุงููุง ููุง ูู ููู ุนููู ุชุนูู ูุบุฉ CUDA ูุชุชููู ูู ุชุดุบูู ูููุฐุฌู ุนูู ุฏูุนุฉ ูู ุงููุฏุฎูุงุช ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU)ุ ููู ุชุญุชุงุฌ ุฅูู ุชุนูู Rust ูุงุณุชุฎุฏุงู ูุญุฏุฏ ุฑููุฒ ุณุฑูุน. ุชููุฑ ููุชุจุฉ ๐ค Tokenizers ุฑูุงุจุท Python ููุนุฏูุฏ ูู ุงูุทุฑู ุงูุชู ุชุณุชุฏุนู ุฏุงุฎูููุง ุจุนุถ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ูู Rustุ ุนูู ุณุจูู ุงููุซุงูุ ูููุงุฒุงุฉ ุชุฏุฑูุจ ูุญุฏุฏ ุฑููุฒ ุงูุฌุฏูุฏ ุงูุฎุงุต ุจู ุฃูุ ููุง ุฑุฃููุง ูู [ุงููุตู 3] (/course/chapter3)ุ ุชุญุฏูุฏ ุฑููุฒ ุฏูุนุฉ ูู ุงููุฏุฎูุงุช.

ุชุชููุฑ ูุนุธู ููุงุฐุฌ ุงููุญูู ูุญุฏุฏ ุฑููุฒ ุณุฑูุน (ููุงู ุจุนุถ ุงูุงุณุชุซูุงุกุงุช ุงูุชู ููููู ุงูุชุญูู ูููุง [ููุง](https://huggingface.co/transformers/#supported-frameworks))ุ ูAPI ูู `AutoTokenizer` ูุฎุชุงุฑ ุฏุงุฆููุง ูุญุฏุฏ ุงูุฑููุฒ ุงูุณุฑูุน ูู ุฅุฐุง ูุงู ูุชุงุญูุง. ูู ุงููุณู ุงูุชุงููุ ุณูููู ูุธุฑุฉ ุนูู ุจุนุถ ุงูููุฒุงุช ุงูุฎุงุตุฉ ุงูุฃุฎุฑู ุงูุชู ุชุชูุชุน ุจูุง ูุญุฏุฏุงุช ุงูุฑููุฒ ุงูุณุฑูุนุฉุ ูุงูุชู ุณุชููู ูููุฏุฉ ุฌุฏูุง ูููุงู ูุซู ุชุตููู ุงูุฑููุฒ ูุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ. ูุจู ุงูุบูุต ูู ุฐููุ ููุน ุฐููุ ุฏุนููุง ูุฌุฑุจ ูุญุฏุฏ ุงูุฑููุฒ ุงูุฌุฏูุฏ ุงูุฎุงุต ุจูุง ุนูู ุงููุซุงู ุงูุณุงุจู:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ฤadd', '_', 'numbers', '(', 'a', ',', 'ฤb', '):', 'ฤฤฤฤ', 'ฤ"""', 'Add', 'ฤthe', 'ฤtwo', 'ฤnumbers', 'ฤ`',
'a', '`', 'ฤand', 'ฤ`', 'b', '`."""', 'ฤฤฤฤ', 'ฤreturn', 'ฤa', 'ฤ+', 'ฤb']
```

ููุง ูุฑู ูุฑุฉ ุฃุฎุฑู ุงูุฑููุฒ ุงูุฎุงุตุฉ `ฤ` ู `ฤ` ุงูุชู ุชุดูุฑ ุฅูู ุงููุณุงูุงุช ูุนูุงูุงุช ุงูุฃุณุทุฑ ุงูุฌุฏูุฏุฉุ ูููู ูููููุง ุฃูุถูุง ุฃู ูุฑู ุฃู ูุญุฏุฏ ุงูุฑููุฒ ุงูุฎุงุต ุจูุง ุชุนูู ุจุนุถ ุงูุฑููุฒ ุงูุชู ุชุฎุต ูุตูุง ูู ูุธุงุฆู Python: ุนูู ุณุจูู ุงููุซุงูุ ููุงู ุฑูุฒ `ฤฤฤฤ` ููุซู ุฅุฒุงุญุฉุ ูุฑูุฒ `ฤ"""` ููุซู ุนูุงูุงุช ุงูุงูุชุจุงุณ ุงูุซูุงุซุฉ ุงูุชู ุชุจุฏุฃ ุจdocstring. ูุงู ูุญุฏุฏ ุงูุฑููุฒ ุฃูุถูุง ุจุชูุณูู ุงุณู ุงููุธููุฉ ุจุดูู ุตุญูุญ ุนูู `_`. ูุฐุง ุชูุซูู ูุถุบูุท ููุบุงูุฉุ ุนูู ุณุจูู ุงูููุงุฑูุฉุ ุจุงุณุชุฎุฏุงู ูุญุฏุฏ ุฑููุฒ ุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ุงูุนุงุฏู ุนูู ููุณ ุงููุซุงู ุณูุนุทููุง ุฌููุฉ ุฃุทูู:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

ููููู ูุธุฑุฉ ุนูู ูุซุงู ุขุฎุฑ:

```python
example = """class LinearLayer():
def __init__(self, input_size, output_size):
self.weight = torch.randn(input_size, output_size)
self.bias = torch.zeros(output_size)

def __call__(self, x):
return x @ self.weights + self.bias
"""
tokenizer.tokenize(example)
```

```python out
['class', 'ฤLinear', 'Layer', '():', 'ฤฤฤฤ', 'ฤdef', 'ฤ__', 'init', '__(', 'self', ',', 'ฤinput', '_', 'size', ',',
'ฤoutput', '_', 'size', '):', 'ฤฤฤฤฤฤฤฤ', 'ฤself', '.', 'weight', 'ฤ=', 'ฤtorch', '.', 'randn', '(', 'input', '_',
'size', ',', 'ฤoutput', '_', 'size', ')', 'ฤฤฤฤฤฤฤฤ', 'ฤself', '.', 'bias', 'ฤ=', 'ฤtorch', '.', 'zeros', '(',
'output', '_', 'size', ')', 'ฤฤฤฤฤ', 'ฤdef', 'ฤ__', 'call', '__(', 'self', ',', 'ฤx', '):', '
## ุญูุธ ุฃุฏุงุฉ ุชุญููู ุงูุฑููุฒ (tokenizer)

ููุชุฃูุฏ ูู ุฅููุงููุฉ ุงุณุชุฎุฏุงููุง ูุงุญููุงุ ูุญุชุงุฌ ุฅูู ุญูุธ ุฃุฏุงุฉ ุชุญููู ุงูุฑููุฒ ุงูุฌุฏูุฏุฉ ุงูุฎุงุตุฉ ุจูุง. ููุชู ุฐููุ ููุง ูู ุงูุญุงู ูุน ุงูููุงุฐุฌุ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

ุณูุคุฏู ูุฐุง ุฅูู ุฅูุดุงุก ูุฌูุฏ ุฌุฏูุฏ ุจุงุณู *code-search-net-tokenizer*ุ ูุงูุฐู ุณูุชุถูู ุฌููุน ุงููููุงุช ุงูุชู ุชุญุชุงุฌูุง ุฃุฏุงุฉ ุชุญููู ุงูุฑููุฒ ูุฅุนุงุฏุฉ ุชุญููููุง. ุฅุฐุง ููุช ุชุฑุบุจ ูู ูุดุงุฑูุฉ ูุฐู ุงูุฃุฏุงุฉ ูุน ุฒููุงุฆู ูุฃุตุฏูุงุฆูุ ููููู ุชุญููููุง ุฅูู ุงููุญููุฑ (Hub) ุนู ุทุฑูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจู. ูุฅุฐุง ููุช ุชุนูู ูู ุฏูุชุฑ ููุงุญุธุงุชุ ูููุงู ุฏุงูุฉ ููุงุฆูุฉ ููุณุงุนุฏุชู ูู ุฐูู:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ุณูุคุฏู ูุฐุง ุฅูู ุนุฑุถ ูุงูุฐุฉ ููุจุซูุฉ ุญูุซ ููููู ุฅุฏุฎุงู ุจูุงูุงุช ุงุนุชูุงุฏ ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face. ุฅุฐุง ูู ุชูู ุชุนูู ูู ุฏูุชุฑ ุงูููุงุญุธุงุชุ ููุง ุนููู ุณูู ูุชุงุจุฉ ุงูุณุทุฑ ุงูุชุงูู ูู ุทุฑููุฉ ุงูุญุงุณูุจ:

```bash
huggingface-cli login
```

ุจูุฌุฑุฏ ุชุณุฌูู ุงูุฏุฎููุ ููููู ุฅุฑุณุงู ุฃุฏุงุฉ ุชุญููู ุงูุฑููุฒ ุงูุฎุงุตุฉ ุจู ุนู ุทุฑูู ุชูููุฐ ุงูุฃูุฑ ุงูุชุงูู:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

ุณูุคุฏู ูุฐุง ุฅูู ุฅูุดุงุก ูุณุชูุฏุน ุฌุฏูุฏ ูู ูุณุงุญุฉ ุงูุงุณู ุงูุฎุงุตุฉ ุจู ุจุงุณู `code-search-net-tokenizer`ุ ูุงูุฐู ูุญุชูู ุนูู ููู ุฃุฏุงุฉ ุชุญููู ุงูุฑููุฒ. ุจุนุฏ ุฐููุ ููููู ุชุญููู ุฃุฏุงุฉ ุชุญููู ุงูุฑููุฒ ูู ุฃู ููุงู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `from_pretrained()`:

```py
# ุงุณุชุจุฏู "huggingface-course" ุฃุฏูุงู ุจูุทุงูู ุงููุนูู ูุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุชุญููู ุงูุฑููุฒ ุงูุฎุงุตุฉ ุจู
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

ูุงูุขูุ ุฃูุช ูุณุชุนุฏ ุชูุงููุง ูุชุฏุฑูุจ ูููุฐุฌ ูุบุฉ ูู ุงูุตูุฑ ูุชุนุฏููู ูููุงุณุจ ุงููููุฉ ุงูุชู ุจูู ูุฏูู! ุณูุชุทุฑู ุฅูู ุฐูู ูู [ุงููุตู 7](/course/chapter7)ุ ูููู ุฃููุงูุ ูู ุจููุฉ ูุฐุง ุงููุตูุ ุณูููู ูุธุฑุฉ ูุงุญุตุฉ ุนูู ุฃุฏูุงุช ุชุญููู ุงูุฑููุฒ ุงูุณุฑูุนุฉ (fast tokenizers) ูุณูุณุชูุดู ุจุงูุชูุตูู ูุง ูุญุฏุซ ุจุงููุนู ุนูุฏ ุงุณุชุฏุนุงุก ุทุฑููุฉ `train_new_from_iterator()`.