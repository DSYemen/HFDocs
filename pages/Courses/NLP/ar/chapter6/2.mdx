# ุชุฏุฑูุจ ูุญุฏุฏ ูููุงุช ุฌุฏูุฏ ูู ูุญุฏุฏ ูููุงุช ูุฏูู [[training-a-new-tokenizer-from-an-old-one]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

ุฅุฐุง ูู ููู ูููุฐุฌ ุงููุบุฉ ูุชุงุญูุง ุจุงููุบุฉ ุงูุชู ุชูุชู ุจูุงุ ุฃู ุฅุฐุง ูุงู ูุตู ูุฎุชูููุง ุฌุฏูุง ุนู ุงููุต ุงูุฐู ุชู ุชุฏุฑูุจ ูููุฐุฌ ุงููุบุฉ ุนูููุ ููู ุงููุญุชูู ุฃูู ุชุฑูุฏ ุฅุนุงุฏุฉ ุชุฏุฑูุจ ุงููููุฐุฌ ูู ุงูุตูุฑ ุจุงุณุชุฎุฏุงู ูุญุฏุฏ ูููุงุช ููุนุฏู ูุจูุงูุงุชู. ุณูุชุทูุจ ุฐูู ุชุฏุฑูุจ ูุญุฏุฏ ูููุงุช ุฌุฏูุฏ ุนูู ูุฌููุนุฉ ุจูุงูุงุชู. ูููู ูุง ูู ุงูููุตูุฏ ุจุฐูู ุจุงูุถุจุทุ ุนูุฏูุง ูุธุฑูุง ูุฃูู ูุฑุฉ ุฅูู ูุญุฏุฏุงุช ุงููููุงุช ูู [ุงููุตู 2](/course/chapter2)ุ ุฑุฃููุง ุฃู ูุนุธู ููุงุฐุฌ ุงููุญูู ุชุณุชุฎุฏู _ุฎูุงุฑุฒููุฉ ุชุฌุฒูุฆูุฉ ูุฑุนูุฉ_. ูุชุญุฏูุฏ ุงููููุงุช ุงููุฑุนูุฉ ุงูุชู ุชูู ูุชุธูุฑ ุจุดูู ูุชูุฑุฑ ูู ุงููุตุ ูุญุชุงุฌ ูุญุฏุฏ ุงููููุงุช ุฅูู ูุญุต ุฌููุน ุงููุตูุต ูู ุงููุต - ููู ุนูููุฉ ูุณูููุง *ุงูุชุฏุฑูุจ*. ุชุนุชูุฏ ุงูููุงุนุฏ ุงูุฏูููุฉ ุงูุชู ุชุญูู ูุฐุง ุงูุชุฏุฑูุจ ุนูู ููุน ูุญุฏุฏ ุงููููุงุช ุงููุณุชุฎุฏูุ ูุณููุฑ ุนุจุฑ ุงูุฎูุงุฑุฒููุงุช ุงูุฑุฆูุณูุฉ ุงูุซูุงุซุฉ ูุงุญููุง ูู ูุฐุง ุงููุตู.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

โ๏ธ ุชุฏุฑูุจ ูุญุฏุฏ ุงููููุงุช ููุณ ููุณ ุชุฏุฑูุจ ุงููููุฐุฌ! ูุณุชุฎุฏู ุชุฏุฑูุจ ุงููููุฐุฌ ุงููุฒูู ุงูุชุฏุฑูุฌู ุงูุนุดูุงุฆู ูุฌุนู ุงูุฎุณุงุฑุฉ ุฃุตุบุฑ ููููุงู ููู ุฏูุนุฉ. ุฅูู ุนุดูุงุฆู ุจุทุจูุนุชู (ููุง ูุนูู ุฃูู ุนููู ุชุนููู ุจุนุถ ุงูุจุฐูุฑ ููุญุตูู ุนูู ููุณ ุงููุชุงุฆุฌ ุนูุฏ ุฅุฌุฑุงุก ููุณ ุงูุชุฏุฑูุจ ูุฑุชูู). ุชุฏุฑูุจ ูุญุฏุฏ ุงููููุงุช ูู ุนูููุฉ ุฅุญุตุงุฆูุฉ ุชุญุงูู ุชุญุฏูุฏ ุงููููุงุช ุงููุฑุนูุฉ ุงูุฃูุถู ูุงุฎุชูุงุฑูุง ููุต ูุนููุ ูุงูููุงุนุฏ ุงูุฏูููุฉ ุงููุณุชุฎุฏูุฉ ูุงุฎุชูุงุฑูุง ุชุนุชูุฏ ุนูู ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒูุฆูุฉ. ุฅูู ุญุชููุ ููุง ูุนูู ุฃูู ุชุญุตู ุฏุงุฆููุง ุนูู ููุณ ุงููุชุงุฆุฌ ุนูุฏ ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ููุณ ุงูุฎูุงุฑุฒููุฉ ุนูู ููุณ ุงููุต.

</Tip>

## ุชุฌููุน ูุต [[assembling-a-corpus]]

ููุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช ุจุณูุทุฉ ุฌุฏูุง ูู ๐ค Transformers ููููู ุงุณุชุฎุฏุงููุง ูุชุฏุฑูุจ ูุญุฏุฏ ูููุงุช ุฌุฏูุฏ ุจููุณ ุฎุตุงุฆุต ูุญุฏุฏ ูููุงุช ููุฌูุฏ: `AutoTokenizer.train_new_from_iterator()`. ูุฑุคูุฉ ุฐูู ูู ุงูุนููุ ูููุชุฑุถ ุฃููุง ูุฑูุฏ ุชุฏุฑูุจ GPT-2 ูู ุงูุตูุฑุ ูููู ุจูุบุฉ ุฃุฎุฑู ุบูุฑ ุงูุฅูุฌููุฒูุฉ. ุณุชููู ูููุชูุง ุงูุฃููู ูู ุฌูุน ุงููุซูุฑ ูู ุงูุจูุงูุงุช ุจุชูู ุงููุบุฉ ูู ูุต ุชุฏุฑูุจู. ูุชูุฏูู ุฃูุซูุฉ ุณูููููุง ุงูุฌููุนุ ูู ูุณุชุฎุฏู ูุบุฉ ูุซู ุงูุฑูุณูุฉ ุฃู ุงูุตูููุฉ ููุงุ ูููู ุจุฏูุงู ูู ุฐูู ูุบุฉ ุฅูุฌููุฒูุฉ ูุชุฎุตุตุฉ: ููุฏ ุจุงูุซูู.

ูููู ูููุชุจุฉ [๐ค Datasets](https://github.com/huggingface/datasets) ูุณุงุนุฏุชูุง ูู ุชุฌููุน ูุต ูู ููุฏ ูุตุฏุฑ ุจุงูุซูู. ุณูุณุชุฎุฏู ุงูุฏุงูุฉ ุงููุนุชุงุฏุฉ `load_dataset()` ูุชูุฒูู ูุชุฎุฒูู ูุฌููุนุฉ ุจูุงูุงุช [CodeSearchNet](https://huggingface.co/datasets/code_search_net) ูุคูุชูุง. ุชู ุฅูุดุงุก ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุช ูุชุญุฏู [CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) ูุชุญุชูู ุนูู ููุงููู ุงูุฏูุงู ูู ููุชุจุงุช ุงููุตุงุฏุฑ ุงูููุชูุญุฉ ุนูู GitHub ุจุนุฏุฉ ูุบุงุช ุจุฑูุฌุฉ. ููุงุ ุณูููู ุจุชุญููู ุงูุฌุฒุก ุงูุฎุงุต ุจูุบุฉ ุจุงูุซูู ูู ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุช:

```py
from datasets import load_dataset

# ูุฏ ูุณุชุบุฑู ูุฐุง ุงูุชุญููู ุจุถุน ุฏูุงุฆูุ ูุฐุง ุงุญุตู ุนูู ูููุฉ ุฃู ุดุงู ุฃุซูุงุก ุงูุงูุชุธุงุฑ!
raw_datasets = load_dataset("code_search_net", "python")
```

ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ุงูุชูุณูู ุงูุชุฏุฑูุจู ููุนุฑูุฉ ุงูุฃุนูุฏุฉ ุงูุชู ูููููุง ุงููุตูู ุฅูููุง:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

ูููููุง ุฃู ูุฑู ุฃู ูุฌููุนุฉ ุงูุจูุงูุงุช ุชูุตู ุงููุซุงุฆู ุงูุชูุถูุญูุฉ ุนู ุงูููุฏ ูุชูุชุฑุญ ุชุฌุฒูุฆูุฉ ููููููุง. ููุงุ ุณูุณุชุฎุฏู ุนููุฏ `whole_func_string` ููุท ูุชุฏุฑูุจ ูุญุฏุฏ ุงููููุงุช ุงูุฎุงุต ุจูุง. ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ูุซุงู ูุงุญุฏ ูู ูุฐู ุงูุฏูุงู ุนู ุทุฑูู ุงูููุฑุณุฉ ูู ุงูุชูุณูู ุงูุชุฏุฑูุจู:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

ุงูุฐู ูุฌุจ ุฃู ูุทุจุน ูุง ููู:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

ุฃูู ุดูุก ูุญุชุงุฌ ุฅูู ูุนูู ูู ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู _ููุฑุฑ_ ูููุงุฆู ุงููุตูุต - ุนูู ุณุจูู ุงููุซุงูุ ูุงุฆูุฉ ูู ููุงุฆู ุงููุตูุต. ุณุชููููุง ููุงุฆู ุงููุตูุต ูู ุฌุนู ูุญุฏุฏ ุงููููุงุช ุฃุณุฑุน (ุงูุชุฏุฑูุจ ุนูู ุฏูุนุงุช ูู ุงููุตูุต ุจุฏูุงู ูู ูุนุงูุฌุฉ ุงููุตูุต ุงููุฑุฏูุฉ ูุงุญุฏุฉ ุชูู ุงูุฃุฎุฑู)ุ ููุฌุจ ุฃู ูููู ููุฑุฑูุง ุฅุฐุง ุฃุฑุฏูุง ุชุฌูุจ ูุฌูุฏ ูู ุดูุก ูู ุงูุฐุงูุฑุฉ ูู ููุณ ุงูููุช. ุฅุฐุง ูุงู ูุตู ุถุฎููุงุ ูุณุชูุฏ ุงูุงุณุชูุงุฏุฉ ูู ุญูููุฉ ุฃู ๐ค Datasets ูุง ูุญูู ูู ุดูุก ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู ูููู ูุฎุฒู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุนูู ุงููุฑุต.

ุณูุนูู ูุนู ูุง ููู ุนูู ุฅูุดุงุก ูุงุฆูุฉ ูู ููุงุฆู 1000 ูุต ููู ูููุงุ ูููู ุณูุญูู ูู ุดูุก ูู ุงูุฐุงูุฑุฉ:

```py
# ูุง ุชูู ุจุฅูุบุงุก ุชุนููู ุงูุณุทุฑ ุงูุชุงูู ุฅูุง ุฅุฐุง ูุงูุช ูุฌููุนุฉ ุจูุงูุงุชู ุตุบูุฑุฉ!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

ุจุงุณุชุฎุฏุงู ูููุฏ ุจุงูุซููุ ูููููุง ุชุฌูุจ ุชุญููู ุจุงูุซูู ูุฃู ุดูุก ูู ุงูุฐุงูุฑุฉ ุญุชู ูููู ุฐูู ุถุฑูุฑููุง ุจุงููุนู. ูุฅูุดุงุก ูุซู ูุฐุง ุงููููุฏุ ุชุญุชุงุฌ ููุท ุฅูู ุงุณุชุจุฏุงู ุงูุฃููุงุณ ุจุงูุฃููุงุณ:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

ูุฐุง ุงูุณุทุฑ ูู ุงูููุฏ ูุง ูุณุชุฑุฏ ุฃู ุนูุงุตุฑ ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ุฅูู ููุท ููุดุฆ ูุงุฆู ููููู ุงุณุชุฎุฏุงูู ูู ุญููุฉ `for` ุจุงูุซูู. ูู ูุชู ุชุญููู ุงููุตูุต ุฅูุง ุนูุฏ ุงูุญุงุฌุฉ ุฅูููุง (ุฃู ุนูุฏูุง ุชููู ูู ุฎุทูุฉ ุญููุฉ `for` ุงูุชู ุชุชุทูุจูุง)ุ ูุณูุชู ุชุญููู 1000 ูุต ููุท ูู ูู ูุฑุฉ. ุจูุฐู ุงูุทุฑููุฉ ูู ุชุณุชููุฏ ูู ุฐุงูุฑุชู ุญุชู ุฅุฐุง ููุช ุชุนุงูุฌ ูุฌููุนุฉ ุจูุงูุงุช ุถุฎูุฉ.

ุงููุดููุฉ ูุน ูุงุฆู ุงููููุฏ ูู ุฃูู ูููู ุงุณุชุฎุฏุงูู ูุฑุฉ ูุงุญุฏุฉ ููุท. ูุฐุงุ ุจุฏูุงู ูู ุฅุนุทุงุฆูุง ูุงุฆูุฉ ุจุฃูู 10 ุฃุฑูุงู ูุฑุชูู:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

ูุญุตู ุนูููุง ูุฑุฉ ูุงุญุฏุฉ ุซู ูุงุฆูุฉ ูุงุฑุบุฉ:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

ููุฐุง ุงูุณุจุจ ูุญุฏุฏ ุฏุงูุฉ ุชุนูุฏ ูููุฏูุง ุจุฏูุงู ูู ุฐูู:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

ููููู ุฃูุถูุง ุชุญุฏูุฏ ูููุฏู ุฏุงุฎู ุญููุฉ `for` ุจุงุณุชุฎุฏุงู ุนุจุงุฑุฉ `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

ูุงูุฐู ุณููุชุฌ ุนูู ููุณ ุงููููุฏ ุจุงูุถุจุท ููุง ูุงู ูู ูุจูุ ููููู ูุณูุญ ูู ุจุงุณุชุฎุฏุงู ููุทู ุฃูุซุฑ ุชุนููุฏูุง ููุง ููููู ูู ุชุนุจูุฑ ูุงุฆูุฉ ุงูููู.

## ุชุฏุฑูุจ ูุญุฏุฏ ูููุงุช ุฌุฏูุฏ [[training-a-new-tokenizer]]

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญ ูุฏููุง ูุตูุง ูู ุดูู ููุฑุฑ ูู ุฏูุนุงุช ุงููุตูุตุ ูุญู ูุณุชุนุฏูู ูุชุฏุฑูุจ ูุญุฏุฏ ูููุงุช ุฌุฏูุฏ. ููููุงู ุจุฐููุ ูุญุชุงุฌ ุฃููุงู ุฅูู ุชุญููู ูุญุฏุฏ ุงููููุงุช ุงูุฐู ูุฑูุฏ ุฅูุฑุงูู ุจูููุฐุฌูุง (ููุงุ GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ุนูู ุงูุฑุบู ูู ุฃููุง ุณูููู ุจุชุฏุฑูุจ ูุญุฏุฏ ูููุงุช ุฌุฏูุฏุ ููู ุงูุฌูุฏ ูุนู ุฐูู ูุชุฌูุจ ุงูุจุฏุก ูู ุงูุตูุฑ ุชูุงููุง. ุจูุฐู ุงูุทุฑููุฉุ ูู ูุถุทุฑ ุฅูู ุชุญุฏูุฏ ุฃู ุดูุก ุนู ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒูุฆูุฉ ุฃู ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุฑูุฏ ุงุณุชุฎุฏุงููุงุ ุณูููู ูุญุฏุฏ ูููุงุชูุง ุงูุฌุฏูุฏ ูุทุงุจููุง ุชูุงููุง ูู GPT-2ุ ูุงูุดูุก ุงููุญูุฏ ุงูุฐู ุณูุชุบูุฑ ูู ุงูููุฑุฏุงุชุ ูุงูุชู ุณูุชู ุชุญุฏูุฏูุง ูู ุฎูุงู ุงูุชุฏุฑูุจ ุนูู ูุตูุง.

ุฃููุงู ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ููููุฉ ูุนุงููุฉ ูุฐุง ุงููุญุฏุฏ ููููุงุช ูุฏุงูุฉ ูุซุงู:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```python
['def', 'ฤadd', '_', 'n', 'umbers', '(', 'a', ',', 'ฤb', '):', 'ฤ', 'ฤ', 'ฤ', 'ฤ', 'ฤ"""', 'Add', 'ฤthe', 'ฤtwo',
 'ฤnumbers', 'ฤ`', 'a', '`', 'ฤand', 'ฤ`', 'b', '`', '."', '""', 'ฤ', 'ฤ', 'ฤ', 'ฤ', 'ฤreturn', 'ฤa', 'ฤ+', 'ฤb']
```

ูุญุชูู ูุฐุง ุงููุญูู ุงูุฑูุฒู ุนูู ุจุนุถ ุงูุฑููุฒ ุงูุฎุงุตุฉุ ูุซู `ฤ` ู`ฤ`ุ ูุงูุชู ุชุดูุฑ ุฅูู ุงููุณุงูุงุช ูุงูุฃุณุทุฑ ุงูุฌุฏูุฏุฉ ุนูู ุงูุชูุงูู. ููุง ูุฑูุ ูุฐุง ููุณ ูุนุงููุง ููุบุงูุฉ: ูููู ุงููุญูู ุงูุฑูุฒู ุจุฅุฑุฌุงุน ุฑููุฒ ูุฑุฏูุฉ ููู ูุณุงูุฉุ ุนูุฏูุง ููููู ุชุฌููุน ูุณุชููุงุช ุงููุณุงูุฉ ุงูุจุงุฏุฆุฉ ูุนูุง (ูุธุฑูุง ูุฃู ูุฌูุฏ ูุฌููุนุงุช ูู ุฃุฑุจุน ุฃู ุซูุงูู ูุณุงูุงุช ุณูููู ุดุงุฆุนูุง ุฌุฏูุง ูู ุงูููุฏ). ููุง ูุงู ุจุชูุณูู ุงุณู ุงูุฏุงูุฉ ุจุทุฑููุฉ ุบุฑูุจุฉุ ุบูุฑ ูุนุชุงุฏ ุนูู ุฑุคูุฉ ูููุงุช ุชุญุชูู ุนูู ุญุฑู `_`.

ุฏุนูุง ูููู ุจุชุฏุฑูุจ ูุญูู ุฑูุฒู ุฌุฏูุฏ ููุฑู ุฅุฐุง ูุงู ุณูุญู ูุฐู ุงููุดููุงุช. ููุฐุงุ ุณูุณุชุฎุฏู ุงูุทุฑููุฉ `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

ูุฏ ูุณุชุบุฑู ูุฐุง ุงูุฃูุฑ ุจุนุถ ุงูููุช ุฅุฐุง ูุงู ูุตู ูุจูุฑูุง ุฌุฏูุงุ ูููู ุจุงููุณุจุฉ ููุฌููุนุฉ ุงูุจูุงูุงุช ูุฐู ุงูุชู ุชุจูุบ 1.6 ุฌูุฌุงุจุงูุช ูู ุงููุตูุตุ ููู ุณุฑูุน ููุบุงูุฉ (ุฏูููุฉ ูุงุญุฏุฉ ู16 ุซุงููุฉ ุนูู ูุนุงูุฌ AMD Ryzen 9 3900X ุจูุนุงูุฌ 12 ููุงุฉ).

ูุงุญุธ ุฃู `AutoTokenizer.train_new_from_iterator()` ูุนูู ููุท ุฅุฐุง ูุงู ุงููุญูู ุงูุฑูุฒู ุงูุฐู ุชุณุชุฎุฏูู ูู ูุญูู ุฑูุฒู "ุณุฑูุน". ููุง ุณุชุฑู ูู ุงููุณู ุงูุชุงููุ ุชุญุชูู ููุชุจุฉ ๐ค Transformers ุนูู ููุนูู ูู ุงููุญููุงุช ุงูุฑูุฒูุฉ: ุจุนุถูุง ููุชูุจ ุจุงููุงูู ูู ุจุงูุซูู ูุงูุจุนุถ ุงูุขุฎุฑ (ุงูุณุฑูุน) ูุฏุนูู ุจููุชุจุฉ ๐ค Tokenizersุ ูุงูุชู ููุชูุจุฉ ุจูุบุฉ [Rust](https://www.rust-lang.org) ุงูุจุฑูุฌูุฉ. ุจุงูุซูู ูู ุงููุบุฉ ุงูุฃูุซุฑ ุงุณุชุฎุฏุงููุง ูู ุนูู ุงูุจูุงูุงุช ูุชุทุจููุงุช ุงูุชุนูู ุงูุนูููุ ูููู ุนูุฏูุง ูุญุชุงุฌ ุฃู ุดูุก ุฅูู ุฃู ูููู ููุงุฒููุง ููููู ุณุฑูุนูุงุ ูุฌุจ ูุชุงุจุชู ุจูุบุฉ ุฃุฎุฑู. ุนูู ุณุจูู ุงููุซุงูุ ุนูููุงุช ุงูุถุฑุจ ุงููุตูููุฉ ุงูุชู ูู ูู ููุจ ุญุณุงุจ ุงููููุฐุฌ ููุชูุจุฉ ูู CUDAุ ููู ููุชุจุฉ C ูุญุณูุฉ ููุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช.

ุณูููู ุชุฏุฑูุจ ูุญูู ุฑูุฒู ุฌุฏูุฏ ูู ุจุงูุซูู ุจุทูุฆูุง ููุบุงูุฉุ ููู ุงูุณุจุจ ูู ุชุทููุฑูุง ูููุชุจุฉ ๐ค Tokenizers. ูุงุญุธ ุฃูู ููุง ูู ููู ุนููู ุชุนูู ูุบุฉ CUDA ููู ุชุชููู ูู ุชูููุฐ ูููุฐุฌู ุนูู ุฏูุนุฉ ูู ุงูุฅุฏุฎุงูุงุช ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุชุ ููู ุชุญุชุงุฌ ุฅูู ุชุนูู Rust ูุงุณุชุฎุฏุงู ูุญูู ุฑูุฒู ุณุฑูุน. ุชููุฑ ููุชุจุฉ ๐ค Tokenizers ุฑูุงุจุท ุจุงูุซูู ููุนุฏูุฏ ูู ุงูุทุฑู ุงูุชู ุชุณุชุฏุนู ุฏุงุฎูููุง ูุทุนุฉ ูู ุงูููุฏ ูู Rustุ ุนูู ุณุจูู ุงููุซุงูุ ูุชูุงุฒู ุชุฏุฑูุจ ูุญูู ุฑูุฒู ุงูุฌุฏูุฏ ุงูุฎุงุต ุจู ุฃูุ ููุง ุฑุฃููุง ูู [ุงููุตู 3](/course/chapter3)ุ ุชุญููู ุฏูุนุฉ ูู ุงูุฅุฏุฎุงูุงุช.

ูุนุธู ููุงุฐุฌ ุงููุญูู ูุฏููุง ูุญูู ุฑูุฒู ุณุฑูุน ูุชุงุญ (ููุงู ุจุนุถ ุงูุงุณุชุซูุงุกุงุช ุงูุชู ููููู ุงูุชุญูู ูููุง [ููุง](https://huggingface.co/transformers/#supported-frameworks))ุ ููููุฑ ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `AutoTokenizer` ุฏุงุฆููุง ุงููุญูู ุงูุฑูุฒู ุงูุณุฑูุน ูู ุฅุฐุง ูุงู ูุชุงุญูุง. ูู ุงููุณู ุงูุชุงููุ ุณูููู ูุธุฑุฉ ุนูู ุจุนุถ ุงูููุฒุงุช ุงูุฎุงุตุฉ ุงูุฃุฎุฑู ุงูุชู ุชูุชูููุง ุงููุญููุงุช ุงูุฑูุฒูุฉ ุงูุณุฑูุนุฉุ ูุงูุชู ุณุชููู ูููุฏุฉ ููุบุงูุฉ ูููุงู ูุซู ุชุตููู ุงูุฑููุฒ ูุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ. ูุจู ุงูุบูุต ูู ุฐููุ ููุน ุฐููุ ุฏุนูุง ูุฌุฑุจ ูุญูููุง ุงูุฑูุฒู ุงูุฌุฏูุฏ ุนูู ุงููุซุงู ุงูุณุงุจู:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ฤadd', '_', 'numbers', '(', 'a', ',', 'ฤb', '):', 'ฤฤฤฤ', 'ฤ"""', 'Add', 'ฤthe', 'ฤtwo', 'ฤnumbers', 'ฤ`',
 'a', '`', 'ฤand', 'ฤ`', 'b', '`."""', 'ฤฤฤฤ', 'ฤreturn', 'ฤa', 'ฤ+', 'ฤb']
```

ููุง ูุฑู ูุฑุฉ ุฃุฎุฑู ุงูุฑููุฒ ุงูุฎุงุตุฉ `ฤ` ู`ฤ` ุงูุชู ุชุดูุฑ ุฅูู ุงููุณุงูุงุช ูุงูุฃุณุทุฑ ุงูุฌุฏูุฏุฉุ ูููู ูููููุง ุฃูุถูุง ุฃู ูุฑู ุฃู ูุญูููุง ุงูุฑูุฒู ุชุนูู ุจุนุถ ุงูุฑููุฒ ุงูุชู ุชุฎุต ูุตูุง ูู ุฏูุงู ุจุงูุซูู: ุนูู ุณุจูู ุงููุซุงูุ ููุงู ุฑูุฒ `ฤฤฤฤ` ุงูุฐู ููุซู ูุณุงูุฉ ุจุงุฏุฆุฉุ ูุฑูุฒ `ฤ"""` ุงูุฐู ููุซู ุนูุงูุงุช ุงูุงูุชุจุงุณ ุงูุซูุงุซุฉ ุงูุชู ุชุจุฏุฃ ุณูุณูุฉ ุงูุชูุซูู. ูุงู ุงููุญูู ุงูุฑูุฒู ุฃูุถูุง ุจุชูุณูู ุงุณู ุงูุฏุงูุฉ ุจุดูู ุตุญูุญ ุนูู `_`. ูุฐู ุชูุซูู ูุถุบูุท ููุบุงูุฉุ ุจุงูููุงุฑูุฉุ ุจุงุณุชุฎุฏุงู ุงููุญูู ุงูุฑูุฒู ุงูุฅูุฌููุฒู ุงูุนุงุฏู ุนูู ููุณ ุงููุซุงู ุณูุนุทููุง ุฌููุฉ ุฃุทูู:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

ุฏุนูุง ููุธุฑ ุฅูู ูุซุงู ุขุฎุฑ:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ฤLinear', 'Layer', '():', 'ฤฤฤฤ', 'ฤdef', 'ฤ__', 'init', '__(', 'self', ',', 'ฤinput', '_', 'size', ',',
 'ฤoutput', '_', 'size', '):', 'ฤฤฤฤฤฤฤฤ', 'ฤself', '.', 'weight', 'ฤ=', 'ฤtorch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'ฤoutput', '_', 'size', ')', 'ฤฤฤฤฤฤฤฤ', 'ฤself', '.', 'bias', 'ฤ=', 'ฤtorch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ฤฤฤฤฤ', 'ฤdef', 'ฤ__', 'call', '__(', 'self', ',', 'ฤx', '):', 'ฤฤฤฤฤฤฤฤ',
 'ฤreturn', 'ฤx', 'ฤ@', 'ฤself', '.', 'weights', 'ฤ+', 'ฤself', '.', 'bias', 'ฤฤฤฤฤ']
```

ุจุงูุฅุถุงูุฉ ุฅูู ุงูุฑูุฒ ุงูููุงุจู ููุณุงูุฉ ุงูุจุงุฏุฆุฉุ ููุง ูููููุง ุฃูุถูุง ุฑุคูุฉ ุฑูุฒ ููุณุงูุฉ ุจุงุฏุฆุฉ ูุฒุฏูุฌุฉ: `ฤฤฤฤฤฤฤฤ`. ูุชู ุชุญููู ุงููููุงุช ุงูุฎุงุตุฉ ูู ุจุงูุซูู ูุซู `class` ู`init` ู`call` ู`self` ู`return` ูุฑูุฒ ูุงุญุฏุ ููููููุง ุฃู ูุฑู ุฃูุถูุง ุฃูู ุจุงูุฅุถุงูุฉ ุฅูู ุงูุชูุณูู ุนูู `_` ู`.`ุ ูููู ุงููุญูู ุงูุฑูุฒู ุจุชูุณูู ุงูุฃุณูุงุก ุงูููุชูุจุฉ ุจุทุฑููุฉ CamelCase ุจุดูู ุตุญูุญ: ูุชู ุชุญููู `LinearLayer` ูู `["ฤLinear", "Layer"]`.

## ุญูุธ ุงููุญูู ุงูุฑูุฒู [[saving-the-tokenizer]]

ููุชุฃูุฏ ูู ุฃููุง ูููููุง ุงุณุชุฎุฏุงูู ูุงุญููุงุ ูุญุชุงุฌ ุฅูู ุญูุธ ูุญูููุง ุงูุฑูุฒู ุงูุฌุฏูุฏ. ูุซู ุงูููุงุฐุฌุ ูุชู ุฐูู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

ุณูุคุฏู ูุฐุง ุฅูู ุฅูุดุงุก ูุฌูุฏ ุฌุฏูุฏ ูุณูู *code-search-net-tokenizer*ุ ูุงูุฐู ุณูุญุชูู ุนูู ุฌููุน ุงููููุงุช ุงูุชู ูุญุชุงุฌูุง ุงููุญูู ุงูุฑูุฒู ูุฅุนุงุฏุฉ ุงูุชุญููู. ุฅุฐุง ููุช ุชุฑุบุจ ูู ูุดุงุฑูุฉ ูุฐุง ุงููุญูู ุงูุฑูุฒู ูุน ุฒููุงุฆู ูุฃุตุฏูุงุฆูุ ููููู ุชุญูููู ุนูู Hub ุนู ุทุฑูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจู. ุฅุฐุง ููุช ุชุนูู ูู ุฏูุชุฑ ููุงุญุธุงุชุ ูููุงู ูุธููุฉ ููุงุฆูุฉ ููุณุงุนุฏุชู ูู ุฐูู:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ุณูุชู ุนุฑุถ ุฃุฏุงุฉ ููููู ูู ุฎูุงููุง ุฅุฏุฎุงู ุจูุงูุงุช ุงุนุชูุงุฏ ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face. ุฅุฐุง ูู ุชูู ุชุนูู ูู ุฏูุชุฑ ููุงุญุธุงุชุ ููุง ุนููู ุณูู ูุชุงุจุฉ ุงูุณุทุฑ ุงูุชุงูู ูู ุทุฑููุชู:

```bash
huggingface-cli login
```

ุจูุฌุฑุฏ ุชุณุฌูู ุงูุฏุฎููุ ููููู ุฏูุน ูุญููู ุงูุฑูุฒู ุนู ุทุฑูู ุชูููุฐ ุงูุฃูุฑ ุงูุชุงูู:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

ุณูุคุฏู ูุฐุง ุฅูู ุฅูุดุงุก ูุณุชูุฏุน ุฌุฏูุฏ ูู ูุณุงุญุฉ ุงุณูู ุจุงุณู `code-search-net-tokenizer`ุ ูุญุชูู ุนูู ููู ุงููุญูู ุงูุฑูุฒู. ุจุนุฏ ุฐููุ ููููู ุชุญููู ุงููุญูู ุงูุฑูุฒู ูู ุฃู ููุงู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `from_pretrained()`:

```py
# ุงุณุชุจุฏู "huggingface-course" ุฃุฏูุงู ุจูุณุงุญุฉ ุงุณูู ุงููุนููุฉ ูุงุณุชุฎุฏุงู ูุญููู ุงูุฑูุฒู ุงูุฎุงุต ุจู
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

ุฃูุช ุงูุขู ุฌุงูุฒ ูุชุฏุฑูุจ ูููุฐุฌ ูุบุฉ ูู ุงูุตูุฑ ูุถุจุทู ุงูุฏููู ุนูู ุงููููุฉ ุงูุชู ุจูู ูุฏูู! ุณูุตู ุฅูู ุฐูู ูู [ุงููุตู 7](/course/chapter7)ุ ูููู ุฃููุงูุ ูู ุจููุฉ ูุฐุง ุงููุตูุ ุณูููู ูุธุฑุฉ ูุงุญุตุฉ ุนูู ุงููุญููุงุช ุงูุฑูุฒูุฉ ุงูุณุฑูุนุฉ ูุงุณุชูุดุงู ูุง ูุญุฏุซ ุจุงููุนู ุนูุฏ ุงุณุชุฏุนุงุก ุทุฑููุฉ `train_new_from_iterator()`.