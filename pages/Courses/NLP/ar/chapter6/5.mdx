# ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช [[byte-pair-encoding-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
]} />

ุชู ุชุทููุฑ ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช (BPE) ูู ุงูุจุฏุงูุฉ ูุฎูุงุฑุฒููุฉ ูุถุบุท ุงููุตูุตุ ุซู ุงุณุชุฎุฏูุชู OpenAI ููุชุฑููุฒ ุนูุฏ ุงูุชุฏุฑูุจ ุงููุณุจู ููููุฐุฌ GPT. ููุณุชุฎุฏูู ุงููุซูุฑ ูู ููุงุฐุฌ ุงููุญููุ ุจูุง ูู ุฐูู GPT ูGPT-2 ูRoBERTa ูBART ูDeBERTa.

<Youtube id="HEikzVL-lZU"/>

<Tip>

๐ก ูุบุทู ูุฐุง ุงููุณู ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช ุจุนููุ ูุตููุงู ุฅูู ุนุฑุถ ุงูุชูููุฐ ุงููุงูู. ููููู ุชุฎุทู ูุฐุง ุงููุณู ูุงูุงูุชูุงู ุฅูู ุงูููุงูุฉ ุฅุฐุง ููุช ุชุฑูุฏ ููุท ูุธุฑุฉ ุนุงูุฉ ุนูู ุฎูุงุฑุฒููุฉ ุงูุชุฑููุฒ.

</Tip>

## ุฎูุงุฑุฒููุฉ ุงูุชุฏุฑูุจ [[training-algorithm]]

ูุจุฏุฃ ุชุฏุฑูุจ ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช ุจุญุณุงุจ ูุฌููุนุฉ ุงููููุงุช ุงููุฑูุฏุฉ ุงููุณุชุฎุฏูุฉ ูู ุงูููุฑุณ (ุจุนุฏ ุงูุชูุงู ุฎุทูุงุช ุงูุชุทุจูุน ูุงูุชุฑููุฒ ุงููุณุจู)ุ ุซู ุจูุงุก ุงูููุฑุฏุงุช ุนู ุทุฑูู ุฃุฎุฐ ุฌููุน ุงูุฑููุฒ ุงููุณุชุฎุฏูุฉ ููุชุงุจุฉ ุชูู ุงููููุงุช. ููุซุงู ุจุณูุท ููุบุงูุฉุ ูููุชุฑุถ ุฃู ููุฑุณูุง ูุณุชุฎุฏู ูุฐู ุงููููุงุช ุงูุฎูุณ:

```
"hug", "pug", "pun", "bun", "hugs"
```

ุณุชููู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ ุญูููุง ูู `["b", "g", "h", "n", "p", "s", "u"]`. ูู ุงูุญุงูุงุช ุงููุงูุนูุฉุ ุณุชุชุถูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ ุฌููุน ุฃุญุฑู ASCII ุนูู ุงูุฃููุ ูุฑุจูุง ุจุนุถ ุฃุญุฑู Unicode ุฃูุถูุง. ุฅุฐุง ูุงู ุงููุซุงู ุงูุฐู ุชููู ุจุชุฑููุฒู ูุณุชุฎุฏู ุญุฑููุง ุบูุฑ ููุฌูุฏ ูู ุงูููุฑุณ ุงูุชุฏุฑูุจูุ ูุณูุชู ุชุญููู ุฐูู ุงูุญุฑู ุฅูู ุงูุฑูุฒ ุงููุฌููู. ูุฐุง ุฃุญุฏ ุงูุฃุณุจุงุจ ุงูุชู ุชุฌุนู ุงูุนุฏูุฏ ูู ููุงุฐุฌ NLP ุณูุฆุฉ ููุบุงูุฉ ูู ุชุญููู ุงููุญุชูู ูุน ุงูุฑููุฒ ุงูุชุนุจูุฑูุฉุ ุนูู ุณุจูู ุงููุซุงู.

<Tip>

ูุฏู ูุญูู ุงูุชุฑููุฒ GPT-2 ูRoBERTa (ูููุง ูุชุดุงุจูุงู ุฅูู ุญุฏ ูุจูุฑ) ุทุฑููุฉ ุฐููุฉ ููุชุนุงูู ูุน ูุฐุง: ูุง ููุธุฑูู ุฅูู ุงููููุงุช ุนูู ุฃููุง ููุชูุจุฉ ุจุญุฑูู Unicodeุ ูููู ุจุงูุจุงูุชุงุช. ุจูุฐู ุงูุทุฑููุฉุ ูููู ุญุฌู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ ุตุบูุฑูุง (256)ุ ูููู ุณูุชู ุชุถููู ูู ุญุฑู ููููู ุงูุชูููุฑ ููู ููู ููุชูู ุจู ุงูุฃูุฑ ุฅูู ุงูุชุญููู ุฅูู ุงูุฑูุฒ ุงููุฌููู. ุชุณูู ูุฐู ุงูุญููุฉ *ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช ุนูู ูุณุชูู ุงูุจุงูุช*.

</Tip>

ุจุนุฏ ุงูุญุตูู ุนูู ูุฐู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉุ ูุถูู ุฑููุฒูุง ุฌุฏูุฏุฉ ุญุชู ูุชู ุงููุตูู ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุทููุจ ุนู ุทุฑูู ุชุนูู *ุงูุฏูุฌ*ุ ููู ููุงุนุฏ ูุฏูุฌ ุนูุตุฑูู ูู ุงูููุฑุฏุงุช ุงูููุฌูุฏุฉ ูุนูุง ูู ุนูุตุฑ ุฌุฏูุฏ. ูุฐููุ ูู ุงูุจุฏุงูุฉุ ุณุชุฎูู ุนูููุงุช ุงูุฏูุฌ ูุฐู ุฑููุฒูุง ุฐุงุช ุญุฑูููุ ุซูุ ูุน ุชูุฏู ุงูุชุฏุฑูุจุ ูููุงุช ูุฑุนูุฉ ุฃุทูู.

ูู ุฃู ุฎุทูุฉ ุฃุซูุงุก ุชุฏุฑูุจ ุงููุญููุ ุณูุจุญุซ ุฎูุงุฑุฒููุฉ ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช ุนู ุฃูุซุฑ ุฃุฒูุงุฌ ุงูุฑููุฒ ุงูููุฌูุฏุฉ ุชูุฑุงุฑูุง (ุจู "ุฒูุฌ"ุ ููุตุฏ ููุง ุฑูุฒูู ูุชุชุงูููู ูู ูููุฉ). ุณูููู ูุฐุง ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุง ูู ุงูุฐู ุณูุชู ุฏูุฌูุ ูููุฑุฑ ุงูุนูููุฉ ููุฎุทูุฉ ุงูุชุงููุฉ.

ุจุงูุนูุฏุฉ ุฅูู ูุซุงููุง ุงูุณุงุจูุ ูููุชุฑุถ ุฃู ุงููููุงุช ูุงูุช ููุง ุงูุชุฑุฏุฏุงุช ุงูุชุงููุฉ:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ููุง ูุนูู ุฃู ุงููููุฉ `"hug"` ูุงูุช ููุฌูุฏุฉ 10 ูุฑุงุช ูู ุงูููุฑุณุ ู `"pug"` 5 ูุฑุงุชุ ู `"pun"` 12 ูุฑุฉุ ู `"bun"` 4 ูุฑุงุชุ ู `"hugs"` 5 ูุฑุงุช. ูุจุฏุฃ ุงูุชุฏุฑูุจ ุจุชูุณูู ูู ูููุฉ ุฅูู ุญุฑูู (ุงูุชู ุชุดูู ููุฑุฏุงุชูุง ุงูุฃูููุฉ) ุจุญูุซ ูููููุง ุฑุคูุฉ ูู ูููุฉ ุนูู ุฃููุง ูุงุฆูุฉ ูู ุงูุฑููุฒ:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ุซู ููุธุฑ ุฅูู ุงูุฃุฒูุงุฌ. ุงูุฒูุฌ `("h", "u")` ููุฌูุฏ ูู ุงููููุชูู `"hug"` ู `"hugs"`ุ ุฃู 15 ูุฑุฉ ูู ุงููุฌููุน ูู ุงูููุฑุณ. ูููู ููุณ ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุง: ูุฐุง ุงูุดุฑู ูุนูุฏ ุฅูู ุงูุฒูุฌ `("u", "g")`ุ ุงูููุฌูุฏ ูู ุงููููุงุช `"hug"` ู `"pug"` ู `"hugs"`ุ ุจูุฌููุน 20 ูุฑุฉ ูู ุงูููุฑุณ.

ูุจุงูุชุงููุ ุชููู ูุงุนุฏุฉ ุงูุฏูุฌ ุงูุฃููู ุงูุชู ูุชุนูููุง ุงููุญูู ูู `("u", "g") -> "ug"`ุ ููุง ูุนูู ุฃูู ุณูุชู ุฅุถุงูุฉ `"ug"` ุฅูู ุงูููุฑุฏุงุชุ ููุฌุจ ุฏูุฌ ุงูุฒูุฌ ูู ุฌููุน ูููุงุช ุงูููุฑุณ. ูู ููุงูุฉ ูุฐู ุงููุฑุญูุฉุ ุชุจุฏู ุงูููุฑุฏุงุช ูุงูููุฑุณ ุนูู ุงููุญู ุงูุชุงูู:

```
ุงูููุฑุฏุงุช: ["b", "g", "h", "n", "p", "s", "u", "ug"]
ุงูููุฑุณ: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

ุงูุขู ูุฏููุง ุจุนุถ ุงูุฃุฒูุงุฌ ุงูุชู ุชุคุฏู ุฅูู ุฑูุฒ ุฃุทูู ูู ุญุฑููู: ุงูุฒูุฌ `("h", "ug")`ุ ุนูู ุณุจูู ุงููุซุงู (ููุฌูุฏ 15 ูุฑุฉ ูู ุงูููุฑุณ). ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุง ูู ูุฐู ุงููุฑุญูุฉ ูู `("u", "n")`ุ ููุน ุฐููุ ููุฌูุฏ 16 ูุฑุฉ ูู ุงูููุฑุณุ ูุฐุง ุชููู ูุงุนุฏุฉ ุงูุฏูุฌ ุงูุซุงููุฉ ุงูุชู ูุชู ุชุนูููุง ูู `("u", "n") -> "un"`. ุฅุถุงูุฉ ุฐูู ุฅูู ุงูููุฑุฏุงุช ูุฏูุฌ ุฌููุน ุงูุชูุงุฌุฏุงุช ุงูููุฌูุฏุฉ ูุคุฏู ุฅูู:

```
ุงูููุฑุฏุงุช: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
ุงูููุฑุณ: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

ุงูุขู ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุง ูู `("h", "ug")`ุ ูุฐุง ูุชุนูู ูุงุนุฏุฉ ุงูุฏูุฌ `("h", "ug") -> "hug"`ุ ูุงูุชู ุชุนุทููุง ุฃูู ุฑูุฒ ูููู ูู ุซูุงุซุฉ ุฃุญุฑู. ุจุนุฏ ุงูุฏูุฌุ ูุจุฏู ุงูููุฑุณ ุนูู ุงููุญู ุงูุชุงูู:

```
ุงูููุฑุฏุงุช: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
ุงูููุฑุณ: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

ููุณุชูุฑ ุนูู ูุฐุง ุงููุญู ุญุชู ูุตู ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุทููุจ.

<Tip>

โ๏ธ **ุงูุขู ุฏูุฑู!** ูุง ูู ูุงุนุฏุฉ ุงูุฏูุฌ ุงูุชุงููุฉ ุจุฑุฃููุ

</Tip>

## ุฎูุงุฑุฒููุฉ ุงูุชุฑููุฒ [[tokenization-algorithm]]

ูุชุจุน ุงูุชุฑููุฒ ุนูููุฉ ุงูุชุฏุฑูุจ ุนู ูุซุจุ ุจูุนูู ุฃู ุงูุฅุฏุฎุงูุงุช ุงูุฌุฏูุฏุฉ ูุชู ุชุฑููุฒูุง ุนู ุทุฑูู ุชุทุจูู ุงูุฎุทูุงุช ุงูุชุงููุฉ:

1. ุงูุชุทุจูุน
2. ุงูุชุฑููุฒ ุงููุณุจู
3. ุชูุณูู ุงููููุงุช ุฅูู ุญุฑูู ูุฑุฏูุฉ
4. ุชุทุจูู ููุงุนุฏ ุงูุฏูุฌ ุงูุชู ุชู ุชุนูููุง ุจุงูุชุฑุชูุจ ุนูู ุชูู ุงูุชูุณููุงุช

ุฏุนูุง ูุฃุฎุฐ ุงููุซุงู ุงูุฐู ุงุณุชุฎุฏููุงู ุฃุซูุงุก ุงูุชุฏุฑูุจุ ูุน ููุงุนุฏ ุงูุฏูุฌ ุงูุซูุงุซุฉ ุงูุชู ุชู ุชุนูููุง:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

ุณูุชู ุชุฑููุฒ ุงููููุฉ `"bug"` ุนูู ุฃููุง `["b", "ug"]`. ููุน ุฐููุ ุณูุชู ุชุฑููุฒ ุงููููุฉ `"mug"` ุนูู ุฃููุง `["[UNK]", "ug"]` ูุฃู ุงูุญุฑู `"m"` ูู ููู ูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ. ูุจุงููุซูุ ุณูุชู ุชุฑููุฒ ุงููููุฉ `"thug"` ุนูู ุฃููุง `["[UNK]", "hug"]`: ุงูุญุฑู `"t"` ููุณ ูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉุ ูุชุทุจูู ููุงุนุฏ ุงูุฏูุฌ ูุคุฏู ุฃููุงู ุฅูู ุฏูุฌ `"u"` ู `"g"` ุซู ุฏูุฌ `"h"` ู `"ug"`.

<Tip>

โ๏ธ **ุงูุขู ุฏูุฑู!** ููู ุจุฑุฃูู ุณูุชู ุชุฑููุฒ ุงููููุฉ `"unhug"`ุ

</Tip>

## ุชูููุฐ ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช [[implementing-bpe]]

ุงูุขู ูููู ูุธุฑุฉ ุนูู ุชูููุฐ ูุฎูุงุฑุฒููุฉ ุงูุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช. ูู ุชููู ูุฐู ุงููุณุฎุฉ ูุณุฎุฉ ูุญุณูุฉ ููููู ุงุณุชุฎุฏุงููุง ุจุงููุนู ุนูู ููุฑุณ ูุจูุฑุ ูุฑูุฏ ููุท ุฃู ูุฑูู ุงูููุฏ ููู ุชููู ุงูุฎูุงุฑุฒููุฉ ุจุดูู ุฃูุถู ููููุงู.

ุฃููุงูุ ูุญุชุงุฌ ุฅูู ููุฑุณุ ูุฐุง ุฏุนูุง ููุดุฆ ูุงุญุฏูุง ุจุณูุทูุง ุจุจุถุน ุฌูู:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

ุจุนุฏ ุฐููุ ูุญุชุงุฌ ุฅูู ุชุฑููุฒ ูุฐุง ุงูููุฑุณ ูุณุจููุง ุฅูู ูููุงุช. ุญูุซ ุฃููุง ููุฑุฑ ูุญูู ุชุฑููุฒ ุซูุงุฆู ุงูุจุงูุช (ูุซู GPT-2)ุ ูุณูุณุชุฎุฏู ูุญูู ุงูุชุฑููุฒ `gpt2` ููุชุฑููุฒ ุงููุณุจู:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ุซู ูุญุณุจ ุชุฑุฏุฏุงุช ูู ูููุฉ ูู ุงูููุฑุณ ุฃุซูุงุก ุงูุชุฑููุฒ ุงููุณุจู:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'ฤis': 2, 'ฤthe': 1, 'ฤHugging': 1, 'ฤFace': 1, 'ฤCourse': 1, '.': 4, 'ฤchapter': 1,
    'ฤabout': 1, 'ฤtokenization': 1, 'ฤsection': 1, 'ฤshows': 1, 'ฤseveral': 1, 'ฤtokenizer': 1, 'ฤalgorithms': 1,
    'Hopefully': 1, ',': 1, 'ฤyou': 1, 'ฤwill': 1, 'ฤbe': 1, 'ฤable': 1, 'ฤto': 1, 'ฤunderstand': 1, 'ฤhow': 1,
    'ฤthey': 1, 'ฤare': 1, 'ฤtrained': 1, 'ฤand': 1, 'ฤgenerate': 1, 'ฤtokens': 1})
```

ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุญุณุงุจ ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉุ ุงูุชู ุชุชููู ูู ุฌููุน ุงูุญุฑูู ุงููุณุชุฎุฏูุฉ ูู ุงูููุฑุณ:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'ฤ']
ูุถูู ุฃูุถูุง ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุณุชุฎุฏููุง ุงููููุฐุฌ ูู ุจุฏุงูุฉ ุงูููุฑุฏุงุช. ูู ุญุงูุฉ GPT-2ุ ุงูุฑูุฒ ุงูุฎุงุต ุงููุญูุฏ ูู "<|endoftext|>":

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

ูุญุชุงุฌ ุงูุขู ุฅูู ุชูุณูู ูู ูููุฉ ุฅูู ุญุฑูู ูููุฑุฏุฉุ ููุชููู ูู ุจุฏุก ุงูุชุฏุฑูุจ:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญูุง ุฌุงูุฒูู ููุชุฏุฑูุจุ ุฏุนูุง ููุชุจ ุฏุงูุฉ ูุญุณุงุจ ุชูุฑุงุฑ ูู ุฒูุฌ. ุณูุญุชุงุฌ ุฅูู ุงุณุชุฎุฏุงู ูุฐุง ูู ูู ุฎุทูุฉ ูู ุงูุชุฏุฑูุจ:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุฌุฒุก ูู ูุฐุง ุงููุงููุณ ุจุนุฏ ุงูุชูุณููุงุช ุงูุฃูููุฉ:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('ฤ', 'i'): 2
('ฤ', 't'): 7
('t', 'h'): 3
```

ุงูุขูุ ุฅูุฌุงุฏ ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุง ูุง ูุชุทูุจ ุณูู ุญููุฉ ุณุฑูุนุฉ:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('ฤ', 't') 7
```

ูุฐุง ูุฅู ุฃูู ุฏูุฌ ูุชุนููู ูู `('ฤ', 't') -> 'ฤt'`ุ ููุถูู 'ฤt' ุฅูู ุงูููุฑุฏุงุช:

```python
merges = {("ฤ", "t"): "ฤt"}
vocab.append("ฤt")
```

ูููุงุตูุฉ ุฐููุ ูุญุชุงุฌ ุฅูู ุชุทุจูู ูุฐุง ุงูุฏูุฌ ูู ูุงููุณ `splits`. ุฏุนูุง ููุชุจ ุฏุงูุฉ ุฃุฎุฑู ููุฐุง ุงูุบุฑุถ:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

ูููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ูุชูุฌุฉ ุงูุฏูุฌ ุงูุฃูู:

```py
splits = merge_pair("ฤ", "t", splits)
print(splits["ฤtrained"])
```

```python out
['ฤt', 'r', 'a', 'i', 'n', 'e', 'd']
```

ุงูุขู ูุฏููุง ูู ูุง ูุญุชุงุฌู ูุนูู ุญููุฉ ุญุชู ูุชุนูู ุฌููุน ุนูููุงุช ุงูุฏูุฌ ุงูุชู ูุฑูุฏ. ุฏุนูุง ููุฏู ุฅูู ุญุฌู ููุฑุฏุงุช ูุจูุบ 50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

ูุชูุฌุฉ ูุฐููุ ุชุนูููุง 19 ูุงุนุฏุฉ ุฏูุฌ (ูุงู ุญุฌู ุงูููุฑุฏุงุช ุงูุฃูููุฉ 31 - 30 ุญุฑููุง ูู ุงูุฃุจุฌุฏูุฉุ ุจุงูุฅุถุงูุฉ ุฅูู ุงูุฑูุฒ ุงูุฎุงุต):

```py
print(merges)
```

```python out
{('ฤ', 't'): 'ฤt', ('i', 's'): 'is', ('e', 'r'): 'er', ('ฤ', 'a'): 'ฤa', ('ฤt', 'o'): 'ฤto', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ฤto', 'k'): 'ฤtok',
 ('ฤtok', 'en'): 'ฤtoken', ('n', 'd'): 'nd', ('ฤ', 'is'): 'ฤis', ('ฤt', 'h'): 'ฤth', ('ฤth', 'e'): 'ฤthe',
 ('i', 'n'): 'in', ('ฤa', 'b'): 'ฤab', ('ฤtoken', 'i'): 'ฤtokeni'}
```

ูุชุชููู ุงูููุฑุฏุงุช ูู ุงูุฑูุฒ ุงูุฎุงุตุ ูุงูุฃุจุฌุฏูุฉ ุงูุฃูููุฉุ ููุชุงุฆุฌ ุฌููุน ุนูููุงุช ุงูุฏูุฌ:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ฤ', 'ฤt', 'is', 'er', 'ฤa', 'ฤto', 'en', 'Th', 'This', 'ou', 'se',
 'ฤtok', 'ฤtoken', 'nd', 'ฤis', 'ฤth', 'ฤthe', 'in', 'ฤab', 'ฤtokeni']
```

<Tip>

๐ก ุงุณุชุฎุฏุงู `train_new_from_iterator()` ุนูู ููุณ ุงููุฌููุนุฉ ูู ูุคุฏู ุฅูู ููุณ ุงูููุฑุฏุงุช ุจุงูุถุจุท. ูุฐูู ูุฃููุง ุนูุฏูุง ููุงุฌู ุฎูุงุฑ ุงูุฒูุฌ ุงูุฃูุซุฑ ุชูุฑุงุฑูุงุ ุงุฎุชุฑูุง ุงูุฃูู ุงูุฐู ุตุงุฏููุงูุ ุจูููุง ูุฎุชุงุฑ ููุชุจุฉ ๐ค Tokenizers ุงูุฃูู ุจูุงุกู ุนูู ูุนุฑูุงุชู ุงูุฏุงุฎููุฉ.

</Tip>

ูุชุญููู ูุต ุฌุฏูุฏุ ูููู ุจุชุญูููู ูุณุจููุงุ ุซู ููุณููุ ุซู ูุทุจู ุฌููุน ููุงุนุฏ ุงูุฏูุฌ ุงูุชู ุชุนูููุงูุง:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

ูููููุง ุชุฌุฑุจุฉ ูุฐุง ุนูู ุฃู ูุต ูุชููู ูู ุญุฑูู ูู ุงูุฃุจุฌุฏูุฉ:

```py
tokenize("This is not a token.")
```

```python out
['This', 'ฤis', 'ฤ', 'n', 'o', 't', 'ฤa', 'ฤtoken', '.']
```

<Tip warning={true}>

โ๏ธ ุณุชุฑูู ุนูููุฉ ุงูุชูููุฐ ุฎุทุฃ ุฅุฐุง ูุงู ููุงู ุญุฑู ุบูุฑ ูุนุฑููุ ูุฃููุง ูู ููุนู ุฃู ุดูุก ููุชุนุงูู ูุนู. ูุง ูุญุชูู GPT-2 ูู ุงููุงูุน ุนูู ุฑูุฒ ุบูุฑ ูุนุฑูู (ูู ุงููุณุชุญูู ุงูุญุตูู ุนูู ุญุฑู ุบูุฑ ูุนุฑูู ุนูุฏ ุงุณุชุฎุฏุงู BPE ุนูู ูุณุชูู ุงูุจุงูุช)ุ ูููู ูุฏ ูุญุฏุซ ูุฐุง ููุง ูุฃููุง ูู ูุฏุฑุฌ ุฌููุน ุงูุจุงูุชุงุช ุงููุญุชููุฉ ูู ุงูููุฑุฏุงุช ุงูุฃูููุฉ. ูุฐุง ุงูุฌุงูุจ ูู BPE ูุชุฌุงูุฒ ูุทุงู ูุฐุง ุงููุณูุ ูุฐุง ููุฏ ุชุฑููุง ุงูุชูุงุตูู.

</Tip>

ูุฐุง ูู ุดูุก ุจุงููุณุจุฉ ูุฎูุงุฑุฒููุฉ BPE! ุจุนุฏ ุฐููุ ุณูููู ูุธุฑุฉ ุนูู WordPiece.