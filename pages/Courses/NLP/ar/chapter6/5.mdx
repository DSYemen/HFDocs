# ุชุฑููุฒ ุงูุชุฑููุฒ ุงููุฒุฏูุฌ Byte-Pair 

Byte-Pair Encoding (BPE) ูู ูู ุงูุฃุตู ุฎูุงุฑุฒููุฉ ุชู ุชุทููุฑูุง ูุถุบุท ุงููุตูุตุ ุซู ุงุณุชุฎุฏูุชูุง OpenAI ููุชุฑููุฒ ุงููุณุจู ุนูุฏ ุงูุชุฏุฑูุจ ุงููุณุจู ููููุฐุฌ GPT. ููุณุชุฎุฏูู ุงูุนุฏูุฏ ูู ููุงุฐุฌ Transformerุ ุจูุง ูู ุฐูู GPT ูGPT-2 ูRoBERTa ูBART ูDeBERTa.

## ุฎูุงุฑุฒููุฉ ุงูุชุฏุฑูุจ

ูุจุฏุฃ ุงูุชุฏุฑูุจ ุนูู BPE ุจุญุณุงุจ ูุฌููุนุฉ ูุฑูุฏุฉ ูู ุงููููุงุช ุงููุณุชุฎุฏูุฉ ูู ุงููููู (ุจุนุฏ ุงูุชูุงู ุฎุทูุงุช ุงูุชูุญูุฏ ูุงูุชูููุฏ)ุ ุซู ุจูุงุก ุงูููุฑุฏุงุช ูู ุฎูุงู ุฃุฎุฐ ุฌููุน ุงูุฑููุฒ ุงููุณุชุฎุฏูุฉ ููุชุงุจุฉ ุชูู ุงููููุงุช. ููุซุงู ุจุณูุท ุฌุฏูุงุ ุฏุนูุง ูููู ุฃู ูููููุง ูุณุชุฎุฏู ูุฐู ุงููููุงุช ุงูุฎูุณ:

```
"hug", "pug", "pun", "bun", "hugs"
```

ุณุชููู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ ูู ุฅุฐู `["b"ุ "g"ุ "h"ุ "n"ุ "p"ุ "s"ุ "u"]`. ุจุงููุณุจุฉ ููุญุงูุงุช ุงููุงูุนูุฉุ ุณุชุชุถูู ูุฐู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ ุฌููุน ุฃุญุฑู ASCIIุ ุนูู ุงูุฃููุ ูุฑุจูุง ุจุนุถ ุฃุญุฑู Unicode ุฃูุถูุง. ุฅุฐุง ุงุณุชุฎุฏูุช ูุซุงููุง ุชููู ุจุชุฑููุฒู ุญุฑููุง ุบูุฑ ููุฌูุฏ ูู ูููู ุงูุชุฏุฑูุจุ ูุณูุชู ุชุญููู ูุฐุง ุงูุญุฑู ุฅูู ุฑูุฒ ุบูุฑ ูุนุฑูู. ูุฐุง ูู ุฃุญุฏ ุงูุฃุณุจุงุจ ุงูุชู ุชุฌุนู ุงูุนุฏูุฏ ูู ููุงุฐุฌ NLP ุณูุฆุฉ ููุบุงูุฉ ูู ุชุญููู ุงููุญุชูู ุจุงุณุชุฎุฏุงู ุงูุฑููุฒ ุงูุชุนุจูุฑูุฉุ ุนูู ุณุจูู ุงููุซุงู.

ูุฏู GPT-2 ูRoBERTa tokenizer (ูููุง ูุชุดุงุจูุงู ุฅูู ุญุฏ ูุง) ุทุฑููุฉ ุฐููุฉ ููุชุนุงูู ูุน ูุฐุง: ููู ูุง ุชูุธุฑ ุฅูู ุงููููุงุช ุนูู ุฃููุง ููุชูุจุฉ ุจุญุฑูู Unicodeุ ูููู ุจุญุฑูู ุจุงูุช. ุจูุฐู ุงูุทุฑููุฉุ ูููู ุญุฌู ุงูููุฑุฏุงุช ุงูุฃุณุงุณู ุตุบูุฑูุง (256)ุ ูููู ุณูุธู ูู ุญุฑู ููููู ุงูุชูููุฑ ููู ูุฏุฑุฌูุง ููู ููุชูู ุจู ุงูุฃูุฑ ุฅูู ุงูุชุญููู ุฅูู ุงูุฑูุฒ ุงููุฌููู. ุชูุนุฑู ูุฐู ุงูุญููุฉ ุจุงุณู *BPE ุนูู ูุณุชูู ุงูุจุงูุช*.

ุจุนุฏ ุงูุญุตูู ุนูู ูุฐู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉุ ูุถูู ุฑููุฒูุง ุฌุฏูุฏุฉ ุญุชู ูุชู ุงููุตูู ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุฑุบูุจ ูู ุฎูุงู ุชุนูู *ุนูููุงุช ุงูุฏูุฌ*ุ ูุงูุชู ูู ููุงุนุฏ ูุฏูุฌ ุนูุตุฑูู ูู ุงูููุฑุฏุงุช ุงูููุฌูุฏุฉ ูู ุนูุตุฑ ุฌุฏูุฏ. ูุฐููุ ูู ุงูุจุฏุงูุฉุ ุณุชููู ุนูููุงุช ุงูุฏูุฌ ูุฐู ุจุฅูุดุงุก ุฑููุฒ ุฐุงุช ุญุฑูููุ ุซูุ ูุน ุชูุฏู ุงูุชุฏุฑูุจุ ูููุงุช ูุฑุนูุฉ ุฃุทูู.

ูู ุฃู ุฎุทูุฉ ุฃุซูุงุก ุชุฏุฑูุจ ุงููุญูู ุงููุบููุ ุณุชุจุญุซ ุฎูุงุฑุฒููุฉ BPE ุนู ุฃูุซุฑ ุฃุฒูุงุฌ ุงูุฑููุฒ ุงูููุฌูุฏุฉ ุดููุนูุง (ุจู "ุฒูุฌ"ุ ูุนูู ููุง ุฑูุฒูู ูุชุชุงูููู ูู ูููุฉ). ูุฐุง ุงูุฒูุฌ ุงูุฃูุซุฑ ุดููุนูุง ูู ุงูุฐู ุณูุชู ุฏูุฌูุ ูููุฑุฑ ุฐูู ููุฎุทูุฉ ุงูุชุงููุฉ.

ุจุงูุนูุฏุฉ ุฅูู ูุซุงููุง ุงูุณุงุจูุ ุฏุนูุง ููุชุฑุถ ุฃู ุงููููุงุช ูุงูุช ููุง ุงูุชุฑุฏุฏุงุช ุงูุชุงููุฉ:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ููุง ูุนูู ุฃู `"hug"` ูุงู ููุฌูุฏูุง 10 ูุฑุงุช ูู ุงูููููุ ู`"pug"` 5 ูุฑุงุชุ ู`"pun"` 12 ูุฑุฉุ ู`"bun"` 4 ูุฑุงุชุ ู`"hugs"` 5 ูุฑุงุช. ูุจุฏุฃ ุงูุชุฏุฑูุจ ุนู ุทุฑูู ุชูุณูู ูู ูููุฉ ุฅูู ุฃุญุฑู (ุงูุชู ุชุดูู ููุฑุฏุงุชูุง ุงูุฃูููุฉ) ุญุชู ูุชููู ูู ุฑุคูุฉ ูู ูููุฉ ุนูู ุฃููุง ูุงุฆูุฉ ูู ุงูุฑููุฒ:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ุซู ููุธุฑ ุฅูู ุงูุฃุฒูุงุฌ. ุงูุฒูุฌ `("h"ุ "u")` ููุฌูุฏ ูู ุงููููุงุช `"hug"` ู`"hugs"`ุ ูุฐุง ููู ููุฌูุฏุฉ 15 ูุฑุฉ ูู ุงููุฌููุน ูู ุงููููู. ููุน ุฐููุ ูุฅููุง ููุณุช ุงูุฒูุฌ ุงูุฃูุซุฑ ุดููุนูุง: ููุชูู ูุฐุง ุงูุดุฑู ุฅูู `("u"ุ "g")`ุ ูุงูุฐู ููุฌุฏ ูู `"hug"` ู`"pug"` ู`"hugs"`ุ ุจุฅุฌูุงูู 20 ูุฑุฉ ูู ุงูููุฑุฏุงุช.

ูููุฐุงุ ูุฅู ุฃูู ูุงุนุฏุฉ ุฏูุฌ ูุชุนูููุง ุงููุญูู ุงููุบูู ูู `("u"ุ "g") -> "ug"`ุ ูุงูุชู ุชุนูู ุฃู `"ug"` ุณูุชู ุฅุถุงูุชูุง ุฅูู ุงูููุฑุฏุงุชุ ููุฌุจ ุฏูุฌ ุงูุฒูุฌ ูู ุฌููุน ูููุงุช ุงููููู. ูู ููุงูุฉ ูุฐู ุงููุฑุญูุฉุ ุชุจุฏู ุงูููุฑุฏุงุช ูุงููููู ุนูู ุงููุญู ุงูุชุงูู:

```
ููุฑุฏุงุช: ["b"ุ "g"ุ "h"ุ "n"ุ "p"ุ "s"ุ "u"ุ "ug"]
ูููู: ("h" "ug"ุ 10)ุ ("p" "ug"ุ 5)ุ ("p" "u" "n"ุ 12)ุ ("b" "u" "n"ุ 4)ุ ("h" "ug" "s"ุ 5)
```

ุงูุขู ูุฏููุง ุจุนุถ ุงูุฃุฒูุงุฌ ุงูุชู ููุชุฌ ุนููุง ุฑูุฒ ุฃุทูู ูู ุญุฑููู: ุงูุฒูุฌ `("h"ุ "ug")`ุ ุนูู ุณุจูู ุงููุซุงู (ููุฌูุฏ 15 ูุฑุฉ ูู ุงููููู). ููุน ุฐููุ ูุฅู ุงูุฒูุฌ ุงูุฃูุซุฑ ุดููุนูุง ูู ูุฐู ุงููุฑุญูุฉ ูู `("u"ุ "n")`ุ ุงูููุฌูุฏ 16 ูุฑุฉ ูู ุงูููููุ ูุฐุง ูุฅู ูุงุนุฏุฉ ุงูุฏูุฌ ุงูุซุงููุฉ ุงูุชู ุชู ุชุนูููุง ูู `("u"ุ "n") -> "un"`. ูุคุฏู ุฅุถุงูุฉ ุฐูู ุฅูู ุงูููุฑุฏุงุช ูุฏูุฌ ุฌููุน ุญุงูุงุช ุงูุชูุฑุงุฑ ุงูููุฌูุฏุฉ ุฅูู ูุง ููู:

```
ููุฑุฏุงุช: ["b"ุ "g"ุ "h"ุ "n"ุ "p"ุ "s"ุ "u"ุ "ug"ุ "un"]
ูููู: ("h" "ug"ุ 10)ุ ("p" "ug"ุ 5)ุ ("p" "un"ุ 12)ุ ("b" "un"ุ 4)ุ ("h" "ug" "s"ุ 5)
```

ุงูุขู ุงูุฒูุฌ ุงูุฃูุซุฑ ุดููุนูุง ูู `("h"ุ "ug")`ุ ูุฐุง ูุฅููุง ูุชุนูู ูุงุนุฏุฉ ุงูุฏูุฌ `("h"ุ "ug") -> "hug"`ุ ูุงูุชู ุชุนุทููุง ุฃูู ุฑูุฒ ูููู ูู ุซูุงุซุฉ ุฃุญุฑู. ุจุนุฏ ุงูุฏูุฌุ ูุจุฏู ุงููููู ุนูู ุงููุญู ุงูุชุงูู:

```
ููุฑุฏุงุช: ["b"ุ "g"ุ "h"ุ "n"ุ "p"ุ "s"ุ "u"ุ "ug"ุ "un"ุ "hug"]
ูููู: ("hug"ุ 10)ุ ("p" "ug"ุ 5)ุ ("p" "un"ุ 12)ุ ("b" "un"ุ 4)ุ ("hug" "s"ุ 5)
```

ููุณุชูุฑ ุนูู ูุฐุง ุงููููุงู ุญุชู ูุตู ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุฑุบูุจ.

## ุฎูุงุฑุฒููุฉ ุงูุชุฑููุฒ

ูุชุจุน ุงูุชุฑููุฒ ุนูููุฉ ุงูุชุฏุฑูุจ ุนู ูุซุจุ ุจูุนูู ุฃู ุงููุฏุฎูุงุช ุงูุฌุฏูุฏุฉ ูุชู ุชุฑููุฒูุง ูู ุฎูุงู ุชุทุจูู ุงูุฎุทูุงุช ุงูุชุงููุฉ:

1. ุงูุชูุญูุฏ
2. ูุง ูุจู ุงูุชุฑููุฒ
3. ุชูุณูู ุงููููุงุช ุฅูู ุฃุญุฑู ูุฑุฏูุฉ
4. ุชุทุจูู ููุงุนุฏ ุงูุฏูุฌ ุงูุชู ุชู ุชุนูููุง ุจุงูุชุฑุชูุจ ุนูู ุชูู ุงูุงููุณุงูุงุช

ููุฃุฎุฐ ูุซุงููุง ุงูุฐู ุงุณุชุฎุฏููุงู ุฃุซูุงุก ุงูุชุฏุฑูุจุ ูุน ููุงุนุฏ ุงูุฏูุฌ ุงูุซูุงุซุฉ ุงูุชู ุชู ุชุนูููุง:

```
("u"ุ "g") -> "ug"
("u"ุ "n") -> "un"
("h"ุ "ug") -> "hug"
```

ุณูุชู ุชุฑููุฒ ุงููููุฉ `"bug"` ุนูู ุฃููุง `["b"ุ "ug"]`. ููุน ุฐููุ ุณูุชู ุชุฑููุฒ ูููุฉ `"mug"` ุนูู ุฃููุง `["[UNK]"ุ "ug"]` ูุฃู ุงูุญุฑู `"m"` ูู ููู ูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ. ูุจุงููุซูุ ุณูุชู ุชุฑููุฒ ูููุฉ `"thug"` ุนูู ุฃููุง `["[UNK]"ุ "hug"]`: ุงูุญุฑู `"t"` ุบูุฑ ููุฌูุฏ ูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉุ ููุชูุฌุฉ ูุชุทุจูู ููุงุนุฏ ุงูุฏูุฌุ ูุชู ุฏูุฌ `"u"` ู`"g"` ุฃููุงูุ ุซู `"h"` ู`"ug"`.

โ๏ธ **ุงูุขู ุฏูุฑู!** ููู ุชุนุชูุฏ ุฃู ุงููููุฉ `"unhug"` ุณูุชู ุชุฑููุฒูุงุ
## ุชุทุจูู BPE

ูุงูุขูุ ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุชุทุจูู ูุฎูุงุฑุฒููุฉ BPE. ูู ูููู ูุฐุง ุงูุฅุตุฏุงุฑ ุงูุฃูุซู ุงูุฐู ููููู ุงุณุชุฎุฏุงูู ุจุงููุนู ูู ูุฌููุนุฉ ุจูุงูุงุช ูุจูุฑุฉุ ููุญู ูุฑูุฏ ููุท ุฃู ูุฑูู ุงูุดูุฑุฉ ุงูุจุฑูุฌูุฉ ุญุชู ุชุชููู ูู ููู ุงูุฎูุงุฑุฒููุฉ ุจุดูู ุฃูุถู ูููููุง.

ุฃูููุงุ ูุญุชุงุฌ ุฅูู ูุฌููุนุฉ ุจูุงูุงุชุ ูุฐุง ุฏุนููุง ููุดุฆ ูุงุญุฏุฉ ุจุณูุทุฉ ุจุจุถุน ุฌูู:

```python
corpus = [
"This is the Hugging Face Course.",
"This chapter is about tokenization.",
"This section shows several tokenizer algorithms.",
"Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

ุจุนุฏ ุฐููุ ูุญุชุงุฌ ุฅูู ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฐู ูุจุฏุฆููุง ุฅูู ูููุงุช. ูุธุฑูุง ูุฃููุง ููุฑุฑ ูุญูููุง ูุญูููุง ูู ููุน BPE (ูุซู GPT-2)ุ ูุณูุณุชุฎุฏู ุงููุญูู ุงููุญูู "gpt2" ููุชูุณูู ุงููุจุฏุฆู:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ุจุนุฏ ุฐููุ ูุญุณุจ ุชูุฑุงุฑุงุช ูู ูููุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฃุซูุงุก ููุงููุง ุจุงูุชูุณูู ุงููุจุฏุฆู:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
new_words = [word for word, offset in words_with_offsets]
for word in new_words:
word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'ฤis': 2, 'ฤthe': 1, 'ฤHugging': 1, 'ฤFace': 1, 'ฤCourse': 1, '.': 4, 'ฤchapter': 1,
'ฤabout': 1, 'ฤtokenization': 1, 'ฤsection': 1, 'ฤshows': 1, 'ฤseveral': 1, 'ฤtokenizer': 1, 'ฤalgorithms': 1,
'Hopefully': 1, ',': 1, 'ฤyou': 1, 'ฤwill': 1, 'ฤbe': 1, 'ฤable': 1, 'ฤto': 1, 'ฤunderstand': 1, 'ฤhow': 1,
'ฤthey': 1, 'ฤare': 1, 'ฤtrained': 1, 'ฤand': 1, 'ฤgenerate': 1, 'ฤtokens': 1})
```

ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุญุณุงุจ ูุฌููุนุฉ ุงููุญุงุฑู ุงูุฃุณุงุณูุฉุ ุงููุดููุฉ ูู ุฌููุน ุงููุญุงุฑู ุงููุณุชุฎุฏูุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช:

```python
alphabet = []

for word in word_freqs.keys():
for letter in word:
if letter not in alphabet:
alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
't', 'u', 'v', 'w', 'y', 'z', 'ฤ']
```

ูุถูู ุฃูุถูุง ุงูุฑููุฒ ุงูุฎุงุตุฉ ุงูุชู ูุณุชุฎุฏููุง ุงููููุฐุฌ ูู ุจุฏุงูุฉ ูุฌููุนุฉ ุงููุญุงุฑู ูุฐู. ูู ุญุงูุฉ GPT-2ุ ุงูุฑูุฒ ุงูุฎุงุต ุงููุญูุฏ ูู "<|endoftext|>":

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

ุงูุขูุ ูุญุชุงุฌ ุฅูู ุชูุณูู ูู ูููุฉ ุฅูู ูุญุงุฑู ูุฑุฏูุฉุ ููุชููู ูู ุงูุจุฏุก ูู ุงูุชุฏุฑูุจ:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญูุง ุฌุงูุฒูู ููุชุฏุฑูุจุ ุฏุนููุง ููุชุจ ุฏุงูุฉ ุชุญุณุจ ุชูุฑุงุฑ ูู ุฒูุฌ ูู ุงููุญุงุฑู. ุณูุญุชุงุฌ ุฅูู ุงุณุชุฎุฏุงู ูุฐู ุงูุฏุงูุฉ ูู ูู ุฎุทูุฉ ูู ุฎุทูุงุช ุงูุชุฏุฑูุจ:

```python
def compute_pair_freqs(splits):
pair_freqs = defaultdict(int)
for word, freq in word_freqs.items():
split = splits[word]
if len(split) == 1:
continue
for i in range(len(split) - 1):
pair = (split[i], split[i + 1])
pair_freqs[pair] += freq
return pair_freqs
```

ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุฌุฒุก ูู ูุฐุง ุงููุงููุณ ุจุนุฏ ุงูุงููุณุงูุงุช ุงูุฃูููุฉ:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
print(f"{key}: {pair_freqs[key]}")
if i >= 5:
break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('ฤ', 'i'): 2
('ฤ', 't'): 7
('t', 'h'): 3
```

ุงูุขูุ ูุง ูุณุชุบุฑู ุงูุนุซูุฑ ุนูู ุฃูุซุฑ ุงูุฃุฒูุงุฌ ุชูุฑุงุฑูุง ุณูู ุญููุฉ ุณุฑูุนุฉ:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
if max_freq is None or max_freq < freq:
best_pair = pair
max_freq = freq

print(best_pair, max_freq)
```

```python out
('ฤ', 't') 7
```

ูุฐุง ูุฅู ุฃูู ุฏูุฌ ูุชุนููู ูู `('ฤ', 't') -> 'ฤt'`ุ ููุถูู `'ฤt'` ุฅูู ูุฌููุนุฉ ุงููุญุงุฑู:

```python
merges = {("ฤ", "t"): "ฤt"}
vocab.append("ฤt")
```

ูููุงุตูุฉ ุฐููุ ูุญุชุงุฌ ุฅูู ุชุทุจูู ูุฐุง ุงูุฏูุฌ ูู ูุงููุณ `splits`. ุฏุนููุง ููุชุจ ุฏุงูุฉ ุฃุฎุฑู ููุฐุง ุงูุบุฑุถ:

```python
def merge_pair(a, b, splits):
for word in word_freqs:
split = splits[word]
if len(split) == 1:
continue

i = 0
while i < len(split) - 1:
if split[i] == a and split[i + 1] == b:
split = split[:i] + [a + b] + split[i + 2 :]
else:
i += 1
splits[word] = split
return splits
```

ููููููุง ุฅููุงุก ูุธุฑุฉ ุนูู ูุชูุฌุฉ ุงูุฏูุฌ ุงูุฃูู:

```py
splits = merge_pair("ฤ", "t", splits)
print(splits["ฤtrained"])
```

```python out
['ฤt', 'r', 'a', 'i', 'n', 'e', 'd']
```

ุงูุขู ูุฏููุง ูู ูุง ูุญุชุงุฌ ุฅููู ูุชูููุฐ ุญููุฉ ุญุชู ูุชุนูู ุฌููุน ุนูููุงุช ุงูุฏูุฌ ุงูุชู ูุฑูุฏ. ุฏุนููุง ููุฏู ุฅูู ุญุฌู ูุฌููุนุฉ ูุญุงุฑู ูุจูุบ 50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
pair_freqs = compute_pair_freqs(splits)
best_pair = ""
max_freq = None
for pair, freq in pair_freqs.items():
if max_freq is None or max_freq < freq:
best_pair = pair
max_freq = freq
splits = merge_pair(*best_pair, splits)
merges[best_pair] = best_pair[0] + best_pair[1]
vocab.append(best_pair[0] + best_pair[1])
```

ููุชูุฌุฉ ูุฐููุ ุชุนูููุง 19 ูุงุนุฏุฉ ุฏูุฌ (ูุงู ุญุฌู ูุฌููุนุฉ ุงููุญุงุฑู ุงูุฃูููุฉ 31 - 30 ูุญุฑููุง ูู ุงูุฃุจุฌุฏูุฉุ ุจุงูุฅุถุงูุฉ ุฅูู ุงูุฑูุฒ ุงูุฎุงุต):

```py
print(merges)
```

```python out
{('ฤ', 't'): 'ฤt', ('i', 's'): 'is', ('e', 'r'): 'er', ('ฤ', 'a'): 'ฤa', ('ฤt', 'o'): 'ฤto', ('e', 'n'): 'en',
('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ฤto', 'k'): 'ฤtok',
('ฤtok', 'en'): 'ฤtoken', ('n', 'd'): 'nd', ('ฤ', 'is'): 'ฤis', ('ฤt', 'h'): 'ฤth', ('ฤth', 'e'): 'ฤthe',
('i', 'n'): 'in', ('ฤa', 'b'): 'ฤab', ('ฤtoken', 'i'): 'ฤtokeni'}
```

ูุชุชููู ูุฌููุนุฉ ุงููุญุงุฑู ูู ุงูุฑูุฒ ุงูุฎุงุตุ ูุงูุฃุจุฌุฏูุฉ ุงูุฃูููุฉุ ููุชุงุฆุฌ ุฌููุน ุนูููุงุช ุงูุฏูุฌ:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ฤ', 'ฤt', 'is', 'er', 'ฤa', 'ฤto', 'en', 'Th', 'This', 'ou', 'se',
'ฤtok', 'ฤtoken', 'nd', 'ฤis', 'ฤth', 'ฤthe', 'in', 'ฤab', 'ฤtokeni']
```

<Tip>
๐ก ูู ูุคุฏู ุงุณุชุฎุฏุงู `train_new_from_iterator()` ุนูู ููุณ ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ุงูุญุตูู ุนูู ููุณ ูุฌููุนุฉ ุงููุญุงุฑู ุจุงูุถุจุท. ููุฑุฌุน ุฐูู ุฅูู ุฃูู ุนูุฏูุง ูููู ููุงู ุฎูุงุฑ ูุฃูุซุฑ ุงูุฃุฒูุงุฌ ุชูุฑุงุฑูุงุ ููุฏ ุงุฎุชุฑูุง ุงูุฃูู ุงูุฐู ุตุงุฏููุงูุ ูู ุญูู ุฃู ููุชุจุฉ ๐ค Tokenizers ุชุฎุชุงุฑ ุงูุฃูู ุจูุงุกู ุนูู ูุนุฑููุงุชูุง ุงูุฏุงุฎููุฉ.
</Tip>

ูุชุญููู ูุต ุฌุฏูุฏ ูุญูููู ูุจุฏุฆููุงุ ุซู ููุณููุ ุซู ูุทุจู ุฌููุน ููุงุนุฏ ุงูุฏูุฌ ุงูุชู ุชุนูููุงูุง:

```python
def tokenize(text):
pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
pre_tokenized_text = [word for word, offset in pre_tokenize_result]
splits = [[l for l in word] for word in pre_tokenized_text]
for pair, merge in merges.items():
for idx, split in enumerate(splits):
i = 0
while i < len(split) - 1:
if split[i] == pair[0] and split[i + 1] == pair[1]:
split = split[:i] + [merge] + split[i + 2 :]
else:
i += 1
splits[idx] = split

return sum(splits, [])
```

ูููููุง ุชุฌุฑุจุฉ ุฐูู ุนูู ุฃู ูุต ูุชููู ูู ูุญุงุฑู ูู ุงูุฃุจุฌุฏูุฉ:

```py
tokenize("This is not a token.")
```

```python out
['This', 'ฤis', 'ฤ', 'n', 'o', 't', 'ฤa', 'ฤtoken', '.']
```

<Tip warning={true}>
โ๏ธ ุณููุชุฌ ุนู ุชูููุฐูุง ุฎุทุฃ ุฅุฐุง ูุงู ููุงู ูุญุฑู ุบูุฑ ูุนุฑูู ูุฃููุง ูู ููุนู ุดูุฆูุง ููุชุนุงูู ูุนู. ูุง ูุญุชูู GPT-2 ูู ุงููุงูุน ุนูู ุฑูุฒ ุบูุฑ ูุนุฑูู (ูู ุงููุณุชุญูู ุงูุญุตูู ุนูู ูุญุฑู ุบูุฑ ูุนุฑูู ุนูุฏ ุงุณุชุฎุฏุงู BPE ุนูู ูุณุชูู ุงูุจุงูุช)ุ ูููู ูุฐุง ูุฏ ูุญุฏุซ ููุง ูุฃููุง ูู ูุฏุฑุฌ ุฌููุน ุงูุจุงูุชุงุช ุงูููููุฉ ูู ูุฌููุนุฉ ุงููุญุงุฑู ุงูุฃูููุฉ. ููุฐุง ุงูุฌุงูุจ ูู BPE ูุชุฌุงูุฒ ูุทุงู ูุฐุง ุงููุณูุ ูุฐุง ููุฏ ุชุฑููุง ุงูุชูุงุตูู.
</Tip>

ูุฐุง ูู ุดูุก ุนู ุฎูุงุฑุฒููุฉ BPE! ุจุนุฏ ุฐููุ ุณูููู ูุธุฑุฉ ุนูู WordPiece.