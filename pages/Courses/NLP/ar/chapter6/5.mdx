# الترميز ثنائي البايت [[byte-pair-encoding-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
]} />

تم تطوير الترميز ثنائي البايت (BPE) في البداية كخوارزمية لضغط النصوص، ثم استخدمته OpenAI للترميز عند التدريب المسبق لنموذج GPT. ويستخدمه الكثير من نماذج المحول، بما في ذلك GPT وGPT-2 وRoBERTa وBART وDeBERTa.

<Youtube id="HEikzVL-lZU"/>

<Tip>

💡 يغطي هذا القسم الترميز ثنائي البايت بعمق، وصولاً إلى عرض التنفيذ الكامل. يمكنك تخطي هذا القسم والانتقال إلى النهاية إذا كنت تريد فقط نظرة عامة على خوارزمية الترميز.

</Tip>

## خوارزمية التدريب [[training-algorithm]]

يبدأ تدريب الترميز ثنائي البايت بحساب مجموعة الكلمات الفريدة المستخدمة في الفهرس (بعد اكتمال خطوات التطبيع والترميز المسبق)، ثم بناء المفردات عن طريق أخذ جميع الرموز المستخدمة لكتابة تلك الكلمات. كمثال بسيط للغاية، لنفترض أن فهرسنا يستخدم هذه الكلمات الخمس:

```
"hug", "pug", "pun", "bun", "hugs"
```

ستكون المفردات الأساسية حينها هي `["b", "g", "h", "n", "p", "s", "u"]`. في الحالات الواقعية، ستتضمن المفردات الأساسية جميع أحرف ASCII على الأقل، وربما بعض أحرف Unicode أيضًا. إذا كان المثال الذي تقوم بترميزه يستخدم حرفًا غير موجود في الفهرس التدريبي، فسيتم تحويل ذلك الحرف إلى الرمز المجهول. هذا أحد الأسباب التي تجعل العديد من نماذج NLP سيئة للغاية في تحليل المحتوى مع الرموز التعبيرية، على سبيل المثال.

<Tip>

لدى محول الترميز GPT-2 وRoBERTa (وهما متشابهان إلى حد كبير) طريقة ذكية للتعامل مع هذا: لا ينظرون إلى الكلمات على أنها مكتوبة بحروف Unicode، ولكن بالبايتات. بهذه الطريقة، يكون حجم المفردات الأساسية صغيرًا (256)، ولكن سيتم تضمين كل حرف يمكنك التفكير فيه ولن ينتهي به الأمر إلى التحويل إلى الرمز المجهول. تسمى هذه الحيلة *الترميز ثنائي البايت على مستوى البايت*.

</Tip>

بعد الحصول على هذه المفردات الأساسية، نضيف رموزًا جديدة حتى يتم الوصول إلى حجم المفردات المطلوب عن طريق تعلم *الدمج*، وهي قواعد لدمج عنصرين من المفردات الموجودة معًا في عنصر جديد. لذلك، في البداية، ستخلق عمليات الدمج هذه رموزًا ذات حرفين، ثم، مع تقدم التدريب، كلمات فرعية أطول.

في أي خطوة أثناء تدريب المحول، سيبحث خوارزمية الترميز ثنائي البايت عن أكثر أزواج الرموز الموجودة تكرارًا (بـ "زوج"، نقصد هنا رمزين متتاليين في كلمة). سيكون هذا الزوج الأكثر تكرارًا هو الذي سيتم دمجه، ونكرر العملية للخطوة التالية.

بالعودة إلى مثالنا السابق، لنفترض أن الكلمات كانت لها الترددات التالية:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

مما يعني أن الكلمة `"hug"` كانت موجودة 10 مرات في الفهرس، و `"pug"` 5 مرات، و `"pun"` 12 مرة، و `"bun"` 4 مرات، و `"hugs"` 5 مرات. نبدأ التدريب بتقسيم كل كلمة إلى حروف (التي تشكل مفرداتنا الأولية) بحيث يمكننا رؤية كل كلمة على أنها قائمة من الرموز:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ثم ننظر إلى الأزواج. الزوج `("h", "u")` موجود في الكلمتين `"hug"` و `"hugs"`، أي 15 مرة في المجموع في الفهرس. لكنه ليس الزوج الأكثر تكرارًا: هذا الشرف يعود إلى الزوج `("u", "g")`، الموجود في الكلمات `"hug"` و `"pug"` و `"hugs"`، بمجموع 20 مرة في الفهرس.

وبالتالي، تكون قاعدة الدمج الأولى التي يتعلمها المحول هي `("u", "g") -> "ug"`، مما يعني أنه سيتم إضافة `"ug"` إلى المفردات، ويجب دمج الزوج في جميع كلمات الفهرس. في نهاية هذه المرحلة، تبدو المفردات والفهرس على النحو التالي:

```
المفردات: ["b", "g", "h", "n", "p", "s", "u", "ug"]
الفهرس: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

الآن لدينا بعض الأزواج التي تؤدي إلى رمز أطول من حرفين: الزوج `("h", "ug")`، على سبيل المثال (موجود 15 مرة في الفهرس). الزوج الأكثر تكرارًا في هذه المرحلة هو `("u", "n")`، ومع ذلك، موجود 16 مرة في الفهرس، لذا تكون قاعدة الدمج الثانية التي يتم تعلمها هي `("u", "n") -> "un"`. إضافة ذلك إلى المفردات ودمج جميع التواجدات الموجودة يؤدي إلى:

```
المفردات: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
الفهرس: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

الآن الزوج الأكثر تكرارًا هو `("h", "ug")`، لذا نتعلم قاعدة الدمج `("h", "ug") -> "hug"`، والتي تعطينا أول رمز مكون من ثلاثة أحرف. بعد الدمج، يبدو الفهرس على النحو التالي:

```
المفردات: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
الفهرس: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

ونستمر على هذا النحو حتى نصل إلى حجم المفردات المطلوب.

<Tip>

✏️ **الآن دورك!** ما هي قاعدة الدمج التالية برأيك؟

</Tip>

## خوارزمية الترميز [[tokenization-algorithm]]

يتبع الترميز عملية التدريب عن كثب، بمعنى أن الإدخالات الجديدة يتم ترميزها عن طريق تطبيق الخطوات التالية:

1. التطبيع
2. الترميز المسبق
3. تقسيم الكلمات إلى حروف فردية
4. تطبيق قواعد الدمج التي تم تعلمها بالترتيب على تلك التقسيمات

دعنا نأخذ المثال الذي استخدمناه أثناء التدريب، مع قواعد الدمج الثلاثة التي تم تعلمها:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

سيتم ترميز الكلمة `"bug"` على أنها `["b", "ug"]`. ومع ذلك، سيتم ترميز الكلمة `"mug"` على أنها `["[UNK]", "ug"]` لأن الحرف `"m"` لم يكن في المفردات الأساسية. وبالمثل، سيتم ترميز الكلمة `"thug"` على أنها `["[UNK]", "hug"]`: الحرف `"t"` ليس في المفردات الأساسية، وتطبيق قواعد الدمج يؤدي أولاً إلى دمج `"u"` و `"g"` ثم دمج `"h"` و `"ug"`.

<Tip>

✏️ **الآن دورك!** كيف برأيك سيتم ترميز الكلمة `"unhug"`؟

</Tip>

## تنفيذ الترميز ثنائي البايت [[implementing-bpe]]

الآن لنلق نظرة على تنفيذ لخوارزمية الترميز ثنائي البايت. لن تكون هذه النسخة نسخة محسنة يمكنك استخدامها بالفعل على فهرس كبير؛ نريد فقط أن نريك الكود لكي تفهم الخوارزمية بشكل أفضل قليلاً.

أولاً، نحتاج إلى فهرس، لذا دعنا ننشئ واحدًا بسيطًا ببضع جمل:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

بعد ذلك، نحتاج إلى ترميز هذا الفهرس مسبقًا إلى كلمات. حيث أننا نكرر محول ترميز ثنائي البايت (مثل GPT-2)، فسنستخدم محول الترميز `gpt2` للترميز المسبق:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ثم نحسب ترددات كل كلمة في الفهرس أثناء الترميز المسبق:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1,
    'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1,
    'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1,
    'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})
```

الخطوة التالية هي حساب المفردات الأساسية، التي تتكون من جميع الحروف المستخدمة في الفهرس:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ġ']
نضيف أيضًا الرموز الخاصة التي يستخدمها النموذج في بداية المفردات. في حالة GPT-2، الرمز الخاص الوحيد هو "<|endoftext|>":

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

نحتاج الآن إلى تقسيم كل كلمة إلى حروف منفردة، لنتمكن من بدء التدريب:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

الآن بعد أن أصبحنا جاهزين للتدريب، دعنا نكتب دالة لحساب تكرار كل زوج. سنحتاج إلى استخدام هذا في كل خطوة من التدريب:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

دعنا نلقي نظرة على جزء من هذا القاموس بعد التقسيمات الأولية:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ġ', 'i'): 2
('Ġ', 't'): 7
('t', 'h'): 3
```

الآن، إيجاد الزوج الأكثر تكرارًا لا يتطلب سوى حلقة سريعة:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('Ġ', 't') 7
```

لذا فإن أول دمج نتعلمه هو `('Ġ', 't') -> 'Ġt'`، ونضيف 'Ġt' إلى المفردات:

```python
merges = {("Ġ", "t"): "Ġt"}
vocab.append("Ġt")
```

لمواصلة ذلك، نحتاج إلى تطبيق هذا الدمج في قاموس `splits`. دعنا نكتب دالة أخرى لهذا الغرض:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

يمكننا إلقاء نظرة على نتيجة الدمج الأول:

```py
splits = merge_pair("Ġ", "t", splits)
print(splits["Ġtrained"])
```

```python out
['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']
```

الآن لدينا كل ما نحتاجه لعمل حلقة حتى نتعلم جميع عمليات الدمج التي نريد. دعنا نهدف إلى حجم مفردات يبلغ 50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

نتيجة لذلك، تعلمنا 19 قاعدة دمج (كان حجم المفردات الأولية 31 - 30 حرفًا في الأبجدية، بالإضافة إلى الرمز الخاص):

```py
print(merges)
```

```python out
{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok',
 ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe',
 ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}
```

وتتكون المفردات من الرمز الخاص، والأبجدية الأولية، ونتائج جميع عمليات الدمج:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se',
 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']
```

<Tip>

💡 استخدام `train_new_from_iterator()` على نفس المجموعة لن يؤدي إلى نفس المفردات بالضبط. وذلك لأننا عندما نواجه خيار الزوج الأكثر تكرارًا، اخترنا الأول الذي صادفناه، بينما يختار مكتبة 🤗 Tokenizers الأول بناءً على معرفاته الداخلية.

</Tip>

لتحليل نص جديد، نقوم بتحليله مسبقًا، ثم نقسمه، ثم نطبق جميع قواعد الدمج التي تعلمناها:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

يمكننا تجربة هذا على أي نص يتكون من حروف في الأبجدية:

```py
tokenize("This is not a token.")
```

```python out
['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']
```

<Tip warning={true}>

⚠️ سترمي عملية التنفيذ خطأ إذا كان هناك حرف غير معروف، لأننا لم نفعل أي شيء للتعامل معه. لا يحتوي GPT-2 في الواقع على رمز غير معروف (من المستحيل الحصول على حرف غير معروف عند استخدام BPE على مستوى البايت)، ولكن قد يحدث هذا هنا لأننا لم ندرج جميع البايتات المحتملة في المفردات الأولية. هذا الجانب من BPE يتجاوز نطاق هذا القسم، لذا فقد تركنا التفاصيل.

</Tip>

هذا كل شيء بالنسبة لخوارزمية BPE! بعد ذلك، سنلقي نظرة على WordPiece.