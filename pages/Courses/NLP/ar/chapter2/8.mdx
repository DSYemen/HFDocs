<FrameworkSwitchCourse {fw} />

<!-- DISABLE-FRONTMATTER-SECTIONS -->

# اختبار نهاية الفصل [[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={2}
    classNames="absolute z-10 right-0 top-0"
/>

### 1. ما هو ترتيب خطوات أنبوبية النمذجة اللغوية؟

<Question
	choices={[
		{
			text: "أولاً، النموذج، الذي يتعامل مع النص ويعيد التنبؤات الخام. ثم يأتي دور المعالج اللغوي الذي يفهم هذه التنبؤات ويحولها مرة أخرى إلى نص عند الحاجة.",
			explain: "النموذج لا يفهم النص! يجب على المعالج اللغوي أولاً تقسيم النص إلى رموز وتحويله إلى معرفات حتى يتمكن النموذج من فهمه."
		},
		{
			text: "أولاً، المعالج اللغوي، الذي يتعامل مع النص ويعيد المعرفات. النموذج يتعامل مع هذه المعرفات ويخرج تنبؤًا، والذي يمكن أن يكون نصًا.",
			explain: "لا يمكن أن يكون تنبؤ النموذج نصًا على الفور. يجب استخدام المعالج اللغوي لتحويل التنبؤ مرة أخرى إلى نص!"
		},
		{
			text: "يتعامل المعالج اللغوي مع النص ويعيد المعرفات. النموذج يتعامل مع هذه المعرفات ويخرج تنبؤًا. بعد ذلك، يمكن استخدام المعالج اللغوي مرة أخرى لتحويل هذه التنبؤات إلى نص.",
			explain: "صحيح! يمكن استخدام المعالج اللغوي للتقسيم وإزالة التقسيم.",
            correct: true
		}
	]}
/>

### 2. ما هو عدد أبعاد المخرجات التي ينتجها نموذج المحول الأساسي، وما هي هذه الأبعاد؟

<Question
	choices={[
		{
			text: "2: طول التسلسل وحجم الدفعة",
			explain: "خطأ! للمخرجات التي ينتجها النموذج بُعد ثالث: حجم المخفي.",
			correct: false
		},
		{
			text: "2: طول التسلسل وحجم المخفي",
			explain: "خطأ! جميع نماذج المحول تتعامل مع الدفعات، حتى مع تسلسل واحد؛ سيكون حجم الدفعة في هذه الحالة 1!",
			correct: false
		},
		{
			text: "3: طول التسلسل، وحجم الدفعة، وحجم المخفي",
			explain: "صحيح!",
            correct: true
		}
	]}
/>

### 3. أي مما يلي هو مثال على تقسيم الرموز الفرعية للكلمة؟

<Question
	choices={[
		{
			text: "WordPiece",
			explain: "نعم، هذا مثال على تقسيم الرموز الفرعية للكلمة!",
            correct: true
		},
		{
			text: "تقسيم الرموز على أساس الحروف",
			explain: "تقسيم الرموز على أساس الحروف ليس نوعًا من تقسيم الرموز الفرعية للكلمة."
		},
		{
			text: "الانقسام على المسافات والفواصل",
			explain: "هذا مخطط لتقسيم الرموز على أساس الكلمات!"
		},
		{
			text: "BPE",
			explain: "نعم، هذا مثال على تقسيم الرموز الفرعية للكلمة!",
            correct: true
        },
		{
			text: "Unigram",
			explain: "نعم، هذا مثال على تقسيم الرموز الفرعية للكلمة!",
            correct: true
        },
		{
			text: "لا شيء مما سبق",
			explain: "غير صحيح!"
        }
	]}
/>

### 4. ما هو رأس النموذج؟

<Question
	choices={[
		{
			text: "مكون من شبكة المحول الأساسية التي تحول المصفوفات إلى طبقاتها الصحيحة",
			explain: "غير صحيح! لا يوجد مثل هذا المكون.",
			correct: false
		},
		{
			text: "المعروف أيضًا بآلية الانتباه الذاتي، فهو يُكيّف تمثيل الرمز وفقًا للرموز الأخرى في التسلسل",
			explain: "غير صحيح! تحتوي طبقة الانتباه الذاتي على رؤوس للانتباه، ولكن هذه ليست رؤوس تكيف.",
			correct: false
		},
		{
			text: "مكون إضافي، يتكون عادة من طبقة أو بضع طبقات، لتحويل تنبؤات المحول إلى مخرجات خاصة بالمهمة",
			explain: "هذا صحيح. رؤوس التكيف، المعروفة أيضًا ببساطة بالرؤوس، تأتي بأشكال مختلفة: رؤوس نمذجة اللغة، رؤوس الإجابة على الأسئلة، رؤوس تصنيف التسلسل... ",
			correct: true
		} 
	]}
/>

{#if fw === 'pt'}
### 5. ما هو النموذج التلقائي؟

<Question
	choices={[
		{
			text: "نموذج يتدرب تلقائيًا على بياناتك",
			explain: "غير صحيح. هل تخلط بين هذا ومنتجنا <a href='https://huggingface.co/autotrain'>AutoTrain</a>؟",
			correct: false
		},
		{
			text: "كائن يعيد الهندسة الصحيحة بناءً على نقطة التفتيش",
			explain: "بالضبط: <code>AutoModel</code> يحتاج فقط إلى معرفة نقطة التفتيش التي سيبدأ منها لإعادة الهندسة الصحيحة.",
			correct: true
		},
		{
			text: "نموذج يكتشف تلقائيًا اللغة المستخدمة في مدخلاته لتحميل الأوزان الصحيحة",
			explain: "غير صحيح؛ بينما بعض نقاط التفتيش والنماذج قادرة على التعامل مع لغات متعددة، لا توجد أدوات مدمجة لاختيار نقطة التفتيش التلقائي وفقًا للغة. يجب عليك التوجه إلى <a href='https://huggingface.co/models'>Model Hub</a> لإيجاد أفضل نقطة تفتيش لمهمتك!",
			correct: false
		} 
	]}
/>

{:else}
### 5. ما هو TFAutoModel؟

<Question
	choices={[
		{
			text: "نموذج يتدرب تلقائيًا على بياناتك",
			explain: "غير صحيح. هل تخلط بين هذا ومنتجنا <a href='https://huggingface.co/autotrain'>AutoTrain</a>؟",
			correct: false
		},
		{
			text: "كائن يعيد الهندسة الصحيحة بناءً على نقطة التفتيش",
			explain: "بالضبط: <code>TFAutoModel</code> يحتاج فقط إلى معرفة نقطة التفتيش التي سيبدأ منها لإعادة الهندسة الصحيحة.",
			correct: true
		},
		{
			text: "نموذج يكتشف تلقائيًا اللغة المستخدمة في مدخلاته لتحميل الأوزان الصحيحة",
			explain: "غير صحيح؛ بينما بعض نقاط التفتيش والنماذج قادرة على التعامل مع لغات متعددة، لا توجد أدوات مدمجة لاختيار نقطة التفتيش التلقائي وفقًا للغة. يجب عليك التوجه إلى <a href='https://huggingface.co/models'>Model Hub</a> لإيجاد أفضل نقطة تفتيش لمهمتك!",
			correct: false
		} 
	]}
/>

{/if}

### 6. ما هي التقنيات التي يجب الانتباه إليها عند تجميع تسلسلات بأطوال مختلفة معًا؟

<Question
	choices={[
		{
			text: "الاقتطاع",
			explain: "نعم، الاقتطاع طريقة صحيحة لتقريب التسلسلات حتى تتناسب في شكل مستطيل. ولكن، هل هي الطريقة الوحيدة؟",
			correct: true
		},
		{
			text: "إرجاع المصفوفات",
			explain: "بينما تسمح التقنيات الأخرى بإرجاع مصفوفات مستطيلة، فإن إرجاع المصفوفات ليس مفيدًا عند تجميع التسلسلات معًا.",
			correct: false
		},
		{
			text: "التعبئة",
			explain: "نعم، التعبئة طريقة صحيحة لتقريب التسلسلات حتى تتناسب في شكل مستطيل. ولكن، هل هي الطريقة الوحيدة؟",
			correct: true
		}, 
		{
			text: "قناع الانتباه",
			explain: "بالتأكيد! أقنعة الانتباه ذات أهمية قصوى عند التعامل مع تسلسلات بأطوال مختلفة. ولكن، هذه ليست التقنية الوحيدة التي يجب الانتباه إليها.",
			correct: true
		} 
	]}
/>

### 7. ما هو الغرض من تطبيق دالة SoftMax على اللوغاريتمات التي يخرجها نموذج تصنيف التسلسل؟

<Question
	choices={[
		{
			text: "تجعل اللوغاريتمات أكثر ليونة حتى تكون أكثر موثوقية.",
			explain: "لا، دالة SoftMax لا تؤثر على موثوقية النتائج.",
			correct: false
		},
		{
			text: "تطبق حدًا أدنى وحدًا أقصى حتى تكون مفهومة.",
			explain: "صحيح! القيم الناتجة تكون محصورة بين 0 و1. ولكن، هذا ليس السبب الوحيد لاستخدام دالة SoftMax.",
            correct: true
		},
		{
			text: "يكون مجموع المخرجات 1، مما يؤدي إلى تفسير احتمالي ممكن.",
			explain: "صحيح! ولكن، هذا ليس السبب الوحيد لاستخدام دالة SoftMax.",
            correct: true
		}
	]}
/>

### 8. ما هي الطريقة التي تدور حولها معظم واجهة برمجة التطبيقات للمعالج اللغوي؟

<Question
	choices={[
		{
			text: "<code>encode</code>، حيث يمكنها ترميز النص إلى معرفات والمعرفات إلى تنبؤات",
			explain: "خطأ! بينما توجد طريقة <code>encode</code> على المعالجات اللغوية، إلا أنها لا توجد على النماذج.",
			correct: false
		},
		{
			text: "استدعاء كائن المعالج اللغوي مباشرة.",
			explain: "بالضبط! طريقة <code>__call__</code> للمعالج اللغوي هي طريقة قوية جدًا يمكنها التعامل مع أي شيء تقريبًا. وهي أيضًا الطريقة المستخدمة لاسترجاع التنبؤات من النموذج.",
			correct: true
		},
		{
			text: "<code>pad</code>",
			explain: "خطأ! التعبئة مفيدة جدًا، ولكنها مجرد جزء من واجهة برمجة التطبيقات للمعالج اللغوي.",
			correct: false
		},
		{
			text: "<code>tokenize</code>",
			explain: "طريقة <code>tokenize</code> هي إحدى أكثر الطرق المفيدة، ولكنها ليست جوهر واجهة برمجة التطبيقات للمعالج اللغوي.",
			correct: false
		}
	]}
/>

### 9. ما الذي يحتويه متغير `result` في هذا المثال البرمجي؟

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```
<Question
	choices={[
		{
			text: "قائمة من السلاسل النصية، كل سلسلة تمثل رمزًا",
			explain: "بالتأكيد! قم بتحويلها إلى معرفات، وأرسلها إلى النموذج!",
            correct: true
		},
		{
			text: "قائمة من المعرفات",
			explain: "غير صحيح؛ هذا هو الغرض من طريقة <code>__call__</code> أو <code>convert_tokens_to_ids</code>!"
		},
		{
			text: "سلسلة نصية تحتوي على جميع الرموز",
			explain: "هذا ليس مثاليًا، لأن الهدف هو تقسيم السلسلة إلى رموز متعددة."
		}
	]}
/>

{#if fw === 'pt'}
### 10. هل هناك خطأ في الكود التالي؟

```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

<Question
	choices={[
		{
			text: "لا، يبدو صحيحًا.",
			explain: "لسوء الحظ، ربط نموذج بمُرمز (tokenizer) تم تدريبه على نقطة تفتيش مختلفة ليس فكرة جيدة في الغالب. لم يتم تدريب النموذج لفهم ناتج المرمز، لذلك لن يكون لناتج النموذج (إذا كان بإمكانه حتى العمل!) أي معنى."
		},
		{
			text: "يجب أن يكون المرمز والنموذج دائمًا من نفس نقطة التفتيش.",
			explain: "صحيح!",
            correct: true
		},
		{
			text: "من الجيد استخدام الحشو والتقطيع مع المرمز لأن كل إدخال هو دفعة.",
			explain: "صحيح أن كل إدخال للنموذج يحتاج إلى أن يكون دفعة. ومع ذلك، فإن تقطيع أو حشو هذه السلسلة لن يكون له معنى بالضرورة لأنه يوجد واحد فقط منها، وهذه تقنيات لتجميع قائمة من الجمل في دفعة واحدة."
		}
	]}
/>

{:else}
### 10. هل هناك خطأ في الكود التالي؟

```py
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

<Question
	choices={[
		{
			text: "لا، يبدو صحيحًا.",
			explain: "لسوء الحظ، ربط نموذج بمُرمز (tokenizer) تم تدريبه على نقطة تفتيش مختلفة ليس فكرة جيدة في الغالب. لم يتم تدريب النموذج لفهم ناتج المرمز، لذلك لن يكون لناتج النموذج (إذا كان بإمكانه حتى العمل!) أي معنى."
		},
		{
			text: "يجب أن يكون المرمز والنموذج دائمًا من نفس نقطة التفتيش.",
			explain: "صحيح!",
            correct: true
		},
		{
			text: "من الجيد استخدام الحشو والتقطيع مع المرمز لأن كل إدخال هو دفعة.",
			explain: "صحيح أن كل إدخال للنموذج يحتاج إلى أن يكون دفعة. ومع ذلك، فإن تقطيع أو حشو هذه السلسلة لن يكون له معنى بالضرورة لأنه يوجد واحد فقط منها، وهذه تقنيات لتجميع قائمة من الجمل في دفعة واحدة."
		}
	]}
/>

{/if}