<FrameworkSwitchCourse {fw} />

# ูุถุน ูู ุดูุก ูุนูุง [[putting-it-all-together]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
]} />

{/if}

ูู ุงูุฃูุณุงู ุงูููููุฉ ุงููุงุถูุฉุ ุญุงูููุง ุจุฐู ูุตุงุฑู ุฌูุฏูุง ููููุงู ุจูุนุธู ุงูุนูู ูุฏูููุง. ููุฏ ุงุณุชูุดููุง ููููุฉ ุนูู ุงููุญููุงุช ุงููุบููุฉ (Tokenizers) ูุฏุฑุณูุง ุนูููุฉ ุงูุชุญููู ุงููุบููุ ูุงูุชุญููู ุฅูู ูุนุฑูุงุช ุงูุฅุฏุฎุงูุ ูุงูุชุนุจุฆุฉุ ูุงูุชูุทูุนุ ูุฃููุนุฉ ุงูุงูุชุจุงู.

ููุน ุฐููุ ููุง ุฑุฃููุง ูู ุงููุณู 2ุ ูููู ููุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช (API) ุงูุฎุงุตุฉ ุจู ๐ค Transformers ุงูุชุนุงูู ูุน ูู ูุฐุง ูู ุฃุฌููุง ุจูุธููุฉ ุนุงููุฉ ุงููุณุชูู ุณูุบูุต ูููุง ููุง. ุนูุฏูุง ุชููู ุจุงุณุชุฏุนุงุก ุงููุญูู ุงููุบูู `tokenizer` ูุจุงุดุฑุฉ ุนูู ุงูุฌููุฉุ ุณุชุญุตู ุนูู ุฅุฏุฎุงูุงุช ุฌุงูุฒุฉ ูููุฑูุฑ ุนุจุฑ ูููุฐุฌู:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ููุงุ ุชุญุชูู ูุชุบูุฑ `model_inputs` ุนูู ูู ูุง ูู ุถุฑูุฑู ูุชุดุบูู ุงููููุฐุฌ ุจุดูู ุฌูุฏ. ุจุงููุณุจุฉ ูู DistilBERTุ ูุชุถูู ุฐูู ูุนุฑูุงุช ุงูุฅุฏุฎุงู ุจุงูุฅุถุงูุฉ ุฅูู ููุงุน ุงูุงูุชุจุงู. ุงูููุงุฐุฌ ุงูุฃุฎุฑู ุงูุชู ุชูุจู ุฅุฏุฎุงูุงุช ุฅุถุงููุฉ ุณูููู ูุฏููุง ุฃูุถูุง ุชูู ุงูุชู ูุชู ุฅุฎุฑุงุฌูุง ุจูุงุณุทุฉ ูุงุฆู `tokenizer`.

ููุง ุณูุฑู ูู ุจุนุถ ุงูุฃูุซูุฉ ุฃุฏูุงูุ ูุฐู ุงูุทุฑููุฉ ูููุฉ ุฌุฏูุง. ุฃููุงูุ ูููููุง ุชุญููู ุณูุณูุฉ ูุงุญุฏุฉ:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ููุง ุฃููุง ุชุชุนุงูู ูุน ุณูุงุณู ูุชุนุฏุฏุฉ ูู ููุณ ุงูููุชุ ุฏูู ุชุบููุฑ ูู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

ูููููุง ุฃูุถูุง ุงูุชุนุจุฆุฉ ููููุง ูุนุฏุฉ ุฃูุฏุงู:

```py
# ุณุชููู ุจุชุนุจุฆุฉ ุงูุณูุงุณู ุญุชู ุทูู ุงูุชุณูุณู ุงูุฃูุตู
model_inputs = tokenizer(sequences, padding="longest")

# ุณุชููู ุจุชุนุจุฆุฉ ุงูุณูุงุณู ุญุชู ุงูุทูู ุงูุฃูุตู ูููููุฐุฌ
# (512 ูููุงุฐุฌ BERT ุฃู DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# ุณุชููู ุจุชุนุจุฆุฉ ุงูุณูุงุณู ุญุชู ุงูุทูู ุงูุฃูุตู ุงููุญุฏุฏ
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

ูููููุง ุฃูุถูุง ุชูุทูุน ุงูุณูุงุณู:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# ุณุชููู ุจุชูุทูุน ุงูุณูุงุณู ุงูุชู ุชููู ุฃุทูู ูู ุงูุทูู ุงูุฃูุตู ูููููุฐุฌ
# (512 ูููุงุฐุฌ BERT ุฃู DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# ุณุชููู ุจุชูุทูุน ุงูุณูุงุณู ุงูุชู ุชููู ุฃุทูู ูู ุงูุทูู ุงูุฃูุตู ุงููุญุฏุฏ
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

ูููู ููุงุฆู `tokenizer` ุงูุชุนุงูู ูุน ุงูุชุญููู ุฅูู ุชูุณูุฑุงุช ุฅุทุงุฑ ุงูุนูู ุงููุญุฏุฏุ ูุงูุชู ูููู ุฅุฑุณุงููุง ุจุนุฏ ุฐูู ูุจุงุดุฑุฉ ุฅูู ุงููููุฐุฌ. ุนูู ุณุจูู ุงููุซุงูุ ูู ุนููุฉ ุงูููุฏ ุงูุชุงููุฉุ ูุทูุจ ูู ุงููุญูู ุงููุบูู ุฅุฑุฌุงุน ุชูุณูุฑุงุช ูู ุฃุทุฑ ุงูุนูู ุงููุฎุชููุฉ - `"pt"` ูุนูุฏ ุชูุณูุฑุงุช PyTorchุ ู `"tf"` ูุนูุฏ ุชูุณูุฑุงุช TensorFlowุ ู `"np"` ูุนูุฏ ูุตูููุงุช NumPy:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# ูุนูุฏ ุชูุณูุฑุงุช PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# ูุนูุฏ ุชูุณูุฑุงุช TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# ูุนูุฏ ูุตูููุงุช NumPy
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## ุงูุฑููุฒ ุงูุฎุงุตุฉ [[special-tokens]]

ุฅุฐุง ุฃููููุง ูุธุฑุฉ ุนูู ูุนุฑูุงุช ุงูุฅุฏุฎุงู ุงูุชู ูุนูุฏูุง ุงููุญูู ุงููุบููุ ูุณูุฑู ุฃููุง ุชุฎุชูู ูููููุง ุนูุง ูุงู ูุฏููุง ุณุงุจููุง:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

ุชูุช ุฅุถุงูุฉ ูุนุฑู ุฑูุฒ ูุงุญุฏ ูู ุงูุจุฏุงูุฉุ ูุขุฎุฑ ูู ุงูููุงูุฉ. ุฏุนูุง ููู ุชุดููุฑ ุชุณูุณูู ูุนุฑูุงุช ุงูุฑููุฒ ุฃุนูุงู ููุฑู ูุง ูู ูุฐุง:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

ุฃุถุงู ุงููุญูู ุงููุบูู ุงููููุฉ ุงูุฎุงุตุฉ `[CLS]` ูู ุงูุจุฏุงูุฉ ูุงููููุฉ ุงูุฎุงุตุฉ `[SEP]` ูู ุงูููุงูุฉ. ูุฐุง ูุฃู ุงููููุฐุฌ ุชู ุชุฏุฑูุจู ูุณุจููุง ุจูุฐู ุงููููุงุชุ ูุฐูู ููุญุตูู ุนูู ููุณ ุงููุชุงุฆุฌ ููุงุณุชุฏูุงูุ ูุญุชุงุฌ ุฅูู ุฅุถุงูุชูุง ุฃูุถูุง. ูุงุญุธ ุฃู ุจุนุถ ุงูููุงุฐุฌ ูุง ุชุถูู ูููุงุช ุฎุงุตุฉุ ุฃู ุชุถูู ูููุงุช ูุฎุชููุฉุ ูุฏ ุชุถูู ุงูููุงุฐุฌ ุฃูุถูุง ูุฐู ุงููููุงุช ุงูุฎุงุตุฉ ููุท ูู ุงูุจุฏุงูุฉุ ุฃู ููุท ูู ุงูููุงูุฉ. ูู ุฃู ุญุงูุ ูุนุฑู ุงููุญูู ุงููุบูู ุฃู ูููุง ูุชููุน ูุณูุชุนุงูู ูุน ูุฐุง ูู ุฃุฌูู.

## ุงูุฎูุงุตุฉ: ูู ุงููุญูู ุงููุบูู ุฅูู ุงููููุฐุฌ [[wrapping-up-from-tokenizer-to-model]]

ุงูุขู ุจุนุฏ ุฃู ุฑุฃููุง ุฌููุน ุงูุฎุทูุงุช ุงููุฑุฏูุฉ ุงูุชู ูุณุชุฎุฏููุง ูุงุฆู `tokenizer` ุนูุฏ ุชุทุจููู ุนูู ุงููุตูุตุ ุฏุนูุง ูุฑู ูุฑุฉ ูุงุญุฏุฉ ุฃุฎูุฑุฉ ููู ููููู ุงูุชุนุงูู ูุน ุณูุงุณู ูุชุนุฏุฏุฉ (ุงูุชุนุจุฆุฉ!)ุ ูุณูุงุณู ุทูููุฉ ุฌุฏูุง (ุงููุทุน!)ุ ูุฃููุงุน ูุชุนุฏุฏุฉ ูู ุงูุชูุณูุฑุงุช ูุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช ุงูุฑุฆูุณูุฉ ุงูุฎุงุตุฉ ุจู:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}