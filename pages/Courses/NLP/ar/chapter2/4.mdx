<FrameworkSwitchCourse {fw} />

# معالجات الرموز {#tokenizers}

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
]} />

{/if}

<Youtube id="VFp38yj8h3A"/>

تعد معالجات الرموز أحد المكونات الأساسية في خط أنابيب معالجة اللغات الطبيعية. لديها هدف واحد: تحويل النص إلى بيانات يمكن معالجتها بواسطة النموذج. حيث لا يمكن للنموذج معالجة سوى الأرقام، لذا تحتاج معالجات الرموز إلى تحويل إدخالات النص لدينا إلى بيانات رقمية. في هذا القسم، سنستكشف بالضبط ما يحدث في خط أنابيب معالجة الرموز.

في مهام معالجة اللغات الطبيعية، فإن البيانات التي تتم معالجتها بشكل عام هي نص خام. إليك مثال على مثل هذا النص:

```
كان جيم هينسون صانع دمى
```

ومع ذلك، لا يمكن للنموذج معالجة سوى الأرقام، لذا نحتاج إلى إيجاد طريقة لتحويل النص الخام إلى أرقام. هذا ما تقوم به معالجات الرموز، وهناك الكثير من الطرق للقيام بذلك. الهدف هو إيجاد التمثيل الأكثر دلالة - أي التمثيل الذي يكون أكثر منطقية بالنسبة للنموذج - وإذا أمكن، التمثيل الأصغر.

دعونا نلقي نظرة على بعض الأمثلة لخوارزميات معالجة الرموز، وحاول الإجابة على بعض الأسئلة التي قد تكون لديك حول معالجة الرموز.

## معالجة الرموز المبنية على الكلمات {#word-based}

<Youtube id="nhJxYji1aho"/>

أول نوع من معالجات الرموز الذي يتبادر إلى الذهن هو _معالجة الرموز المبنية على الكلمات_. من السهل جدًا إعداده واستخدامه مع بعض القواعد فقط، وغالبًا ما يعطي نتائج جيدة. على سبيل المثال، في الصورة أدناه، الهدف هو تقسيم النص الخام إلى كلمات وإيجاد تمثيل رقمي لكل منها:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg" alt="مثال على معالجة الرموز المبنية على الكلمات."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg" alt="مثال على معالجة الرموز المبنية على الكلمات."/>
</div>

هناك طرق مختلفة لتقسيم النص. على سبيل المثال، يمكننا استخدام المسافات البيضاء لمعالجة النص إلى كلمات عن طريق تطبيق دالة `split()` في بايثون:

```py
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```

```python out
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```
```python out
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

هناك أيضًا متغيرات من محللات الكلمات التي لديها قواعد إضافية للترقيم. مع هذا النوع من المحللات، يمكن أن ننتهي ببعض "المفردات" الكبيرة جدًا، حيث يتم تعريف المفردات من خلال العدد الإجمالي للرموز المستقلة التي لدينا في نصنا.

يتم تعيين معرف لكل كلمة، بدءًا من 0 وحتى حجم المفردات. يستخدم النموذج هذه المعرفات لتحديد كل كلمة.

إذا أردنا تغطية لغة بالكامل باستخدام محلل قائم على الكلمات، فسنحتاج إلى معرف لكل كلمة في اللغة، مما سيولد كمية هائلة من الرموز. على سبيل المثال، هناك أكثر من 500,000 كلمة في اللغة الإنجليزية، لذا لبناء خريطة من كل كلمة إلى معرف الإدخال، سنحتاج إلى تتبع هذا العدد من المعرفات. علاوة على ذلك، يتم تمثيل كلمات مثل "كلب" بشكل مختلف عن كلمات مثل "كلاب"، ولن يكون لدى النموذج في البداية أي طريقة لمعرفة أن "كلب" و"كلاب" متشابهان: سيحدد الكلمتين على أنهما غير مرتبطتين. وينطبق الشيء نفسه على كلمات أخرى متشابهة، مثل "يركض" و"جري"، والتي لن يراها النموذج على أنها متشابهة في البداية.

أخيرًا، نحتاج إلى رمز مخصص لتمثيل الكلمات التي ليست في مفرداتنا. وهذا ما يعرف بالرمز "المجهول"، والذي غالبًا ما يتم تمثيله على أنه "[UNK]" أو "&lt;unk&gt;". من السيء أن ترى أن المحلل اللغوي ينتج الكثير من هذه الرموز، لأنه لم يتمكن من استرداد تمثيل معقول لكلمة ما، وأنت تفقد المعلومات على طول الطريق. الهدف عند صياغة المفردات هو القيام بذلك بطريقة تجعل المحلل اللغوي يحلل أقل عدد ممكن من الكلمات إلى الرمز المجهول.

هناك طريقة لتقليل كمية الرموز المجهولة وهي الانتقال إلى مستوى أعمق، باستخدام محلل قائم على _الحروف_.

## محلل قائم على الحروف[[character-based]]

<Youtube id="ssLq_EK2jLE"/>

تقوم المحللات القائمة على الحروف بتقسيم النص إلى حروف، بدلاً من الكلمات. لهذا فائدتان رئيسيتان:

- المفردات أصغر بكثير.
- هناك عدد أقل بكثير من الرموز خارج المفردات (المجهولة)، حيث يمكن بناء كل كلمة من الحروف.

ولكن هنا أيضًا تنشأ بعض الأسئلة المتعلقة بالمسافات والترقيم:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg" alt="مثال على التحليل القائم على الحروف."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg" alt="مثال على التحليل القائم على الحروف."/>
</div>

هذا النهج ليس مثاليًا أيضًا. حيث أن التمثيل الآن يعتمد على الحروف بدلاً من الكلمات، يمكن للمرء أن يجادل بأنه، بديهيًا، أقل دلالة: كل حرف لا يعني الكثير بمفرده، في حين أن هذا هو الحال مع الكلمات. ومع ذلك، يختلف هذا مرة أخرى وفقًا للغة؛ في اللغة الصينية، على سبيل المثال، يحمل كل حرف معلومات أكثر من حرف في لغة لاتينية.

شيء آخر يجب أخذه في الاعتبار هو أننا سننتهي بكمية كبيرة جدًا من الرموز التي يجب معالجتها بواسطة نموذجنا: في حين أن الكلمة ستكون رمزًا واحدًا فقط مع محلل قائم على الكلمات، يمكن أن تتحول بسهولة إلى 10 رموز أو أكثر عند تحويلها إلى حروف.

للحصول على أفضل ما في العالمين، يمكننا استخدام تقنية ثالثة تجمع بين النهجين: *التحليل القائم على الكلمات الفرعية*.

## التحليل القائم على الكلمات الفرعية[[subword-tokenization]]

<Youtube id="zHvTiHr506c"/>

تعتمد خوارزميات التحليل القائم على الكلمات الفرعية على المبدأ القائل بأن الكلمات المستخدمة بشكل متكرر لا ينبغي تقسيمها إلى كلمات فرعية أصغر، ولكن يجب تحليل الكلمات النادرة إلى كلمات فرعية ذات معنى.

على سبيل المثال، يمكن اعتبار كلمة "annoyingly" كلمة نادرة ويمكن تحليلها إلى "annoying" و "ly". من المحتمل أن تظهر كلتا الكلمتين بشكل متكرر أكثر ككلمات فرعية منفصلة، في حين أن معنى "annoyingly" يتم الحفاظ عليه من خلال المعنى المركب لـ "annoying" و "ly".

هنا مثال يوضح كيف ستقوم خوارزمية التحليل القائم على الكلمات الفرعية بتحليل تسلسل "Let's do tokenization!":

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg" alt="خوارزمية التحليل القائم على الكلمات الفرعية."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg" alt="خوارزمية التحليل القائم على الكلمات الفرعية."/>
</div>

تنتهي هذه الكلمات الفرعية بتوفير الكثير من المعنى الدلالي: على سبيل المثال، في المثال أعلاه، تم تقسيم "tokenization" إلى "token" و "ization"، وهما رمزان لهما معنى دلالي بينما يكونان فعالين من حيث المساحة (يتم استخدام رمزين فقط لتمثيل كلمة طويلة). هذا يسمح لنا بالحصول على تغطية جيدة نسبيًا مع مفردات صغيرة، ولا يوجد تقريبًا أي رموز مجهولة.

هذا النهج مفيد بشكل خاص في اللغات اللاصقة مثل التركية، حيث يمكنك تشكيل كلمات معقدة طويلة (تقريبًا) بشكل تعسفي عن طريق ربط الكلمات الفرعية معًا.

### وأكثر من ذلك![[and-more]]

ليس من المستغرب أن هناك العديد من التقنيات الأخرى هناك. على سبيل المثال لا الحصر:

- Byte-level BPE، كما هو مستخدم في GPT-2
- WordPiece، كما هو مستخدم في BERT
- SentencePiece أو Unigram، كما هو مستخدم في العديد من النماذج متعددة اللغات

الآن يجب أن يكون لديك معرفة كافية حول كيفية عمل المحللات اللغوية للبدء في استخدام واجهة برمجة التطبيقات.

## التحميل والحفظ[[loading-and-saving]]

تحميل وحفظ المحللات اللغوية بسيط مثل النماذج. في الواقع، يعتمد على نفس الطريقتين: `from_pretrained()` و `save_pretrained()`. ستقوم هذه الطرق بتحميل أو حفظ الخوارزمية المستخدمة بواسطة المحلل اللغوي (قليلًا مثل *بنية* النموذج) بالإضافة إلى مفرداته (قليلًا مثل *أوزان* النموذج).

يتم تحميل محلل BERT اللغوي المدرب بنفس نقطة التفتيش مثل BERT بنفس طريقة تحميل النموذج، باستثناء أننا نستخدم فئة `BertTokenizer`:

```py
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

{#if fw === 'pt'}
على غرار `AutoModel`، ستقوم فئة `AutoTokenizer` بالتقاط فئة المحلل اللغوي الصحيحة في المكتبة بناءً على اسم نقطة التفتيش، ويمكن استخدامها مباشرة مع أي نقطة تفتيش:

{:else}
على غرار `TFAutoModel`، ستقوم فئة `AutoTokenizer` بالتقاط فئة المحلل اللغوي الصحيحة في المكتبة بناءً على اسم نقطة التفتيش، ويمكن استخدامها مباشرة مع أي نقطة تفتيش:

{/if}

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

يمكننا الآن استخدام المحلل اللغوي كما هو موضح في القسم السابق:

```python
tokenizer("Using a Transformer network is simple")
```

```python out
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

حفظ المحلل اللغوي مطابق لحفظ النموذج:

```py
tokenizer.save_pretrained("directory_on_my_computer")
```
سنتكلم أكثر عن `token_type_ids` في [الفصل 3](/course/chapter3)، وسنوضح مفتاح `attention_mask` لاحقاً. أولاً، دعنا نرى كيف يتم توليد `input_ids`. للقيام بذلك، سنحتاج إلى النظر في الطرق الوسيطة للمحلل الرمزي.

## الترميز[[encoding]]

<Youtube id="Yffk5aydLzg"/>

يُعرف تحويل النص إلى أرقام باسم _الترميز_. ويتم الترميز في عملية من خطوتين: التحليل الرمزي، يليه التحويل إلى معرفات الإدخال.

كما رأينا، تتمثل الخطوة الأولى في تقسيم النص إلى كلمات (أو أجزاء من الكلمات، رموز الترقيم، إلخ)، والتي تُسمى عادةً *الرموز*. هناك العديد من القواعد التي يمكن أن تحكم تلك العملية، وهو السبب في أننا نحتاج إلى إنشاء مثيل للمحلل الرمزي باستخدام اسم النموذج، للتأكد من استخدامنا لنفس القواعد التي تم استخدامها عند تدريب النموذج مسبقاً.

وتتمثل الخطوة الثانية في تحويل تلك الرموز إلى أرقام، بحيث يمكننا بناء مصفوفة منها وإدخالها إلى النموذج. للقيام بذلك، يمتلك المحلل الرمزي *معجم*، وهو الجزء الذي نقوم بتنزيله عندما ننشئه باستخدام طريقة `from_pretrained()`. مرة أخرى، نحتاج إلى استخدام نفس المعجم المستخدم عند تدريب النموذج مسبقاً.

لفهم أفضل للخطوتين، سنستكشفها بشكل منفصل. لاحظ أننا سنستخدم بعض الطرق التي تؤدي أجزاء من خط أنابيب التحليل الرمزي بشكل منفصل لإظهار النتائج الوسيطة لتلك الخطوات، ولكن في الممارسة العملية، يجب عليك استدعاء المحلل الرمزي مباشرة على إدخالاتك (كما هو موضح في القسم 2).

### التحليل الرمزي[[tokenization]]

تتم عملية التحليل الرمزي بواسطة طريقة `tokenize()` للمحلل الرمزي:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

مخرجات هذه الطريقة هي قائمة من السلاسل النصية، أو الرموز:

```python out
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

هذا المحلل الرمزي هو محلل رمزي فرعي: يقوم بتقسيم الكلمات حتى يحصل على رموز يمكن تمثيلها بمعجمه. هذا هو الحال هنا مع `transformer`، والذي يتم تقسيمه إلى رمزين: `transform` و `##er`.

### من الرموز إلى معرفات الإدخال[[from-tokens-to-input-ids]]

يتولى التحويل إلى معرفات الإدخال طريقة `convert_tokens_to_ids()` للمحلل الرمزي:

```py
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

```python out
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

يمكن استخدام هذه المخرجات، بعد تحويلها إلى مصفوفة إطار العمل المناسب، كإدخالات لنموذج كما رأينا سابقاً في هذا الفصل.

<Tip>

✏️ **جربها!** كرر الخطوتين الأخيرتين (التحليل الرمزي والتحويل إلى معرفات الإدخال) على جمل الإدخال التي استخدمناها في القسم 2 ("I've been waiting for a HuggingFace course my whole life." و "I hate this so much!"). تأكد من حصولك على نفس معرفات الإدخال التي حصلنا عليها سابقاً!

</Tip>

## فك الترميز[[decoding]]

*فك الترميز* هو القيام بالعكس: من مؤشرات المعجم، نريد الحصول على سلسلة نصية. يمكن القيام بذلك باستخدام طريقة `decode()` كما يلي:

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

لاحظ أن طريقة `decode` لا تقوم فقط بتحويل المؤشرات مرة أخرى إلى رموز، ولكنها أيضاً تجمع الرموز التي كانت جزءاً من نفس الكلمات لإنتاج جملة مقروءة. سيكون هذا السلوك مفيداً للغاية عندما نستخدم نماذج تتنبأ بنص جديد (سواء كان النص المولد من موجه، أو لمشاكل التسلسل إلى تسلسل مثل الترجمة أو التلخيص).

الآن يجب أن تفهم العمليات الذرية التي يمكن للمحلل الرمزي التعامل معها: التحليل الرمزي، والتحويل إلى معرفات، وتحويل المعرفات مرة أخرى إلى سلسلة نصية. ومع ذلك، فقد قمنا للتو بتجاوز قمة جبل الجليد. في القسم التالي، سنأخذ نهجنا إلى حدوده ونلقي نظرة على كيفية التغلب عليها.