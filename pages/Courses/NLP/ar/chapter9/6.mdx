# ميزات واجهة المستخدم المتقدمة [[advanced-interface-features]]

<CourseFloatingBanner chapter={9}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section6.ipynb"},
]} />

الآن بعد أن أصبح بإمكاننا بناء ومشاركة واجهة مستخدم أساسية، دعنا نستكشف بعض الميزات الأكثر تقدماً مثل الحالة والتفسير.

### استخدام الحالة لإبقاء البيانات [[using-state-to-persist-data]]

يدعم Gradio *حالة الجلسة*، حيث تستمر البيانات عبر عدة إرسالات ضمن تحميل الصفحة. حالة الجلسة مفيدة لبناء عروض توضيحية، على سبيل المثال، للمحادثات الآلية حيث تريد إبقاء البيانات أثناء تفاعل المستخدم مع النموذج. لاحظ أن حالة الجلسة لا تشارك البيانات بين مستخدمين مختلفين لنموذجك.

لتخزين البيانات في حالة الجلسة، تحتاج إلى القيام بثلاثة أشياء:

1. تمرير *معامل إضافي* إلى دالتك، والذي يمثل حالة واجهة المستخدم.
1. في نهاية الدالة، قم بإرجاع القيمة المحدثة للحالة كـ *قيمة إرجاع إضافية*.
1. أضف مكونات الإدخال 'state' والإخراج 'state' عند إنشاء واجهة المستخدم الخاصة بك `Interface`.

انظر مثال المحادثة الآلية أدناه:

```py
import random

import gradio as gr


def chat(message, history):
    history = history or []
    if message.startswith("How many"):
        response = random.randint(1, 10)
    elif message.startswith("How"):
        response = random.choice(["Great", "Good", "Okay", "Bad"])
    elif message.startswith("Where"):
        response = random.choice(["Here", "There", "Somewhere"])
    else:
        response = "I don't know"
    history.append((message, response))
    return history, history


iface = gr.Interface(
    chat,
    ["text", "state"],
    ["chatbot", "state"],
    allow_screenshot=False,
    allow_flagging="never",
)
iface.launch()
```

<iframe src="https://course-demos-Chatbot-Demo.hf.space" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

لاحظ كيف تستمر حالة مكون الإخراج عبر الإرسالات.
ملاحظة: يمكنك تمرير قيمة افتراضية لمعامل الحالة،
والتي تستخدم كقيمة أولية للحالة.

### استخدام التفسير لفهم التنبؤات [[using-interpretation-to-understand-predictions]]

معظم نماذج التعلم الآلي عبارة عن صناديق سوداء والمنطق الداخلي للوظيفة مخفي عن المستخدم النهائي. لتشجيع الشفافية، جعلنا من السهل جداً إضافة التفسير إلى نموذجك ببساطة عن طريق تعيين معامل التفسير في فئة واجهة المستخدم إلى الافتراضي. يسمح هذا لمستخدميك بفهم أي أجزاء من الإدخال مسؤولة عن الإخراج. ألق نظرة على واجهة المستخدم البسيطة أدناه والتي تعرض مصنف الصور الذي يتضمن أيضًا التفسير:

```py
import requests
import tensorflow as tf

import gradio as gr

inception_net = tf.keras.applications.MobileNetV2()  # تحميل النموذج

# تنزيل التصنيفات التي يمكن قراءتها بواسطة الإنسان لـ ImageNet.
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")


def classify_image(inp):
    inp = inp.reshape((-1, 224, 224, 3))
    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)
    prediction = inception_net.predict(inp).flatten()
    return {labels[i]: float(prediction[i]) for i in range(1000)}


image = gr.Image(shape=(224, 224))
label = gr.Label(num_top_classes=3)

title = "Gradio Image Classifiction + Interpretation Example"
gr.Interface(
    fn=classify_image, inputs=image, outputs=label, interpretation="default", title=title
).launch()
```

اختبر دالة التفسير عن طريق إرسال إدخال ثم النقر على "تفسير" أسفل مكون الإخراج.

<iframe src="https://course-demos-gradio-image-interpretation.hf.space" frameBorder="0" height="570" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

بالإضافة إلى طريقة التفسير الافتراضية التي يوفرها Gradio، يمكنك أيضًا تحديد `shap` لمعامل `interpretation` وتعيين معامل `num_shap`. يستخدم هذا التفسير القائم على Shapley، والذي يمكنك قراءة المزيد عنه [هنا](https://christophm.github.io/interpretable-ml-book/shap.html).
أخيرًا، يمكنك أيضًا تمرير دالة التفسير الخاصة بك إلى معامل `interpretation`. انظر مثالًا في صفحة Gradio للبدء [هنا](https://gradio.app/getting_started/).

هذا يلخص بحثنا العميق في فئة `Interface` من Gradio. كما رأينا، تجعل هذه الفئة من السهل إنشاء عروض توضيحية للتعلم الآلي في بضع سطور من كود بايثون. ومع ذلك، في بعض الأحيان سترغب في تخصيص عرضك التوضيحي عن طريق تغيير التخطيط أو ربط وظائف التنبؤ المتعددة معًا. أليس من الرائع إذا استطعنا بطريقة ما تقسيم واجهة المستخدم إلى "كتل" قابلة للتخصيص؟ لحسن الحظ، هناك! هذا هو موضوع القسم الأخير.