<FrameworkSwitchCourse {fw} />

# ุชูุฎูุต ุงููุตูุต [[summarization]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
]} />

{/if}


ูู ูุฐุง ุงููุณูุ ุณูููู ูุธุฑุฉ ุนูู ููููุฉ ุงุณุชุฎุฏุงู ููุงุฐุฌ ุงููุญูู ูุชูุฎูุต ุงููุซุงุฆู ุงูุทูููุฉ ูู ููุฎุตุงุชุ ููู ูููุฉ ุชูุนุฑู ุจุงุณู _ุชูุฎูุต ุงููุตูุต_. ูุฐู ูุงุญุฏุฉ ูู ุฃุตุนุจ ููุงู ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ ูุฃููุง ุชุชุทูุจ ูุฌููุนุฉ ูู ุงููุฏุฑุงุชุ ูุซู ููู ุงูููุงุทุน ุงูุทูููุฉ ูุชูููุฏ ูุต ูุชูุงุณู ููุฎุต ุงูููุงุถูุน ุงูุฑุฆูุณูุฉ ูู ุงููุซููุฉ. ููุน ุฐููุ ุนูุฏ ุงูููุงู ุจูุง ุจุดูู ุฌูุฏุ ูุฅู ุชูุฎูุต ุงููุตูุต ูู ุฃุฏุงุฉ ูููุฉ ูููู ุฃู ุชุณุฑุน ูุฎุชูู ุงูุนูููุงุช ุงูุชุฌุงุฑูุฉ ูู ุฎูุงู ุชุฎููู ุนุจุก ุงูุฎุจุฑุงุก ูู ุงููุฌุงู ููุฑุงุกุฉ ุงููุซุงุฆู ุงูุทูููุฉ ุจุงูุชูุตูู.

<Youtube id="yHnr5Dk2zCI"/>

ุนูู ุงูุฑุบู ูู ูุฌูุฏ ุงูุนุฏูุฏ ูู ุงูููุงุฐุฌ ุงููุนุฏูุฉ ูุณุจููุง ููุชูุฎูุต ุนูู [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads)ุ ุฅูุง ุฃู ุฌููุน ูุฐู ุงูููุงุฐุฌ ุชูุฑูุจูุง ููุงุณุจุฉ ูููุซุงุฆู ุงูุฅูุฌููุฒูุฉ ููุท. ูุฐููุ ูุฅุถุงูุฉ ููุณุฉ ูุฎุชููุฉ ูู ูุฐุง ุงููุณูุ ุณูููู ุจุชุฏุฑูุจ ูููุฐุฌ ุซูุงุฆู ุงููุบุฉ ููุฅูุฌููุฒูุฉ ูุงูุฅุณุจุงููุฉ. ูุจููุงูุฉ ูุฐุง ุงููุณูุ ุณูููู ูุฏูู [ูููุฐุฌ](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) ููููู ุชูุฎูุต ูุฑุงุฌุนุงุช ุงูุนููุงุก ูุซู ุงููุฑุงุฌุนุฉ ุงูููุถุญุฉ ููุง:

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ููุง ุณูุฑูุ ูุฅู ูุฐู ุงูููุฎุตุงุช ููุฌุฒุฉ ูุฃููุง ูุณุชูุงุฏุฉ ูู ุงูุนูุงููู ุงูุชู ููุฏููุง ุงูุนููุงุก ูู ูุฑุงุฌุนุงุชูู ููููุชุฌุงุช. ุฏุนูุง ูุจุฏุฃ ุจุชุฌููุน ูุฌููุนุฉ ุซูุงุฆูุฉ ุงููุบุฉ ููุงุณุจุฉ ููุฐู ุงููููุฉ.

## ุฅุนุฏุงุฏ ูุฌููุนุฉ ุจูุงูุงุช ูุชุนุฏุฏุฉ ุงููุบุงุช [[preparing-a-multilingual-corpus]]

ุณูุณุชุฎุฏู [ูุฌููุนุฉ ุจูุงูุงุช ูุฑุงุฌุนุงุช ุฃูุงุฒูู ูุชุนุฏุฏุฉ ุงููุบุงุช](https://huggingface.co/datasets/amazon_reviews_multi) ูุฅูุดุงุก ููุฎุต ุซูุงุฆู ุงููุบุฉ. ุชุชููู ูุฐู ุงููุฌููุนุฉ ูู ูุฑุงุฌุนุงุช ููุชุฌุงุช ุฃูุงุฒูู ุจุณุช ูุบุงุชุ ูุชุณุชุฎุฏู ุนุงุฏุฉู ูููุงุณ ุฃุฏุงุก ุงููุตููุงุช ูุชุนุฏุฏุฉ ุงููุบุงุช. ููุน ุฐููุ ูุธุฑูุง ูุฃู ูู ูุฑุงุฌุนุฉ ูุตุญูุจุฉ ุจุนููุงู ูุตูุฑุ ูููููุง ุงุณุชุฎุฏุงู ุงูุนูุงููู ูููุฎุตุงุช ูุณุชูุฏูุฉ ููุชุนูู ูููุง ูููุฐุฌูุง! ููุจุฏุกุ ุฏุนูุง ูููู ุจุชูุฒูู ุงููุฌููุนุงุช ุงููุฑุนูุฉ ุงูุฅูุฌููุฒูุฉ ูุงูุฅุณุจุงููุฉ ูู Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

ููุง ุชุฑูุ ููุงู 200,000 ูุฑุงุฌุนุฉ ููู ูุบุฉ ูู ูุฌููุนุฉ `train`ุ ู5,000 ูุฑุงุฌุนุฉ ููู ูู ูุฌููุนุชู `validation` ู`test`. ุชุชูุงุฌุฏ ูุนูููุงุช ุงููุฑุงุฌุนุฉ ุงูุชู ููุชู ุจูุง ูู ุนููุฏู `review_body` ู`review_title`. ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุจุนุถ ุงูุฃูุซูุฉ ูู ุฎูุงู ุฅูุดุงุก ุฏุงูุฉ ุจุณูุทุฉ ุชุฃุฎุฐ ุนููุฉ ุนุดูุงุฆูุฉ ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ุงูุชูููุงุช ุงูุชู ุชุนูููุงูุง ูู [ุงููุตู 5](/course/chapter5):

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does itโs job and itโs gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesnโt get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ูู ุจุชุบููุฑ ุงูุจุฐุฑุฉ ุงูุนุดูุงุฆูุฉ ูู ุฃูุฑ `Dataset.shuffle()` ูุงุณุชูุดุงู ูุฑุงุฌุนุงุช ุฃุฎุฑู ูู ุงููุฌููุนุฉ. ุฅุฐุง ููุช ุชุชุญุฏุซ ุงูุฅุณุจุงููุฉุ ูู ุจุงูุงุทูุงุน ุนูู ุจุนุถ ุงููุฑุงุฌุนุงุช ูู `spanish_dataset` ููุนุฑูุฉ ูุง ุฅุฐุง ูุงูุช ุงูุนูุงููู ุชุจุฏู ููุฎุตุงุช ูุนูููุฉ ุฃูุถูุง.

</Tip>

ุชูุถุญ ูุฐู ุงูุนููุฉ ุชููุน ุงููุฑุงุฌุนุงุช ุงูุชู ูุฌุฏูุง ุนุงุฏุฉ ุนุจุฑ ุงูุฅูุชุฑูุชุ ูุงูุชู ุชุชุฑุงูุญ ูู ุงูุฅูุฌุงุจูุฉ ุฅูู ุงูุณูุจูุฉ (ููู ูุง ุจููููุง). ุนูู ุงูุฑุบู ูู ุฃู ุงููุซุงู ุจุนููุงู "meh" ููุณ ูููุฏูุง ุฌุฏูุงุ ุฅูุง ุฃู ุงูุนูุงููู ุงูุฃุฎุฑู ุชุจุฏู ููุฎุตุงุช ุฌูุฏุฉ ูููุฑุงุฌุนุงุช ููุณูุง. ุณูุณุชุบุฑู ุชุฏุฑูุจ ูููุฐุฌ ุงูุชูุฎูุต ุนูู ุฌููุน ุงููุฑุงุฌุนุงุช ุงูุฃุฑุจุนูุงุฆุฉ ุฃูู ููุชูุง ุทูููุงู ููุบุงูุฉ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุฑุณูููุฉ ูุงุญุฏุฉุ ูุฐูู ุจุฏูุงู ูู ุฐููุ ุณูุฑูุฒ ุนูู ุชูููุฏ ููุฎุตุงุช ููุฌุงู ูุงุญุฏ ูู ุงูููุชุฌุงุช. ููุญุตูู ุนูู ููุฑุฉ ุนู ุงููุฌุงูุงุช ุงูุชู ูููููุง ุงูุงุฎุชูุงุฑ ูููุงุ ุฏุนูุง ูุญูู `english_dataset` ุฅูู `pandas.DataFrame` ููุญุณุจ ุนุฏุฏ ุงููุฑุงุฌุนุงุช ููู ูุฆุฉ ูู ูุฆุงุช ุงูููุชุฌุงุช:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```python
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

ุงูููุชุฌุงุช ุงูุฃูุซุฑ ุดููุนูุง ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฅูุฌููุฒูุฉ ูู ุนู ุงูุฃุฏูุงุช ุงูููุฒููุฉุ ูุงูููุงุจุณุ ูุงูุฅููุชุฑูููุงุช ุงููุงุณูููุฉ. ูููู ููุงูุชุฒุงู ุจููุถูุน ุฃูุงุฒููุ ุฏุนูุง ูุฑูุฒ ุนูู ุชูุฎูุต ูุฑุงุฌุนุงุช ุงููุชุจ - ููู ุงูููุงูุฉุ ูุฐู ูู ุงูููุฑุฉ ุงูุชู ุชุฃุณุณุช ุนูููุง ุงูุดุฑูุฉ! ูููููุง ุฃู ูุฑู ูุฆุชูู ูู ุงูููุชุฌุงุช ุชูุงุณุจ ูุฐุง ุงููุตู (`ุงููุชุงุจ` ู`ุดุฑุงุก ุงููุชุงุจ ุงูุฅููุชุฑููู ุงูุฑููู`)ุ ูุฐุง ุฏุนูุง ููุตููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ููุชุง ุงููุบุชูู ููุฐู ุงูููุชุฌุงุช ููุท. ููุง ุฑุฃููุง ูู [ุงููุตู 5](/course/chapter5)ุ ูุฅู ุฏุงูุฉ `Dataset.filter()` ุชุณูุญ ููุง ุจุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจูุนุงููุฉุ ูุฐุง ูููููุง ุชุนุฑูู ุฏุงูุฉ ุจุณูุทุฉ ููููุงู ุจุฐูู:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

ุงูุขู ุนูุฏูุง ูุทุจู ูุฐู ุงูุฏุงูุฉ ุนูู `english_dataset` ู`spanish_dataset`ุ ุณุชุชุถูู ุงููุชูุฌุฉ ููุท ุงูุตููู ุงููุชุนููุฉ ุจูุฆุงุช ุงููุชุจ. ููุจู ุชุทุจูู ุงูุชุตููุฉุ ุฏุนูุง ูุบูุฑ ุชูุณูู `english_dataset` ูู `"pandas"` ุฅูู `"arrow"`:

```python
english_dataset.reset_format()
```

ุจุนุฏ ุฐูู ูููููุง ุชุทุจูู ุฏุงูุฉ ุงูุชุตููุฉุ ูููุญุต ููุณูุงูุฉุ ุฏุนูุง ูุชูุญุต ุนููุฉ ูู ุงููุฑุงุฌุนุงุช ููุฑู ุฅู ูุงูุช ุจุงููุนู ุนู ุงููุชุจ:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

ุญุณููุงุ ูููููุง ุฃู ูุฑู ุฃู ุงููุฑุงุฌุนุงุช ููุณุช ููุท ุนู ุงููุชุจุ ููุฏ ุชุดูุฑ ุฅูู ุฃุดูุงุก ูุซู ุงูุชูุงููู ูุงูุชุทุจููุงุช ุงูุฅููุชุฑูููุฉ ูุซู OneNote. ููุน ุฐููุ ูุจุฏู ุงููุฌุงู ููุงุณุจูุง ูุชุฏุฑูุจ ูููุฐุฌ ุงูุชูุฎูุต. ููุจู ุฃู ููุธุฑ ุฅูู ุงูููุงุฐุฌ ุงููุฎุชููุฉ ุงูููุงุณุจุฉ ููุฐู ุงููููุฉุ ูุฏููุง ุฎุทูุฉ ุฃุฎูุฑุฉ ูู ุฅุนุฏุงุฏ ุงูุจูุงูุงุช: ุฏูุฌ ุงููุฑุงุฌุนุงุช ุงูุฅูุฌููุฒูุฉ ูุงูุฅุณุจุงููุฉ ูู ูุงุฆู `DatasetDict` ูุงุญุฏ. ุชููุฑ ููุชุจุฉ ๐ค Datasets ุฏุงูุฉ `concatenate_datasets()` ูููุฏุฉ (ููุง ููุญู ุงูุงุณู) ุณุชุถุน ูุงุฆูู `Dataset` ููู ุจุนุถููุง ุงูุจุนุถ. ูุฐุงุ ูุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ุซูุงุฆูุฉ ุงููุบุฉุ ุณูููู ุจุงูุชูุฑูุฑ ุนูู ูู ุชูุณููุ ูุฏูุฌ ูุฌููุนุงุช ุงูุจูุงูุงุช ูุฐูู ุงูุชูุณููุ ููุฒุฌ ุงููุชูุฌุฉ ูุถูุงู ุนุฏู ุชุนูู ุงููููุฐุฌ ุจุดูู ููุฑุท ุนูู ูุบุฉ ูุงุญุฏุฉ:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# ุชููุฏ ุจุนุถ ุงูุฃูุซูุฉ
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAรADO'
'>> Review: Me llegรณ el dรญa que tocaba, junto a otros libros que pedรญ, pero la caja llegรณ en mal estado lo cual daรฑรณ las esquinas de los libros porque venรญan sin protecciรณn (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

ูุฐุง ุจุงูุชุฃููุฏ ูุจุฏู ูุฒูุฌูุง ูู ุงููุฑุงุฌุนุงุช ุงูุฅูุฌููุฒูุฉ ูุงูุฅุณุจุงููุฉ! ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญ ูุฏููุง ูุฌููุนุฉ ุจูุงูุงุช ููุชุฏุฑูุจุ ููุงู ุดูุก ุฃุฎูุฑ ูุฌุจ ุงูุชุญูู ููู ููู ุชูุฒูุน ุงููููุงุช ูู ุงููุฑุงุฌุนุงุช ูุนูุงููููุง. ูุฐุง ููู ุจุดูู ุฎุงุต ูู ููุงู ุงูุชูุฎูุตุ ุญูุซ ูููู ููููุฎุตุงุช ุงููุฑุฌุนูุฉ ุงููุตูุฑุฉ ูู ุงูุจูุงูุงุช ุฃู ุชุญูุฒ ุงููููุฐุฌ ูุฅูุชุงุฌ ููุฎุตุงุช ูู ูููุฉ ูุงุญุฏุฉ ุฃู ูููุชูู ููุท. ุชูุถุญ ุงูุฑุณูู ุงูุจูุงููุฉ ุฃุฏูุงู ุชูุฒูุนุงุช ุงููููุงุชุ ููููููุง ุฃู ูุฑู ุฃู ุงูุนูุงููู ุชููู ุจุดุฏุฉ ูุญู ูููุฉ ูุงุญุฏุฉ ุฃู ูููุชูู ููุท:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="ุชูุฒูุนุงุช ุนุฏุฏ ุงููููุงุช ููุนูุงููู ูุงููุฑุงุฌุนุงุช."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="ุชูุฒูุนุงุช ุนุฏุฏ ุงููููุงุช ููุนูุงููู ูุงููุฑุงุฌุนุงุช."/>
</div>

ููุชุนุงูู ูุน ูุฐุงุ ุณูููู ุจุชุตููุฉ ุงูุฃูุซูุฉ ุฐุงุช ุงูุนูุงููู ุงููุตูุฑุฉ ุฌุฏูุง ุญุชู ูุชููู ูููุฐุฌูุง ูู ุฅูุชุงุฌ ููุฎุตุงุช ุฃูุซุฑ ุฅุซุงุฑุฉ ููุงูุชูุงู. ุจูุง ุฃููุง ูุชุนุงูู ูุน ูุตูุต ุฅูุฌููุฒูุฉ ูุฅุณุจุงููุฉุ ูููููุง ุงุณุชุฎุฏุงู ุชูุฑูุจ ุชูุฑูุจู ูุชูุณูู ุงูุนูุงููู ุนูู ุงููุณุงูุงุช ุงูุจูุถุงุกุ ุซู ุงุณุชุฎุฏุงู ุทุฑููุฉ `Dataset.filter()` ุงูููุซููุฉ ููุง ููู:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```
ุงูุขู ุจุนุฏ ุฃู ูููุง ุจุชุญุถูุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุงุ ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุจุนุถ ููุงุฐุฌ ุงููุญูู (Transformer) ุงููุญุชููุฉ ุงูุชู ูููู ุถุจุทูุง ุงูุฏููู ุนูู ูุฐู ุงููุฌููุนุฉ!

## ููุงุฐุฌ ูุชูุฎูุต ุงููุตูุต [[models-for-text-summarization]]

ุฅุฐุง ููุฑุช ูู ุงูุฃูุฑุ ูุฅู ุชูุฎูุต ุงููุตูุต ูู ูููุฉ ูุดุงุจูุฉ ูุชุฑุฌูุฉ ุงูุขูุฉ: ูุฏููุง ูุต ูุซู ูุฑุงุฌุนุฉ ูุฑูุฏ "ุชุฑุฌูุชูุง" ุฅูู ูุณุฎุฉ ุฃูุตุฑ ุชูุชูุท ุงูููุฒุงุช ุงูุจุงุฑุฒุฉ ูููุต ุงูุฃุตูู. ูุจูุงุกู ุนูู ุฐููุ ูุฅู ูุนุธู ููุงุฐุฌ ุงููุญูู ูุชูุฎูุต ุงููุตูุต ุชุนุชูุฏ ุจููุฉ ุงูุชุฑููุฒ-ูู ุงูุชุฑููุฒ (encoder-decoder) ุงูุชู ุชุนุฑููุง ุนูููุง ูู [ุงููุตู 1](/course/chapter1)ุ ุนูู ุงูุฑุบู ูู ูุฌูุฏ ุจุนุถ ุงูุงุณุชุซูุงุกุงุช ูุซู ุนุงุฆูุฉ ููุงุฐุฌ GPT ุงูุชู ูููู ุงุณุชุฎุฏุงููุง ุฃูุถูุง ูุชูุฎูุต ุงููุตูุต ูู ุฅุนุฏุงุฏุงุช ููููุฉ ุงูุชุตููุจ. ูุฏุฑุฌ ุงูุฌุฏูู ุงูุชุงูู ุจุนุถ ุงูููุงุฐุฌ ุงูููุฏุฑุจุฉ ูุณุจููุง ูุงูุดุงุฆุนุฉ ุงูุชู ูููู ุถุจุทูุง ุงูุฏููู ูุชูุฎูุต ุงููุตูุต.

| ูููุฐุฌ ุงููุญูู | ุงููุตู                                                                                                                                                                                                    | ูุชุนุฏุฏ ุงููุบุงุชุ |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | ุนูู ุงูุฑุบู ูู ุชุฏุฑูุจู ููููุฐุฌ ูุบูู ุชูุจุคู ุฐุงุชูุ ููููู ุฌุนู GPT-2 ูููุฏ ููุฎุตุงุช ุจุฅุถุงูุฉ "TL;DR" ูู ููุงูุฉ ุงููุต ุงููุฏุฎู.                                                                          |      โ       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | ูุณุชุฎุฏู ูุฏููุง ูุณุจููุง ููุชูุจุค ุจุงูุฌูู ุงููููุนุฉ ูู ุงููุตูุต ูุชุนุฏุฏุฉ ุงูุฌูู. ูุฐุง ุงููุฏู ุงููุณุจู ุฃูุฑุจ ุฅูู ุชูุฎูุต ุงููุตูุต ูู ุงูููุฐุฌุฉ ุงููุบููุฉ ุงูุชูููุฏูุฉ ููุญูู ูุชุงุฆุฌ ุนุงููุฉ ูู ุงููุนุงููุฑ ุงูุดุงุฆุนุฉ. |      โ       |
|     [T5](https://huggingface.co/t5-base)      | ุจููุฉ ูุญูู ุนุงูููุฉ ุชุตูุบ ุฌููุน ุงูููุงู ูู ุฅุทุงุฑ ูุต-ุฅูู-ูุตุ ุนูู ุณุจูู ุงููุซุงูุ ุชูุณูู ุงูุฅุฏุฎุงู ูููููุฐุฌ ูุชูุฎูุต ูุซููุฉ ูู `summarize: ARTICLE`.                              |      โ       |
|     [mT5](https://huggingface.co/google/mt5-base)     | ูุณุฎุฉ ูุชุนุฏุฏุฉ ุงููุบุงุช ูู T5ุ ููุฏุฑุจุฉ ูุณุจููุง ุนูู ูุฌููุนุฉ ุจูุงูุงุช Common Crawl ูุชุนุฏุฏุฉ ุงููุบุงุช (mC4)ุ ุชุบุทู 101 ูุบุฉ.                                                                                                |      โ       |
|    [BART](https://huggingface.co/facebook/bart-base)     | ุจููุฉ ูุญูู ุฌุฏูุฏุฉ ุชุญุชูู ุนูู ูู ูู ุทุจูุฉ ุงูุชุฑููุฒ ูุทุจูุฉ ูู ุงูุชุฑููุฒ ุงููุฏุฑุจุชูู ุนูู ุฅุนุงุฏุฉ ุจูุงุก ุงูุฅุฏุฎุงู ุงููุดููุ ูุชุฌูุน ุจูู ูุฎุทุทุงุช ุงูุชุฏุฑูุจ ุงููุณุจู ูููุงุฐุฌ BERT ูGPT-2.                                    |      โ       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | ูุณุฎุฉ ูุชุนุฏุฏุฉ ุงููุบุงุช ูู BARTุ ููุฏุฑุจุฉ ูุณุจููุง ุนูู 50 ูุบุฉ.                                                                                                                                                     |      โ       |

ููุง ุชุฑู ูู ูุฐุง ุงูุฌุฏููุ ูุฅู ูุนุธู ููุงุฐุฌ ุงููุญูู ูุชูุฎูุต ุงููุตูุต (ููุนุธู ููุงู ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ) ุฃุญุงุฏูุฉ ุงููุบุฉ. ูุฐุง ุฑุงุฆุน ุฅุฐุง ูุงูุช ูููุชู ุจูุบุฉ "ุบููุฉ ุจุงูููุงุฑุฏ" ูุซู ุงูุฅูุฌููุฒูุฉ ุฃู ุงูุฃููุงููุฉุ ูููู ููุณ ุฌูุฏูุง ููุขูุงู ูู ุงููุบุงุช ุงูุฃุฎุฑู ุงููุณุชุฎุฏูุฉ ูู ุฌููุน ุฃูุญุงุก ุงูุนุงูู. ูุญุณู ุงูุญุธุ ููุงู ูุฆุฉ ูู ููุงุฐุฌ ุงููุญูู ูุชุนุฏุฏุฉ ุงููุบุงุชุ ูุซู mT5 ูmBARTุ ุงูุชู ุชุฃุชู ูููุณุงุนุฏุฉ. ูุฐู ุงูููุงุฐุฌ ููุฏุฑุจุฉ ูุณุจููุง ุจุงุณุชุฎุฏุงู ุงูููุฐุฌุฉ ุงููุบููุฉุ ูููู ูุน ุงุฎุชูุงู: ุจุฏูุงู ูู ุงูุชุฏุฑูุจ ุนูู ูุฌููุนุฉ ุจูุงูุงุช ุจูุบุฉ ูุงุญุฏุฉุ ูุชู ุชุฏุฑูุจูุง ุจุดูู ูุดุชุฑู ุนูู ูุตูุต ุจุฃูุซุฑ ูู 50 ูุบุฉ ูู ููุณ ุงูููุช!

ุณูุฑูุฒ ุนูู mT5ุ ุจููุฉ ูุซูุฑุฉ ููุงูุชูุงู ูุจููุฉ ุนูู T5 ุชู ุชุฏุฑูุจูุง ูุณุจููุง ูู ุฅุทุงุฑ ูุต-ุฅูู-ูุต. ูู T5ุ ูุชู ุตูุงุบุฉ ูู ูููุฉ ูู ููุงู ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ ูู ุญูุซ ุจุงุฏุฆุฉ ููุฌูุฉ ูุซู `summarize:` ุงูุชู ุชุฌุนู ุงููููุฐุฌ ูููููู ุงููุต ุงููููุฏ ููููุง ููููุฌู. ููุง ูู ููุถุญ ูู ุงูุดูู ุฃุฏูุงูุ ูุฐุง ูุฌุนู T5 ูุชุนุฏุฏ ุงูุงุณุชุฎุฏุงูุงุช ููุบุงูุฉุ ุญูุซ ููููู ุญู ุงูุนุฏูุฏ ูู ุงูููุงู ุจุงุณุชุฎุฏุงู ูููุฐุฌ ูุงุญุฏ!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="ููุงู ูุฎุชููุฉ ูุคุฏููุง ูููุฐุฌ T5."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="ููุงู ูุฎุชููุฉ ูุคุฏููุง ูููุฐุฌ T5."/>
</div>

ูุง ูุณุชุฎุฏู mT5 ุงูุจุงุฏุฆุงุชุ ููููู ูุดุงุฑู ุงููุซูุฑ ูู ุชุนุฏุฏ ุงูุงุณุชุฎุฏุงูุงุช ูุน T5 ููุชูุชุน ุจููุฒุฉ ูููู ูุชุนุฏุฏ ุงููุบุงุช. ุงูุขู ุจุนุฏ ุฃู ุงุฎุชุฑูุง ูููุฐุฌูุงุ ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุชุญุถูุฑ ุจูุงูุงุชูุง ููุชุฏุฑูุจ.


<Tip>

โ๏ธ **ุฌุฑุจู!** ุจุนุฏ ุงูุงูุชูุงุก ูู ูุฐุง ุงููุณูุ ุดุงูุฏ ููู ููุงุฑู mT5 ูุน mBART ูู ุฎูุงู ุถุจุท ุงูุฃุฎูุฑ ุจุงุณุชุฎุฏุงู ููุณ ุงูุชูููุงุช. ูููุญุตูู ุนูู ููุงุท ุฅุถุงููุฉุ ููููู ุฃูุถูุง ุชุฌุฑุจุฉ ุถุจุท T5 ุนูู ุงููุฑุงุฌุนุงุช ุงูุฅูุฌููุฒูุฉ ููุท. ูุธุฑูุง ูุฃู T5 ูุฏูู ุจุงุฏุฆุฉ ููุฌูุฉ ุฎุงุตุฉุ ุณุชุญุชุงุฌ ุฅูู ุฅุถุงูุฉ `summarize:` ุฅูู ุงูุฃูุซูุฉ ุงููุฏุฎูุฉ ูู ุฎุทูุงุช ูุง ูุจู ุงููุนุงูุฌุฉ ุฃุฏูุงู.

</Tip>

## ูุนุงูุฌุฉ ุงูุจูุงูุงุช ูุณุจููุง [[preprocessing-the-data]]

<Youtube id="1m7BerpSq8A"/>

ูููุชูุง ุงูุชุงููุฉ ูู ุชูุณูู ุงููุฑุงุฌุนุงุช ูุนูุงููููุง ุฅูู ุฑููุฒ. ููุง ูู ูุนุชุงุฏุ ูุจุฏุฃ ุจุชุญููู ุงูููุณู ุฅูู ุฑููุฒ ุงููุฑุชุจุท ุจููุทุฉ ุชูุชูุด ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง. ุณูุณุชุฎุฏู `mt5-small` ูููุทุฉ ุชูุชูุด ููุง ุญุชู ูุชููู ูู ุถุจุท ุงููููุฐุฌ ุงูุฏููู ูู ููุช ูุนููู:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

๐ก ูู ุงููุฑุงุญู ุงููุจูุฑุฉ ูู ูุดุงุฑูุน ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉุ ูู ุงูููุงุฑุณุงุช ุงูุฌูุฏุฉ ุชุฏุฑูุจ ูุฆุฉ ูู ุงูููุงุฐุฌ "ุงูุตุบูุฑุฉ" ุนูู ุนููุฉ ุตุบูุฑุฉ ูู ุงูุจูุงูุงุช. ูุฐุง ูุณูุญ ูู ุจุชุตุญูุญ ุงูุฃุฎุทุงุก ูุชูุฑุงุฑ ุงูุนูููุฉ ุจุดูู ุฃุณุฑุน ูุญู ุณูุฑ ุนูู ูู ุงูุจุฏุงูุฉ ุฅูู ุงูููุงูุฉ. ุจูุฌุฑุฏ ุฃู ุชููู ูุงุซููุง ูู ุงููุชุงุฆุฌุ ููููู ุฏุงุฆููุง ุฒูุงุฏุฉ ุญุฌู ุงููููุฐุฌ ุจุจุณุงุทุฉ ุนู ุทุฑูู ุชุบููุฑ ููุทุฉ ุชูุชูุด ุงููููุฐุฌ!

</Tip>

ุฏุนูุง ูุฌุฑุจ ููุณู ุงูุฑููุฒ mT5 ุนูู ูุซุงู ุตุบูุฑ:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ููุง ูููููุง ุฑุคูุฉ `input_ids` ู`attention_mask` ุงููุฃููููู ุงูุฐูู ุชุนุฑููุง ุนูููู ูู ุชุฌุงุฑุจูุง ุงูุฃููู ูุถุจุท ุงููููุฐุฌ ุงูุฏููู ูู [ุงููุตู 3](/course/chapter3). ุฏุนูุง ูููู ุจูู ุชุฑููุฒ ูุฐู ุงููุนุฑููุงุช ุงููุฏุฎูุฉ ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `convert_ids_to_tokens()` ูููุณู ุงูุฑููุฒ ููุนุฑูุฉ ููุน ููุณู ุงูุฑููุฒ ุงูุฐู ูุชุนุงูู ูุนู:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['โI', 'โ', 'loved', 'โreading', 'โthe', 'โHung', 'er', 'โGames', '</s>']
```

ุงูุฑูุฒ ุงูุฎุงุต Unicode `โ` ูุฑูุฒ ููุงูุฉ ุงูุณูุณูุฉ `</s>` ูุดูุฑุงู ุฅูู ุฃููุง ูุชุนุงูู ูุน ููุณู ุฑููุฒ SentencePieceุ ูุงูุฐู ูุนุชูุฏ ุนูู ุฎูุงุฑุฒููุฉ ุงูุชุฌุฒุฆุฉ Unigram ุงูุชู ูุงูุดูุงูุง ูู [ุงููุตู 6](/course/chapter6). Unigram ูููุฏ ุจุดูู ุฎุงุต ูููุฌููุนุงุช ูุชุนุฏุฏุฉ ุงููุบุงุช ูุฃูู ูุณูุญ ูู SentencePiece ุจุฃู ูููู ุบูุฑ ูุชุญูุฒ ููุชุดููู ูุงูุชูููุท ูุญูููุฉ ุฃู ุงูุนุฏูุฏ ูู ุงููุบุงุชุ ูุซู ุงููุงุจุงููุฉุ ูุง ุชุญุชูู ุนูู ุฃุญุฑู ุงููุณุงูุฉ.

ูุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ุฅูู ุฑููุฒุ ูุฌุจ ุฃู ูุชุนุงูู ูุน ุฏูุฉ ูุฑุชุจุทุฉ ุจุชูุฎูุต ุงููุตูุต: ูุฃู ุนูุงูุงุชูุง ุฃูุถูุง ูุตุ ููู ุงููููู ุฃู ุชุชุฌุงูุฒ ุญุฌู ุงูุณูุงู ุงูุฃูุตู ูููููุฐุฌ. ูุฐุง ูุนูู ุฃููุง ุจุญุงุฌุฉ ุฅูู ุชุทุจูู ุงูุชูุทูุน ุนูู ูู ูู ุงููุฑุงุฌุนุงุช ูุนูุงููููุง ูุถูุงู ุนุฏู ุชูุฑูุฑ ูุฏุฎูุงุช ุทูููุฉ ุจุดูู ููุฑุท ุฅูู ูููุฐุฌูุง. ุชููุฑ ููุณูุงุช ุงูุฑููุฒ ูู ๐ค Transformers ุญุฌุฉ `text_target` ูููุฏุฉ ุชุณูุญ ูู ุจุชุญููู ุงูุนูุงูุงุช ุฅูู ุฑููุฒ ุจุงูุชูุงุฒู ูุน ุงููุฏุฎูุงุช. ูููุง ููู ูุซุงู ุนูู ููููุฉ ูุนุงูุฌุฉ ุงููุฏุฎูุงุช ูุงูุนูุงูุงุช ูู mT5:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```
ุฏุนููุง ููุฑ ุนุจุฑ ูุฐุง ุงูููุฏ ูููู ูุง ูุญุฏุซ. ุฃูู ุดูุก ูููุง ุจู ูู ุชุญุฏูุฏ ููู ูู `max_input_length` ู `max_target_length`ุ ูุงูุชู ุชุญุฏุฏ ุงูุญุฏูุฏ ุงูุนููุง ูุทูู ุงููุฑุงุฌุน ูุงูุนูุงููู ุงูุชู ูููููุง ุงุณุชุฎุฏุงููุง. ุจูุง ุฃู ูุต ุงููุฑุงุฌุนุฉ ูููู ุนุงุฏุฉู ุฃูุจุฑ ุจูุซูุฑ ูู ุงูุนููุงูุ ููุฏ ูููุง ุจุถุจุท ูุฐู ุงูููู ููููุง ูุฐูู.

ูุน `preprocess_function()`ุ ูุตุจุญ ูู ุงูุณูู ุชููููุฒ (ุชูุณูู ุงููุต ุฅูู ูุญุฏุงุช ูุนุฌููุฉ) ุงูููุฑุจูุณ (ูุฌููุนุฉ ุงููุตูุต) ุจุงููุงูู ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `Dataset.map()` ุงููููุฏุฉ ุงูุชู ุงุณุชุฎุฏููุงูุง ุจุดูู ููุซู ุฎูุงู ูุฐู ุงูุฏูุฑุฉ:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

ุงูุขู ุจุนุฏ ุฃู ุชู ูุนุงูุฌุฉ ุงูููุฑุจูุณ ูุณุจููุงุ ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุจุนุถ ุงูููุงููุณ ุงููุณุชุฎุฏูุฉ ุนุงุฏุฉู ูู ุชูุฎูุต ุงููุตูุต. ููุง ุณูุฑูุ ูุง ููุฌุฏ ุญู ุณุญุฑู ุนูุฏูุง ูุชุนูู ุงูุฃูุฑ ุจููุงุณ ุฌูุฏุฉ ุงููุตูุต ุงููููุฏุฉ ุขูููุง.

<Tip>

๐ก ุฑุจูุง ูุงุญุธุช ุฃููุง ุงุณุชุฎุฏููุง `batched=True` ูู ุฏุงูุฉ `Dataset.map()` ุฃุนูุงู. ูุฐุง ูุดูุฑ ุงูุฃูุซูุฉ ูู ูุฌููุนุงุช ูู 1,000 (ุงููููุฉ ุงูุงูุชุฑุงุถูุฉ) ููุณูุญ ูู ุจุงุณุชุฎุฏุงู ูุฏุฑุงุช ุชุนุฏุฏ ุงูุฎููุท ูู ุงููุญููุงุช ุงูุณุฑูุนุฉ ๐ค. ุญูุซูุง ุฃูููุ ุญุงูู ุงุณุชุฎุฏุงู `batched=True` ููุงุณุชูุงุฏุฉ ุงููุตูู ูู ูุนุงูุฌุฉ ุงูุจูุงูุงุช ุงููุณุจูุฉ!

</Tip>


## ููุงููุณ ุชูุฎูุต ุงููุตูุต [[metrics-for-text-summarization]]

<Youtube id="TMshhnrEXlg"/>

ุจุงูููุงุฑูุฉ ูุน ูุนุธู ุงูููุงู ุงูุฃุฎุฑู ุงูุชู ูููุง ุจุชุบุทูุชูุง ูู ูุฐู ุงูุฏูุฑุฉุ ูุฅู ููุงุณ ุฃุฏุงุก ููุงู ุชูููุฏ ุงููุตูุต ูุซู ุงูุชูุฎูุต ุฃู ุงูุชุฑุฌูุฉ ููุณ ุจุงูุฃูุฑ ุงูุณูู. ุนูู ุณุจูู ุงููุซุงูุ ุจุงููุธุฑ ุฅูู ูุฑุงุฌุนุฉ ูุซู "ุฃุญุจุจุช ูุฑุงุกุฉ ุฃูุนุงุจ ุงูุฌูุน"ุ ููุงู ููุฎุตุงุช ุตุงูุญุฉ ูุชุนุฏุฏุฉุ ูุซู "ุฃุญุจุจุช ุฃูุนุงุจ ุงูุฌูุน" ุฃู "ุฃูุนุงุจ ุงูุฌูุน ูุฑุงุกุฉ ุฑุงุฆุนุฉ". ูู ุงููุงุถุญ ุฃู ุชุทุจูู ููุน ูู ุงููุทุงุจูุฉ ุงูุฏูููุฉ ุจูู ุงูููุฎุต ุงููููุฏ ูุงูููุตู ููุณ ุญูุงู ุฌูุฏูุง - ุญุชู ุงูุจุดุฑ ุณูุญุตููู ุนูู ูุชุงุฆุฌ ุณูุฆุฉ ููููุง ููุซู ูุฐุง ุงููููุงุณุ ูุฃู ููู ููุง ุฃุณููุจู ุงูุฎุงุต ูู ุงููุชุงุจุฉ.

ุฃุญุฏ ุงูููุงููุณ ุงูุฃูุซุฑ ุงุณุชุฎุฏุงููุง ูู ุชูุฎูุต ุงููุตูุต ูู [ุฏุฑุฌุฉ ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (ุงุฎุชุตุงุฑูุง ูู Recall-Oriented Understudy for Gisting Evaluation). ุงูููุฑุฉ ุงูุฃุณุงุณูุฉ ูุฑุงุก ูุฐุง ุงููููุงุณ ูู ููุงุฑูุฉ ููุฎุต ูููุฏ ุจูุฌููุนุฉ ูู ุงูููุฎุตุงุช ุงููุฑุฌุนูุฉ ุงูุชู ุนุงุฏุฉ ูุง ูููู ุจูุง ุงูุจุดุฑ. ูุฌุนู ูุฐุง ุฃูุซุฑ ุฏูุฉุ ูููุชุฑุถ ุฃููุง ูุฑูุฏ ููุงุฑูุฉ ุงูููุฎุตูู ุงูุชุงูููู:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

ุฅุญุฏู ุทุฑู ุงูููุงุฑูุฉ ุจููููุง ูููู ุฃู ุชููู ุจุนุฏุฏ ุงููููุงุช ุงููุชุฏุงุฎูุฉุ ูุงูุชู ูู ูุฐู ุงูุญุงูุฉ ุณุชููู 6. ูููู ูุฐู ุงูุทุฑููุฉ ุจุฏุงุฆูุฉ ุจุนุถ ุงูุดูุกุ ูุฐูู ุจุฏูุงู ูู ุฐููุ ุชุนุชูุฏ ROUGE ุนูู ุญุณุงุจ ุฏุฑุฌุงุช _ุงูุฏูุฉ_ ู_ุงูุงุณุชุฏุนุงุก_ ููุชุฏุงุฎู.

<Tip>

๐ ูุง ุชููู ุฅุฐุง ูุงูุช ูุฐู ูู ุงููุฑุฉ ุงูุฃููู ุงูุชู ุชุณูุน ูููุง ุนู ุงูุฏูุฉ ูุงูุงุณุชุฏุนุงุก - ุณููุฑ ุนุจุฑ ุจุนุถ ุงูุฃูุซูุฉ ุงูุตุฑูุญุฉ ูุนูุง ูุชูุถูุญ ุงูุฃูุฑ. ูุฐู ุงูููุงููุณ ุนุงุฏุฉ ูุง ูุชู ููุงุฌูุชูุง ูู ููุงู ุงูุชุตูููุ ูุฐูู ุฅุฐุง ููุช ุชุฑุบุจ ูู ููู ููููุฉ ุชุญุฏูุฏ ุงูุฏูุฉ ูุงูุงุณุชุฏุนุงุก ูู ูุฐุง ุงูุณูุงูุ ูุฅููุง ููุตู ุจุงูุงุทูุงุน ุนูู [ุงูุฏูุงุฆู](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) ูู `scikit-learn`.

</Tip>

ุจุงููุณุจุฉ ูู ROUGEุ ูููุณ ุงูุงุณุชุฏุนุงุก ููุฏุงุฑ ุงูููุฎุต ุงููุฑุฌุนู ุงูุฐู ุชู ุงูุชูุงุทู ุจูุงุณุทุฉ ุงูููุฎุต ุงููููุฏ. ุฅุฐุง ููุง ููุงุฑู ููุท ุงููููุงุชุ ูููู ุญุณุงุจ ุงูุงุณุชุฏุนุงุก ููููุง ูููุนุงุฏูุฉ ุงูุชุงููุฉ:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

ุจุงููุณุจุฉ ููุซุงููุง ุงูุจุณูุท ุฃุนูุงูุ ุชุนุทู ูุฐู ุงููุนุงุฏูุฉ ุงุณุชุฏุนุงุกู ูุซุงูููุง 6/6 = 1ุ ุฃู ุฃู ุฌููุน ุงููููุงุช ูู ุงูููุฎุต ุงููุฑุฌุนู ุชู ุฅูุชุงุฌูุง ุจูุงุณุทุฉ ุงููููุฐุฌ. ูุฏ ูุจุฏู ูุฐุง ุฑุงุฆุนูุงุ ูููู ุชุฎูู ุฅุฐุง ูุงู ููุฎุตูุง ุงููููุฏ ูู "I really really loved reading the Hunger Games all night". ุณูููู ููุฐุง ุฃูุถูุง ุงุณุชุฏุนุงุกู ูุซุงูููุงุ ููููู ููุฎุต ุฃุณูุฃ ูุฃู ุงููุต ูุทูุจ. ููุชุนุงูู ูุน ูุฐู ุงูุณููุงุฑูููุงุชุ ูุญุณุจ ุฃูุถูุง ุงูุฏูุฉุ ูุงูุชู ูู ุณูุงู ROUGE ุชููุณ ูุฏู ููุงุกูุฉ ุงูููุฎุต ุงููููุฏ:

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

ุชุทุจูู ูุฐุง ุนูู ููุฎุตูุง ุงููุทูุจ ูุนุทู ุฏูุฉ 6/10 = 0.6ุ ูุงูุชู ุชุนุชุจุฑ ุฃุณูุฃ ุจูุซูุฑ ูู ุงูุฏูุฉ 6/7 = 0.86 ุงูุชู ุญุตููุง ุนูููุง ูู ููุฎุตูุง ุงูุฃูุตุฑ. ูู ุงูููุงุฑุณุฉ ุงูุนูููุฉุ ูุชู ุญุณุงุจ ูู ูู ุงูุฏูุฉ ูุงูุงุณุชุฏุนุงุก ุนุงุฏุฉูุ ุซู ูุชู ุงูุฅุจูุงุบ ุนู F1-score (ุงููุชูุณุท ุงูุชูุงููู ููุฏูุฉ ูุงูุงุณุชุฏุนุงุก). ูููููุง ุงูููุงู ุจุฐูู ุจุณูููุฉ ูู ๐ค Datasets ุนู ุทุฑูู ุชุซุจูุช ุญุฒูุฉ `rouge_score` ุฃููุงู:

```py
!pip install rouge_score
```

ุซู ุชุญููู ูููุงุณ ROUGE ููุง ููู:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

ุจุนุฏ ุฐููุ ูููููุง ุงุณุชุฎุฏุงู ุฏุงูุฉ `rouge_score.compute()` ูุญุณุงุจ ุฌููุน ุงูููุงููุณ ูุฑุฉ ูุงุญุฏุฉ:

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

ูุง ูู ูู ูู ูุจูุฑ ูู ุงููุนูููุงุช ูู ูุฐุง ุงููุฎุฑุฌ - ูุงุฐุง ูุนูู ูู ูุฐุงุ ุฃููุงูุ ๐ค Datasets ูุญุณุจ ูุชุฑุงุช ุงูุซูุฉ ููุฏูุฉ ูุงูุงุณุชุฏุนุงุก ูF1-scoreุ ูุฐู ูู ุณูุงุช `low` ู`mid` ู`high` ุงูุชู ููููู ุฑุคูุชูุง ููุง. ุนูุงูุฉ ุนูู ุฐููุ ูุญุณุจ ๐ค Datasets ูุฌููุนุฉ ูุชููุนุฉ ูู ุฏุฑุฌุงุช ROUGE ูุงูุชู ุชุณุชูุฏ ุฅูู ุฃููุงุน ูุฎุชููุฉ ูู ุฏูุฉ ุงููุต ุนูุฏ ููุงุฑูุฉ ุงูููุฎุตุงุช ุงููููุฏุฉ ูุงููุฑุฌุนูุฉ. ุชูุณุชุฎุฏู ูุณุฎุฉ `rouge1` ููุชุฏุงุฎู ุจูู ุงููููุงุช ุงููููุฑุฏุฉ - ูุฐู ุทุฑููุฉ ูุนูุฏุฉ ููููู ุฅููุง ุชุฏุงุฎู ุงููููุงุช ููู ุจุงูุถุจุท ุงููููุงุณ ุงูุฐู ูุงูุดูุงู ุฃุนูุงู. ููุชุญูู ูู ุฐููุ ุฏุนูุง ูุณุชุฎุฑุฌ ูููุฉ `mid` ูู ุฏุฑุฌุงุชูุง:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

ุฑุงุฆุนุ ุฃุฑูุงู ุงูุฏูุฉ ูุงูุงุณุชุฏุนุงุก ุชุชุทุงุจู! ูุงูุขู ูุงุฐุง ุนู ุฏุฑุฌุงุช ROUGE ุงูุฃุฎุฑูุ ุชููุณ `rouge2` ุงูุชุฏุงุฎู ุจูู ุงููููุงุช ุงููุฒุฏูุฌุฉ (ููุฑ ูู ุชุฏุงุฎู ุฃุฒูุงุฌ ุงููููุงุช)ุ ุจูููุง `rougeL` ู`rougeLsum` ุชููุณ ุฃุทูู ุชุณูุณูุงุช ูุชุทุงุจูุฉ ูู ุงููููุงุช ุนู ุทุฑูู ุงูุจุญุซ ุนู ุฃุทูู ุงูุณูุงุณู ุงููุดุชุฑูุฉ ูู ุงูููุฎุตุงุช ุงููููุฏุฉ ูุงููุฑุฌุนูุฉ. ูุดูุฑ "sum" ูู `rougeLsum` ุฅูู ุญูููุฉ ุฃู ูุฐุง ุงููููุงุณ ูุชู ุญุณุงุจู ุนูู ููุฎุต ูุงููุ ุจูููุง ูุชู ุญุณุงุจ `rougeL` ููุชูุณุท ุนูู ุงูุฌูู ุงููุฑุฏูุฉ.

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ูู ุจุฅูุดุงุก ูุซุงูู ุงูุฎุงุต ูููุฎุต ูููุฏ ูููุฎุต ูุฑุฌุนู ูุงูุธุฑ ูุง ุฅุฐุง ูุงูุช ุฏุฑุฌุงุช ROUGE ุชุชูู ูุน ุงูุญุณุงุจ ุงููุฏูู ุจูุงุกู ุนูู ุงููุนุงุฏูุงุช ููุฏูุฉ ูุงูุงุณุชุฏุนุงุก. ููุญุตูู ุนูู ููุงุท ุฅุถุงููุฉุ ูู ุจุชูุณูู ุงููุต ุฅูู ูููุงุช ูุฒุฏูุฌุฉ ููุงุฑู ุงูุฏูุฉ ูุงูุงุณุชุฏุนุงุก ููููุงุณ `rouge2`.

</Tip>

ุณูุณุชุฎุฏู ูุฐู ุงูุฏุฑุฌุงุช ROUGE ูุชุชุจุน ุฃุฏุงุก ูููุฐุฌูุงุ ูููู ูุจู ุงูููุงู ุจุฐูู ุฏุนูุง ููุนู ุดูุฆูุง ูุฌุจ ุนูู ูู ููุงุฑุณ ุฌูุฏ ููู NLP ูุนูู: ุฅูุดุงุก ุฎุท ุฃุณุงุณ ููู ูุจุณูุท!

### ุฅูุดุงุก ุฎุท ุฃุณุงุณ ููู [[creating-a-strong-baseline]]

ุฎุท ุงูุฃุณุงุณ ุงูุดุงุฆุน ูุชูุฎูุต ุงููุตูุต ูู ุจุจุณุงุทุฉ ุฃุฎุฐ ุงูุฌูู ุงูุซูุงุซ ุงูุฃููู ูู ููุงูุ ูุงูุฐู ููุทูู ุนููู ุบุงูุจูุง _lead-3_ baseline. ูููููุง ุงุณุชุฎุฏุงู ุงูููุงุท ุงููุงููุฉ ูุชุชุจุน ุญุฏูุฏ ุงูุฌููุ ูููู ูุฐุง ุณููุดู ูู ุงูุงุฎุชุตุงุฑุงุช ูุซู "U.S." ุฃู "U.N." - ูุฐูู ุจุฏูุงู ูู ุฐููุ ุณูุณุชุฎุฏู ููุชุจุฉ `nltk`ุ ูุงูุชู ุชุชุถูู ุฎูุงุฑุฒููุฉ ุฃูุถู ููุชุนุงูู ูุน ูุฐู ุงูุญุงูุงุช. ููููู ุชุซุจูุช ุงูุญุฒูุฉ ุจุงุณุชุฎุฏุงู `pip` ููุง ููู:

```python
!pip install nltk
```

ุซู ุชูุฒูู ููุงุนุฏ ุงูุชุฑููู:

```python
import nltk

nltk.download("punkt")
```
ุซู ูู ุจุชูุฒูู ููุงุนุฏ ุงูุชุฑููู:

```python
import nltk

nltk.download("punkt")
```

ุจุนุฏ ุฐููุ ูุณุชูุฑุฏ ูุญุฏุฏ ุงูุฌูู ูู `nltk` ูููุดุฆ ุฏุงูุฉ ุจุณูุทุฉ ูุงุณุชุฎุฑุงุฌ ุงูุฌูู ุงูุซูุงุซ ุงูุฃููู ูู ุงููุฑุงุฌุนุฉ. ุงูุงุชูุงููุฉ ูู ุชูุฎูุต ุงููุต ูู ูุตู ูู ููุฎุต ุจุณุทุฑ ุฌุฏูุฏุ ูุฐุง ุฏุนูุง ูููู ุจุฐูู ุฃูุถูุง ููุฎุชุจุฑูุง ุนูู ูุซุงู ุชุฏุฑูุจู:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

ูุจุฏู ุฃู ูุฐุง ูุนููุ ูุฐุง ุฏุนูุง ุงูุขู ูููุฐ ุฏุงูุฉ ุชุณุชุฎุฑุฌ ูุฐู "ุงูููุฎุตุงุช" ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุชุญุณุจ ุฏุฑุฌุงุช ROUGE ููุฎุท ุงูุฃุณุงุณู:

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

ูููููุง ุจุนุฏ ุฐูู ุงุณุชุฎุฏุงู ูุฐู ุงูุฏุงูุฉ ูุญุณุงุจ ุฏุฑุฌุงุช ROUGE ุนูู ูุฌููุนุฉ ุงูุชุญูู ูุฌุนููุง ุฃูุซุฑ ูุถูุญูุง ุจุงุณุชุฎุฏุงู Pandas:

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

ูููููุง ุฃู ูุฑู ุฃู ุฏุฑุฌุฉ `rouge2` ุฃูู ุจูุซูุฑ ูู ุงูุจููุฉุ ููุฐุง ูุนูุณ ุนูู ุงูุฃุฑุฌุญ ุญูููุฉ ุฃู ุนูุงููู ุงููุฑุงุฌุนุงุช ุชููู ุนุงุฏุฉ ููุฌุฒุฉุ ูุฐุง ูุฅู ุฎุท ุงูุฃุณุงุณ ุงููููู ูู ุซูุงุซ ุฌูู ูููู ุทูููุงู ุฌุฏูุง. ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญ ูุฏููุง ุฎุท ุฃุณุงุณ ุฌูุฏ ููุนูู ูููุ ุฏุนูุง ููุฌู ุงูุชุจุงููุง ูุญู ุงูุถุจุท ุงูุฏููู ูู mT5!

{#if fw === 'pt'}

## ุงูุถุจุท ุงูุฏููู ูู mT5 ุจุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `Trainer`[[fine-tuning-mt5-with-the-trainer-api]]

ุงูุถุจุท ุงูุฏููู ููููุฐุฌ ููุชูุฎูุต ูุดุจู ุฅูู ุญุฏ ูุจูุฑ ุงูููุงู ุงูุฃุฎุฑู ุงูุชู ูููุง ุจุชุบุทูุชูุง ูู ูุฐุง ุงููุตู. ุฃูู ุดูุก ูุญุชุงุฌ ุฅูู ุงูููุงู ุจู ูู ุชุญููู ุงููููุฐุฌ ุงููุณุจู ุงูุชุฏุฑูุจ ูู ููุทุฉ ุงูุชุญูู `mt5-small`. ุจูุง ุฃู ุงูุชูุฎูุต ูู ูููุฉ ุชุณูุณู ุฅูู ุชุณูุณูุ ูููููุง ุชุญููู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ูุฆุฉ `AutoModelForSeq2SeqLM`ุ ูุงูุชู ุณุชููู ุชููุงุฆููุง ุจุชูุฒูู ุงูุฃูุฒุงู ูุชุฎุฒูููุง ูุคูุชูุง:

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ุงูุถุจุท ุงูุฏููู ูู mT5 ุจุงุณุชุฎุฏุงู Keras[[fine-tuning-mt5-with-keras]]

ุงูุถุจุท ุงูุฏููู ููููุฐุฌ ููุชูุฎูุต ูุดุจู ุฅูู ุญุฏ ูุจูุฑ ุงูููุงู ุงูุฃุฎุฑู ุงูุชู ูููุง ุจุชุบุทูุชูุง ูู ูุฐุง ุงููุตู. ุฃูู ุดูุก ูุญุชุงุฌ ุฅูู ุงูููุงู ุจู ูู ุชุญููู ุงููููุฐุฌ ุงููุณุจู ุงูุชุฏุฑูุจ ูู ููุทุฉ ุงูุชุญูู `mt5-small`. ุจูุง ุฃู ุงูุชูุฎูุต ูู ูููุฉ ุชุณูุณู ุฅูู ุชุณูุณูุ ูููููุง ุชุญููู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ูุฆุฉ `TFAutoModelForSeq2SeqLM`ุ ูุงูุชู ุณุชููู ุชููุงุฆููุง ุจุชูุฒูู ุงูุฃูุฒุงู ูุชุฎุฒูููุง ูุคูุชูุง:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

๐ก ุฅุฐุง ููุช ุชุชุณุงุกู ุนู ุณุจุจ ุนุฏู ุธููุฑ ุฃู ุชุญุฐูุฑุงุช ุญูู ุงูุถุจุท ุงูุฏููู ูููููุฐุฌ ุนูู ูููุฉ ุฃุณูู ุงูููุฑุ ูุฐูู ูุฃููุง ุจุงููุณุจุฉ ูููุงู ุงูุชุณูุณู ุฅูู ุงูุชุณูุณู ูุญุชูุธ ุจุฌููุน ุฃูุฒุงู ุงูุดุจูุฉ. ูุงุฑู ูุฐุง ุจุงููููุฐุฌ ุงูุฎุงุต ุจูุง ูู ุงูุชุตููู ุงููุตู ูู [ุงููุตู 3](/course/chapter3)ุ ุญูุซ ุชู ุงุณุชุจุฏุงู ุฑุฃุณ ุงููููุฐุฌ ุงููุณุจู ุงูุชุฏุฑูุจ ุจุดุจูุฉ ุชู ุชููุฆุชูุง ุนุดูุงุฆููุง.

</Tip>

ุงูุดูุก ุงูุชุงูู ุงูุฐู ูุญุชุงุฌ ุฅูู ุงูููุงู ุจู ูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face Hub. ุฅุฐุง ููุช ุชููู ุจุชุดุบูู ูุฐุง ุงูุฑูุฒ ูู ุฏูุชุฑ ููุงุญุธุงุชุ ูููููู ุงูููุงู ุจุฐูู ุจุงุณุชุฎุฏุงู ุฏุงูุฉ ุงููุณุงุนุฏุฉ ุงูุชุงููุฉ:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ูุงูุชู ุณุชุนุฑุถ ุฃุฏุงุฉ ููููู ูู ุฎูุงููุง ุฅุฏุฎุงู ุจูุงูุงุช ุงุนุชูุงุฏู. ุฃู ููููู ุชุดุบูู ูุฐุง ุงูุฃูุฑ ูู ุทุฑููุฉ ุงููุธุงู ุงูุฎุงุต ุจู ูุชุณุฌูู ุงูุฏุฎูู ููุงู:

```
huggingface-cli login
```

{#if fw === 'pt'}

ุณูุญุชุงุฌ ุฅูู ุชูููุฏ ููุฎุตุงุช ูุญุณุงุจ ุฏุฑุฌุงุช ROUGE ุฃุซูุงุก ุงูุชุฏุฑูุจ. ูุญุณู ุงูุญุธุ ูููุฑ ๐ค Transformers ูุฆุงุช ูุฎุตุตุฉ `Seq2SeqTrainingArguments` ู`Seq2SeqTrainer` ุงูุชู ูููููุง ุงูููุงู ุจุฐูู ุชููุงุฆููุง! ููุนุฑูุฉ ููููุฉ ุนูู ุฐููุ ุฏุนูุง ูุญุฏุฏ ุฃููุงู ูุฑุท ุงููุนููุงุช ูุงูุญุฌุฌ ุงูุฃุฎุฑู ูุชุฌุงุฑุจูุง:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# ุนุฑุถ ุฎุณุงุฑุฉ ุงูุชุฏุฑูุจ ูุน ูู ุญูุจุฉ
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

ููุงุ ุชู ุถุจุท ุญุฌุฉ `predict_with_generate` ููุฅุดุงุฑุฉ ุฅูู ุฃูู ูุฌุจ ุนูููุง ุชูููุฏ ููุฎุตุงุช ุฃุซูุงุก ุงูุชูููู ุจุญูุซ ูููููุง ุญุณุงุจ ุฏุฑุฌุงุช ROUGE ููู ุญูุจุฉ. ููุง ุชูุช ููุงูุดุชู ูู [ุงููุตู 1](/course/chapter1)ุ ูููู ูู ุงูุชุดููุฑ ุจุงูุงุณุชุฏูุงู ุนู ุทุฑูู ุงูุชูุจุค ุจุงูุฑููุฒ ูุงุญุฏูุง ุชูู ุงูุขุฎุฑุ ููุฐุง ูุชู ุชูููุฐู ุจูุงุณุทุฉ ุทุฑููุฉ `generate()` ูููููุฐุฌ. ูุฎุจุฑ ุชุนููู `predict_with_generate=True` ุงููุฏุฑุจ `Seq2SeqTrainer` ุจุงุณุชุฎุฏุงู ุชูู ุงูุทุฑููุฉ ููุชูููู. ููุฏ ูููุง ุฃูุถูุง ุจุชุนุฏูู ุจุนุถ ูุฑุท ุงููุนููุงุช ุงูุงูุชุฑุงุถูุฉุ ูุซู ูุนุฏู ุงูุชุนููุ ูุนุฏุฏ ุงูุญูุจุงุชุ ููุฒู ุงูุชูุงุดูุ ููููุง ุจุถุจุท ุฎูุงุฑ `save_total_limit` ูุญูุธ ูุง ูุตู ุฅูู 3 ููุงุท ุชุญูู ููุท ุฃุซูุงุก ุงูุชุฏุฑูุจ - ูุฐูู ูุฃู ุญุชู ุงูุฅุตุฏุงุฑ "ุงูุตุบูุฑ" ูู mT5 ูุณุชุฎุฏู ุญูุงูู ุฌูุฌุงุจุงูุช ูู ูุณุงุญุฉ ุงููุฑุต ุงูุตูุจุ ููููููุง ุชูููุฑ ุจุนุถ ุงููุณุงุญุฉ ุนู ุทุฑูู ุงูุญุฏ ูู ุนุฏุฏ ุงููุณุฎ ุงูุชู ูุญูุธูุง.

ุณุชุณูุญ ููุง ุญุฌุฉ `push_to_hub=True` ุจุฏูุน ุงููููุฐุฌ ุฅูู Hub ุจุนุฏ ุงูุชุฏุฑูุจุ ุณุชุฌุฏ ุงููุณุชูุฏุน ุชุญุช ูููู ุงูุดุฎุตู ูู ุงููููุน ุงููุญุฏุฏ ุจูุงุณุทุฉ `output_dir`. ูุงุญุธ ุฃูู ููููู ุชุญุฏูุฏ ุงุณู ุงููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุฏูุนู ุจุงุณุชุฎุฏุงู ุญุฌุฉ `hub_model_id` (ุนูู ูุฌู ุงูุฎุตูุตุ ุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ูุฐู ุงูุญุฌุฉ ููุฏูุน ุฅูู ููุธูุฉ). ุนูู ุณุจูู ุงููุซุงูุ ุนูุฏูุง ูููุง ุจุฏูุน ุงููููุฐุฌ ุฅูู ููุธูุฉ [`huggingface-course`](https://huggingface.co/huggingface-course)ุ ุฃุถููุง `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` ุฅูู `Seq2SeqTrainingArguments`.

ุงูุดูุก ุงูุชุงูู ุงูุฐู ูุญุชุงุฌ ุฅูู ุงูููุงู ุจู ูู ุชุฒููุฏ ุงููุฏุฑุจ ุจุฏุงูุฉ `compute_metrics()` ุจุญูุซ ูููููุง ุชูููู ูููุฐุฌูุง ุฃุซูุงุก ุงูุชุฏุฑูุจ. ุจุงููุณุจุฉ ููุชูุฎูุตุ ูุฐุง ุฃูุซุฑ ุชุนููุฏูุง ูู ูุฌุฑุฏ ุงุณุชุฏุนุงุก `rouge_score.compute()` ุนูู ุชูุจุคุงุช ุงููููุฐุฌุ ุญูุซ ูุญุชุงุฌ ุฅูู _ูู ุชุดููุฑ_ ุงููุฎุฑุฌุงุช ูุงูุนูุงูุงุช ุฅูู ูุต ูุจู ุฃู ูุชููู ูู ุญุณุงุจ ุฏุฑุฌุงุช ROUGE. ุชููู ุงูุฏุงูุฉ ุงูุชุงููุฉ ุจุฐูู ุจุงูุถุจุทุ ูุชุณุชุฎุฏู ุฃูุถูุง ุฏุงูุฉ `sent_tokenize()` ูู `nltk` ููุตู ุฌูู ุงูููุฎุต ุจุณุทูุฑ ุฌุฏูุฏุฉ:

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # ูู ุชุดููุฑ ุงูููุฎุตุงุช ุงููููุฏุฉ ุฅูู ูุต
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # ุงุณุชุจุฏู -100 ูู ุงูุนูุงูุงุช ุญูุซ ูุง ูููููุง ูู ุชุดููุฑูุง
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # ูู ุชุดููุฑ ููุฎุตุงุช ุงููุฑุฌุน ุฅูู ูุต
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ูุชููุน ROUGE ูุฌูุฏ ุณุทุฑ ุฌุฏูุฏ ุจุนุฏ ูู ุฌููุฉ
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # ุญุณุงุจ ุฏุฑุฌุงุช ROUGE
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # ุงุณุชุฎุฑุงุฌ ุงูุฏุฑุฌุงุช ุงููุชูุณุทุฉ
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

ุงูุขูุ ูุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ุฌุงูุน ุจูุงูุงุช ูููุงููุง ูู ุชุณูุณู ุฅูู ุชุณูุณู. ุจูุง ุฃู mT5 ูู ูููุฐุฌ ูุญูู ูู ูุดูุฑ-ูุงูุ ูุฅู ุฅุญุฏู ุงูุฏูุงุฆู ูู ุฅุนุฏุงุฏ ุฏูุนุงุชูุง ูู ุฃูู ุฃุซูุงุก ูู ุงูุชุดููุฑุ ูุญุชุงุฌ ุฅูู ุชุญููู ุงูููุตูุงุช ุฅูู ุงููููู ุจูุงุญุฏ. ูุฐุง ูุทููุจ ูุถูุงู ุฃู ุงููุงู ูุง ูุฑู ุณูู ุงูููุตูุงุช ุงูุญููููุฉ ุงูุณุงุจูุฉ ูููุณ ุงูุญุงููุฉ ุฃู ุงููุณุชูุจููุฉุ ูุงูุชู ุณูููู ูู ุงูุณูู ุนูู ุงููููุฐุฌ ุชุฐูุฑูุง. ูุฐุง ูุดุงุจู ูููููุฉ ุชุทุจูู ุงูุงูุชูุงู ุงูุฐุงุชู ุงููููุน ุนูู ุงููุฏุฎูุงุช ูู ูููุฉ ูุซู [ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](/course/chapter7/6).

ูุญุณู ุงูุญุธุ ูููุฑ ๐ค Transformers ุฌุงูุน ุจูุงูุงุช `DataCollatorForSeq2Seq` ุงูุฐู ุณูููู ุชููุงุฆููุง ุจููุก ุงููุฏุฎูุงุช ูุงูููุตูุงุช. ูุฅูุดุงุก ูุฐุง ุงูุฌุงูุนุ ูุญุชุงุฌ ุจุจุณุงุทุฉ ุฅูู ุชูููุฑ `tokenizer` ู`model`:

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

ุฏุนูุง ูุฑู ูุง ููุชุฌู ูุฐุง ุงูุฌุงูุน ุนูุฏ ุฅุทุนุงูู ุฏูุนุฉ ุตุบูุฑุฉ ูู ุงูุฃูุซูุฉ. ุฃููุงูุ ูุญุชุงุฌ ุฅูู ุฅุฒุงูุฉ ุงูุฃุนูุฏุฉ ุฐุงุช ุงูุณูุงุณู ุงููุตูุฉ ูุฃู ุฌุงูุน ุงูุจูุงูุงุช ูู ูุนุฑู ููููุฉ ููุก ูุฐู ุงูุนูุงุตุฑ:

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

ุจูุง ุฃู ุฌุงูุน ุงูุจูุงูุงุช ูุชููุน ูุงุฆูุฉ ูู `dict`sุ ุญูุซ ููุซู ูู `dict` ูุซุงููุง ูุงุญุฏูุง ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุญุชุงุฌ ุฃูุถูุง ุฅูู ุชุฑุชูุจ ุงูุจูุงูุงุช ูู ุงูุชูุณูู ุงููุชููุน ูุจู ุชูุฑูุฑูุง ุฅูู ุฌุงูุน ุงูุจูุงูุงุช:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

ุงูุดูุก ุงูุฑุฆูุณู ุงูุฐู ูุฌุจ ููุงุญุธุชู ููุง ูู ุฃู ุงููุซุงู ุงูุฃูู ุฃุทูู ูู ุงูุซุงููุ ูุฐุง ุชู ููุก `input_ids` ู`attention_mask` ูููุซุงู ุงูุซุงูู ุนูู ุงููููู ุจุฑูุฒ `[PAD]` (ุงูุฐู ูููู ูุนุฑูู `0`). ูุจุงููุซูุ ูููููุง ุฃู ูุฑู ุฃู `labels` ุชู ููุคูุง ุจุฑููุฒ `-100`ุ ููุชุฃูุฏ ูู ุชุฌุงูู ุฑููุฒ ุงูููุก ุจูุงุณุทุฉ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ. ูุฃุฎูุฑูุงุ ูููููุง ุฃู ูุฑู `decoder_input_ids` ุฌุฏูุฏ ุงูุฐู ูุงู ุจุชุญููู ุงูููุตูุงุช ุฅูู ุงููููู ุจุฅุฏุฑุงุฌ ุฑูุฒ `[PAD]` ูู ุงูุฅุฏุฎุงู ุงูุฃูู.

{#if fw === 'pt'}

ุฃุฎูุฑูุงุ ูุฏููุง ุฌููุน ุงูููููุงุช ุงูุชู ูุญุชุงุฌูุง ููุชุฏุฑูุจ! ูุญุชุงุฌ ุงูุขู ุจุจุณุงุทุฉ ุฅูู ุฅูุดุงุก ุงููุฏุฑุจ ุจุงูุญุฌุฌ ุงูููุงุณูุฉ:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

ุซู ุฅุทูุงู ุนูููุฉ ุงูุชุฏุฑูุจ:

```python
trainer.train()
```

ุฃุซูุงุก ุงูุชุฏุฑูุจุ ูุฌุจ ุฃู ุชุฑู ุงูุฎูุงุถ ุฎุณุงุฑุฉ ุงูุชุฏุฑูุจ ูุฒูุงุฏุฉ ุฏุฑุฌุงุช ROUGE ูุน ูู ุญูุจุฉ. ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ููููู ุฑุคูุฉ ุฏุฑุฌุงุช ROUGE ุงูููุงุฆูุฉ ุจุชุดุบูู `Trainer.evaluate()`:

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

ูู ุงูุฏุฑุฌุงุช ูููููุง ุฃู ูุฑู ุฃู ูููุฐุฌูุง ุชููู ุจุณูููุฉ ุนูู ุฎุท ุงูุฃุณุงุณ ูุฏููุง - ุฑุงุฆุน! ุงูุดูุก ุงูุฃุฎูุฑ ุงูุฐู ูุฌุจ ูุนูู ูู ุฏูุน ุฃูุฒุงู ุงููููุฐุฌ ุฅูู ุงููุฑูุฒุ ููุง ููู:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

ุณูููู ูุฐุง ุจุญูุธ ูููุงุช ููุทุฉ ุงูุชูุชูุด ูุงูุชูููู ุฅูู `output_dir`ุ ูุจู ุชุญููู ุฌููุน ุงููููุงุช ุฅูู ุงููุฑูุฒ. ูู ุฎูุงู ุชุญุฏูุฏ ุญุฌุฉ `tags`ุ ูุถูู ุฃูุถูุง ุฃู ุงูุฃุฏุงุฉ ูู ุงููุฑูุฒ ุณุชููู ูุงุญุฏุฉ ูุฎุท ุฃูุงุจูุจ ุงูููุฎุต ุจุฏูุงู ูู ุฃุฏุงุฉ ุฅูุดุงุก ุงููุต ุงูุงูุชุฑุงุถูุฉ ุงููุฑุชุจุทุฉ ุจููุฏุณุฉ mT5 (ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุนูุงูุงุช ุงููููุฐุฌุ ุฑุงุฌุน [ูุซุงุฆู ๐ค Hub](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). ุงููุฎุฑุฌุงุช ูู `trainer.push_to_hub()` ูู ุนููุงู URL ูุฑูุฒ Git commitุ ูุฐุง ููููู ุจุณูููุฉ ุฑุคูุฉ ุงูุชุบููุฑุงุช ุงูุชู ุชู ุฅุฌุฑุงุคูุง ุนูู ูุณุชูุฏุน ุงููููุฐุฌ!

ูุฅููุงุก ูุฐุง ุงููุณูุ ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ููููุฉ ุถุจุท ูููุฐุฌ mT5 ุฃูุถูุง ุจุงุณุชุฎุฏุงู ุงูููุฒุงุช ููุฎูุถุฉ ุงููุณุชูู ุงูุชู ูููุฑูุง ๐ค Accelerate.

{:else}

ูุญู ุนูู ุงุณุชุนุฏุงุฏ ููุชุฏุฑูุจ ุชูุฑูุจูุง! ูุญุชุงุฌ ููุท ุฅูู ุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ุฅูู `tf.data.Dataset`s ุจุงุณุชุฎุฏุงู ุฌุงูุน ุงูุจูุงูุงุช ุงูุฐู ุญุฏุฏูุงู ุฃุนูุงูุ ุซู `compile()` ู`fit()` ุงููููุฐุฌ. ุฃููุงูุ ูุฌููุนุงุช ุงูุจูุงูุงุช:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

ุงูุขูุ ูุญุฏุฏ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ููููู ุจุงูุชุฑุฌูุฉ:

```python
from transformers import create_optimizer
import tensorflow as tf

# ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ ูู ุนุฏุฏ ุงูุนููุงุช ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ููุณูููุง ุนูู ุญุฌู ุงูุฏูุนุฉ ุซู ูุถุฑูุจูุง
# ูู ุงูุนุฏุฏ ุงูุฅุฌูุงูู ููุญูุจุงุช. ูุงุญุธ ุฃู tf_train_dataset ููุง ูู tf.data.Dataset ุจูุฌููุนุงุชุ
# ูููุณ ูุฌููุนุฉ ุจูุงูุงุช Hugging Face ุงูุฃุตููุฉุ ูุฐุง ูุฅู len() ุงูุฎุงุตุฉ ุจู ูู ุจุงููุนู num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ูุฃุฎูุฑูุงุ ูููู ุจุถุจุท ุงููููุฐุฌ. ูุณุชุฎุฏู `PushToHubCallback` ูุญูุธ ุงููููุฐุฌ ุฅูู ุงููุฑูุฒ ุจุนุฏ ูู ุญูุจุฉุ ููุง ุณูุณูุญ ููุง ุจุงุณุชุฎุฏุงูู ููุงุณุชุฏูุงู ูุงุญููุง:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)

```
ููุฏ ุญุตููุง ุนูู ุจุนุถ ููู ุงูุฎุณุงุฑุฉ ุฃุซูุงุก ุงูุชุฏุฑูุจุ ูููููุง ูู ุงููุงูุน ูุฑูุฏ ุฑุคูุฉ ููุงููุณ ROUGE ุงูุชู ูููุง ุจุญุณุงุจูุง ูุณุจููุง. ููุญุตูู ุนูู ุชูู ุงูููุงููุณุ ุณูุญุชุงุฌ ุฅูู ุชูููุฏ ุงููุฎุฑุฌุงุช ูู ุงููููุฐุฌ ูุชุญููููุง ุฅูู ุณูุงุณู ูุตูุฉ. ุฏุนูุง ูุจูู ุจุนุถ ุงูููุงุฆู ูู ุงูุนูุงูุงุช ูุงูุชูุจุคุงุช ูููุงุฑูุฉ ูููุงุณ ROUGE (ููุงุญุธุฉ: ุฅุฐุง ุญุตูุช ุนูู ุฃุฎุทุงุก ุงุณุชูุฑุงุฏ ููุฐุง ุงููุณูุ ููุฏ ุชุญุชุงุฌ ุฅูู `!pip install tqdm`). ุณูุณุชุฎุฏู ุฃูุถูุง ุฎุฏุนุฉ ุชุฒูุฏ ุงูุฃุฏุงุก ุจุดูู ูุจูุฑ - ููู ุชุฌููุน ููุฏ ุงูุชูููุฏ ุงูุฎุงุต ุจูุง ูุน [XLA](https://www.tensorflow.org/xla)ุ ููู ุงููุฌูุน ุงููุนุฌู ููุฌุจุฑ ุงูุฎุทู ูู TensorFlow. ูุทุจู XLA ุงูุนุฏูุฏ ูู ุงูุชุญุณููุงุช ุนูู ุฑุณู ุงูุญุณุงุจ ูููููุฐุฌุ ููุง ูุคุฏู ุฅูู ุชุญุณููุงุช ูุจูุฑุฉ ูู ุงูุณุฑุนุฉ ูุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ. ููุง ูู ููุถุญ ูู ูุฏููุฉ Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate)ุ ูุนูู XLA ุจุดูู ุฃูุถู ุนูุฏูุง ูุง ุชุฎุชูู ุฃุดูุงู ุงูุฅุฏุฎุงู ูุฏููุง ูุซูุฑูุง. ููุนุงูุฌุฉ ูุฐุงุ ุณูููู ุจููุก ุฅุฏุฎุงูุงุชูุง ุฅูู ูุถุงุนูุงุช 128ุ ููุตูุน ูุฌููุนุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ ูุน ุฌุงูุน ุงูููุกุ ุซู ุณูุทุจู ุงูุฏูููุฑ `@tf.function(jit_compile=True)` ุนูู ุฏุงูุฉ ุงูุชูููุฏ ุงูุฎุงุตุฉ ุจูุงุ ูุงูุชู ุชุดูุฑ ุฅูู ุงูุฏุงูุฉ ุจุฃููููุง ููุชุฌููุน ูุน XLA.

```python
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

ุจูุฌุฑุฏ ุฃู ูุญุตู ุนูู ููุงุฆููุง ูู ุงูุณูุงุณู ุงููุตูุฉ ููุนูุงูุงุช ูุงูุชูุจุคุงุชุ ุณูููู ุญุณุงุจ ุฏุฑุฌุฉ ROUGE ุณููุงู:

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```

{/if}

{#if fw === 'pt'}

## ุถุจุท ูููุฐุฌ mT5 ุงูุฏููู ูุน ๐ค Accelerate[[fine-tuning-mt5-with-accelerate]]

ุถุจุท ูููุฐุฌูุง ุงูุฏููู ูุน ๐ค Accelerate ูุดุงุจู ุฌุฏูุง ููุซุงู ุชุตููู ุงููุตูุต ุงูุฐู ูุงุฌููุงู ูู [ุงููุตู 3](/course/chapter3). ุณุชููู ุงูุงุฎุชูุงูุงุช ุงูุฑุฆูุณูุฉ ูู ุงูุญุงุฌุฉ ุฅูู ุชูููุฏ ููุฎุตุงุชูุง ุจุดูู ุตุฑูุญ ุฃุซูุงุก ุงูุชุฏุฑูุจ ูุชุญุฏูุฏ ููููุฉ ุญุณุงุจ ุฏุฑุฌุงุช ROUGE (ุชุฐูุฑ ุฃู `Seq2SeqTrainer` ุชููู ุนูููุฉ ุงูุชูููุฏ ููุงุจุฉ ุนูุง). ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ููููุฉ ุชูููุฐ ูุฐูู ุงูุดุฑุทูู ุถูู ๐ค Accelerate!

### ุฅุนุฏุงุฏ ูู ุดูุก ููุชุฏุฑูุจ[[preparing-everything-for-training]]

ุฃูู ุดูุก ูุญุชุงุฌ ุฅูู ูุนูู ูู ุฅูุดุงุก `DataLoader` ููู ูู ุฃูุณุงู ุจูุงูุงุชูุง. ูุธุฑูุง ูุฃู ูุญููุงุช ุจูุงูุงุช PyTorch ุชุชููุน ุฏูุนุงุช ูู ุงููุตูููุงุชุ ูุญุชุงุฌ ุฅูู ุชุนููู ุงูุชูุณูู ุฅูู `"torch"` ูู ูุฌููุนุงุช ุจูุงูุงุชูุง:

```python
tokenized_datasets.set_format("torch")
```

ุงูุขู ุจุนุฏ ุฃู ุญุตููุง ุนูู ูุฌููุนุงุช ุจูุงูุงุช ุชุชููู ููุท ูู ุงููุตูููุงุชุ ูุฅู ุงูุดูุก ุงูุชุงูู ุงูุฐู ูุฌุจ ูุนูู ูู ุฅูุดุงุก ูุซูู ูู `DataCollatorForSeq2Seq` ูุฑุฉ ุฃุฎุฑู. ููุฐุง ูุญุชุงุฌ ุฅูู ุชูููุฑ ูุณุฎุฉ ุฌุฏูุฏุฉ ูู ุงููููุฐุฌุ ูุฐุง ุฏุนูุง ูุญููู ูุฑุฉ ุฃุฎุฑู ูู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ูุฏููุง:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ุจุนุฏ ุฐููุ ูููููุง ุฅูุดุงุก ูุซูู ูุฌุงูุน ุงูุจูุงูุงุช ูุงุณุชุฎุฏุงูู ูุชุญุฏูุฏ ูุญููุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

ุงูุดูุก ุงูุชุงูู ุงูุฐู ูุฌุจ ูุนูู ูู ุชุญุฏูุฏ ุงููุญุณู ุงูุฐู ูุฑูุฏ ุงุณุชุฎุฏุงูู. ููุง ูู ุงูุฃูุซูุฉ ุงูุฃุฎุฑูุ ุณูุณุชุฎุฏู `AdamW`ุ ูุงูุฐู ูุนูู ุจุดูู ุฌูุฏ ููุนุธู ุงููุดุงูู:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ุฃุฎูุฑูุงุ ูููู ุจุชุบุฐูุฉ ูููุฐุฌูุง ููุญุณููุง ููุญููุงุช ุงูุจูุงูุงุช ูุฏููุง ุฅูู ุทุฑููุฉ `accelerator.prepare()`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

๐จ ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู TPUุ ูุณุชุญุชุงุฌ ุฅูู ููู ูู ุงูููุฏ ุฃุนูุงู ุฅูู ูุธููุฉ ุชุฏุฑูุจ ูุฎุตุตุฉ. ุฑุงุฌุน [ุงููุตู 3](/course/chapter3) ููุฒูุฏ ูู ุงูุชูุงุตูู.

</Tip>

ุงูุขู ุจุนุฏ ุฃู ูููุง ุจุฅุนุฏุงุฏ ูุงุฆูุงุชูุงุ ููุงู ุซูุงุซุฉ ุฃุดูุงุก ูุชุจููุฉ ูุฌุจ ุงูููุงู ุจูุง:

* ุชุญุฏูุฏ ุฌุฏูู ูุนุฏู ุงูุชุนูู.
* ุชูููุฐ ูุธููุฉ ููุนุงูุฌุฉ ุงูููุฎุตุงุช ุจุนุฏ ุงูุชุฏุฑูุจ.
* ุฅูุดุงุก ูุณุชูุฏุน ุนูู Hub ูููููุง ุฏูุน ูููุฐุฌูุง ุฅููู.

ุจุงููุณุจุฉ ูุฌุฏูู ูุนุฏู ุงูุชุนููุ ุณูุณุชุฎุฏู ุงูุฌุฏูู ุงูุฎุทู ุงูููุงุณู ูู ุงูุฃูุณุงู ุงูุณุงุจูุฉ:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

ุจุงููุณุจุฉ ูููุนุงูุฌุฉ ุงููุงุญูุฉุ ูุญุชุงุฌ ุฅูู ูุธููุฉ ุชููู ุจุชูุณูู ุงูููุฎุตุงุช ุงููููุฏุฉ ุฅูู ุฌูู ููุตููุฉ ุจููุงุตู ุฃุณุทุฑ. ูุฐุง ูู ุงูุชูุณูู ุงูุฐู ูุชููุนู ูููุงุณ ROUGEุ ููููููุง ุชุญููู ุฐูู ุจุงุณุชุฎุฏุงู ููุชุทู ุงูููุฏ ุงูุชุงูู:

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE ูุชููุน ูุฌูุฏ ูุงุตู ุฃุณุทุฑ ุจุนุฏ ูู ุฌููุฉ
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

ูุฌุจ ุฃู ูุจุฏู ูุฐุง ูุฃููููุง ูู ุฅุฐุง ุชุฐูุฑุช ููู ูููุง ุจุชุนุฑูู ุฏุงูุฉ `compute_metrics()` ููููุฐุฌ `Seq2SeqTrainer`.

ุฃุฎูุฑูุงุ ูุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุณุชูุฏุน ูููุฐุฌ ุนูู Hugging Face Hub. ููุฐุงุ ูููููุง ุงุณุชุฎุฏุงู ููุชุจุฉ ๐ค Hub ุงูููุงุณุจุฉ. ูุญุชุงุฌ ููุท ุฅูู ุชุญุฏูุฏ ุงุณู ููุณุชูุฏุนูุงุ ูููููุชุจุฉ ูุธููุฉ ูุงุฆุฏุฉ ูุฏูุฌ ูุนุฑู ุงููุณุชูุฏุน ูุน ููู ุชุนุฑูู ุงููุณุชุฎุฏู:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

ุงูุขู ูููููุง ุงุณุชุฎุฏุงู ุงุณู ุงููุณุชูุฏุน ูุฐุง ูุงุณุชูุณุงุฎ ูุณุฎุฉ ูุญููุฉ ุฅูู ุฏููู ุงููุชุงุฆุฌ ูุฏููุง ูุงูุฐู ุณูุฎุฒู ูุชุงุฆุฌ ุงูุชุฏุฑูุจ:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ูุฐุง ุณูุณูุญ ููุง ุจุฅุนุงุฏุฉ ุงูุขุซุงุฑ ุฅูู ุงููุฑูุฒ ุงูุฑุฆูุณู ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `repo.push_to_hub()` ุฃุซูุงุก ุงูุชุฏุฑูุจ! ุฏุนูุง ุงูุขู ูููู ุชุญููููุง ุนู ุทุฑูู ูุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ.

### ุญููุฉ ุงูุชุฏุฑูุจ[[training-loop]]

ุญููุฉ ุงูุชุฏุฑูุจ ููุชูุฎูุต ูุดุงุจูุฉ ุฌุฏูุง ูุฃูุซูุฉ ๐ค Accelerate ุงูุฃุฎุฑู ุงูุชู ูุงุฌููุงูุง ูุชููุณู ุชูุฑูุจูุง ุฅูู ุฃุฑุจุน ุฎุทูุงุช ุฑุฆูุณูุฉ:

1. ุชุฏุฑูุจ ุงููููุฐุฌ ุนู ุทุฑูู ุงูุชูุฑุงุฑ ุนูู ุฌููุน ุงูุฃูุซูุฉ ูู `train_dataloader` ููู ุญูุจุฉ.
2. ุชูููุฏ ููุฎุตุงุช ุงููููุฐุฌ ูู ููุงูุฉ ูู ุญูุจุฉุ ุนู ุทุฑูู ุชูููุฏ ุงูุฑููุฒ ุฃููุงู ุซู ูู ุชุฑููุฒูุง (ูุงูููุฎุตุงุช ุงููุฑุฌุนูุฉ) ุฅูู ูุต.
3. ุญุณุงุจ ุฏุฑุฌุงุช ROUGE ุจุงุณุชุฎุฏุงู ููุณ ุงูุชูููุงุช ุงูุชู ุฑุฃููุงูุง ุณุงุจููุง.
4. ุญูุธ ููุงุท ุงูุชูุชูุด ูุฏูุน ูู ุดูุก ุฅูู ุงููุฑูุฒ ุงูุฑุฆูุณู. ููุง ูุนุชูุฏ ุนูู ุญุฌุฉ `blocking=False` ุงููููุฏุฉ ูููุถูุน `Repository` ุจุญูุซ ูููููุง ุฏูุน ููุงุท ุงูุชูุชูุด ููู ุญูุจุฉ _ุจุดูู ุบูุฑ ูุชุฒุงูู_. ูุณูุญ ููุง ูุฐุง ุจุงูุงุณุชูุฑุงุฑ ูู ุงูุชุฏุฑูุจ ุฏูู ุงูุญุงุฌุฉ ุฅูู ุงูุงูุชุธุงุฑ ููุชุญููู ุงูุจุทูุก ุฅูู ุญุฏ ูุง ุงููุฑุชุจุท ุจูููุฐุฌ ุจุญุฌู ุฌูุฌุงุจุงูุช!

ูููู ุฑุคูุฉ ูุฐู ุงูุฎุทูุงุช ูู ูุชูุฉ ุงูููุฏ ุงูุชุงููุฉ:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

ููุฐุง ูู ุดูุก! ุจูุฌุฑุฏ ุชุดุบูู ูุฐุงุ ุณูููู ูุฏูู ูููุฐุฌ ููุชุงุฆุฌ ูุดุงุจูุฉ ุฌุฏูุง ูุชูู ุงูุชู ุญุตููุง ุนูููุง ูุน `Trainer`.

{/if}

## ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงููุนุฏู ูุฏูู[[using-your-fine-tuned-model]]

ุจูุฌุฑุฏ ุฃู ุชุฏูุน ุจุงููููุฐุฌ ุฅูู ุงููุฑูุฒ ุงูุฑุฆูุณูุ ููููู ุงููุนุจ ุจู ุฅูุง ุนุจุฑ ุฃุฏุงุฉ ุงูุงุณุชุฏูุงู ุฃู ูุน ูุงุฆู `pipeline`ุ ููุง ููู:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

ูููููุง ุฅุทุนุงู ุจุนุถ ุงูุฃูุซูุฉ ูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ (ุงูุชู ูู ูุฑุงูุง ุงููููุฐุฌ) ุฅูู ุฎุท ุงูุฃูุงุจูุจ ุงูุฎุงุต ุจูุง ููุญุตูู ุนูู ููุฑุฉ ุนู ุฌูุฏุฉ ุงูููุฎุตุงุช. ุฏุนูุง ูููุฐ ูุธููุฉ ุจุณูุทุฉ ูุนุฑุถ ุงููุฑุงุฌุนุฉ ูุงูุนููุงู ูุงูููุฎุต ุงููููุฏ ูุนูุง:

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุฃุญุฏ ุงูุฃูุซูุฉ ุงูุฅูุฌููุฒูุฉ ุงูุชู ูุญุตู ุนูููุง:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesnโt come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. Itโs also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

ูุฐุง ููุณ ุณูุฆูุง ููุบุงูุฉ! ูููููุง ุฃู ูุฑู ุฃู ูููุฐุฌูุง ุชููู ุจุงููุนู ูู ุฃุฏุงุก ุงูุชูุฎูุต ุงูุงุณุชุฎูุงุตู ุนู ุทุฑูู ุฅุถุงูุฉ ุฃุฌุฒุงุก ูู ุงููุฑุงุฌุนุฉ ุจูููุงุช ุฌุฏูุฏุฉ. ููุนู ุงูุฌุงูุจ ุงูุฃุฑูุน ูู ูููุฐุฌูุง ูู ุฃูู ุซูุงุฆู ุงููุบุฉุ ูุฐูู ูููููุง ุฃูุถูุง ุชูููุฏ ููุฎุตุงุช ููุฑุงุฌุนุงุช ุจุงููุบุฉ ุงูุฅุณุจุงููุฉ:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

ูุชุฑุฌู ุงูููุฎุต ุฅูู "ุณูู ุงููุฑุงุกุฉ ุฌุฏูุง" ุจุงููุบุฉ ุงูุฅูุฌููุฒูุฉุ ูุงูุชู ูููููุง ุฃู ูุฑู ูู ูุฐู ุงูุญุงูุฉ ุชู ุงุณุชุฎูุงุตูุง ูุจุงุดุฑุฉ ูู ุงููุฑุงุฌุนุฉ. ููุน ุฐููุ ูุฅู ูุฐุง ูุธูุฑ ูุฑููุฉ ูููุฐุฌ mT5 ููุฏ ุฃุนุทุงู ููุฑุฉ ุนู ุงูุชุนุงูู ูุน ูุฌููุนุฉ ุจูุงูุงุช ูุชุนุฏุฏุฉ ุงููุบุงุช!

ุจุนุฏ ุฐููุ ุณููุฌู ุงูุชุจุงููุง ุฅูู ูููุฉ ุฃูุซุฑ ุชุนููุฏูุง ููููุงู: ุชุฏุฑูุจ ูููุฐุฌ ุงููุบุฉ ูู ุงูุตูุฑ.