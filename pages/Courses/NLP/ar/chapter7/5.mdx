<FrameworkSwitchCourse {fw} />

# تلخيص النصوص [[summarization]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
]} />

{/if}


في هذا القسم، سنلقي نظرة على كيفية استخدام نماذج المحول لتلخيص الوثائق الطويلة في ملخصات، وهي مهمة تُعرف باسم _تلخيص النصوص_. هذه واحدة من أصعب مهام معالجة اللغات الطبيعية لأنها تتطلب مجموعة من القدرات، مثل فهم المقاطع الطويلة وتوليد نص متماسك يلخص المواضيع الرئيسية في الوثيقة. ومع ذلك، عند القيام بها بشكل جيد، فإن تلخيص النصوص هو أداة قوية يمكن أن تسرع مختلف العمليات التجارية من خلال تخفيف عبء الخبراء في المجال لقراءة الوثائق الطويلة بالتفصيل.

<Youtube id="yHnr5Dk2zCI"/>

على الرغم من وجود العديد من النماذج المعدلة مسبقًا للتلخيص على [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads)، إلا أن جميع هذه النماذج تقريبًا مناسبة للوثائق الإنجليزية فقط. لذلك، لإضافة لمسة مختلفة في هذا القسم، سنقوم بتدريب نموذج ثنائي اللغة للإنجليزية والإسبانية. وبنهاية هذا القسم، سيكون لديك [نموذج](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) يمكنه تلخيص مراجعات العملاء مثل المراجعة الموضحة هنا:

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

كما سنرى، فإن هذه الملخصات موجزة لأنها مستفادة من العناوين التي يقدمها العملاء في مراجعاتهم للمنتجات. دعنا نبدأ بتجميع مجموعة ثنائية اللغة مناسبة لهذه المهمة.

## إعداد مجموعة بيانات متعددة اللغات [[preparing-a-multilingual-corpus]]

سنستخدم [مجموعة بيانات مراجعات أمازون متعددة اللغات](https://huggingface.co/datasets/amazon_reviews_multi) لإنشاء ملخص ثنائي اللغة. تتكون هذه المجموعة من مراجعات منتجات أمازون بست لغات، وتستخدم عادةً لقياس أداء المصنفات متعددة اللغات. ومع ذلك، نظرًا لأن كل مراجعة مصحوبة بعنوان قصير، يمكننا استخدام العناوين كملخصات مستهدفة ليتعلم منها نموذجنا! للبدء، دعنا نقوم بتنزيل المجموعات الفرعية الإنجليزية والإسبانية من Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

كما ترى، هناك 200,000 مراجعة لكل لغة في مجموعة `train`، و5,000 مراجعة لكل من مجموعتي `validation` و`test`. تتواجد معلومات المراجعة التي نهتم بها في عمودي `review_body` و`review_title`. دعنا نلقي نظرة على بعض الأمثلة من خلال إنشاء دالة بسيطة تأخذ عينة عشوائية من مجموعة التدريب باستخدام التقنيات التي تعلمناها في [الفصل 5](/course/chapter5):

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn’t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

✏️ **جربها!** قم بتغيير البذرة العشوائية في أمر `Dataset.shuffle()` لاستكشاف مراجعات أخرى في المجموعة. إذا كنت تتحدث الإسبانية، قم بالاطلاع على بعض المراجعات في `spanish_dataset` لمعرفة ما إذا كانت العناوين تبدو ملخصات معقولة أيضًا.

</Tip>

توضح هذه العينة تنوع المراجعات التي نجدها عادة عبر الإنترنت، والتي تتراوح من الإيجابية إلى السلبية (وكل ما بينهما). على الرغم من أن المثال بعنوان "meh" ليس مفيدًا جدًا، إلا أن العناوين الأخرى تبدو ملخصات جيدة للمراجعات نفسها. سيستغرق تدريب نموذج التلخيص على جميع المراجعات الأربعمائة ألف وقتًا طويلاً للغاية على وحدة معالجة رسومية واحدة، لذلك بدلاً من ذلك، سنركز على توليد ملخصات لمجال واحد من المنتجات. للحصول على فكرة عن المجالات التي يمكننا الاختيار منها، دعنا نحول `english_dataset` إلى `pandas.DataFrame` ونحسب عدد المراجعات لكل فئة من فئات المنتجات:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```python
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

المنتجات الأكثر شيوعًا في مجموعة البيانات الإنجليزية هي عن الأدوات المنزلية، والملابس، والإلكترونيات اللاسلكية. ولكن للالتزام بموضوع أمازون، دعنا نركز على تلخيص مراجعات الكتب - ففي النهاية، هذه هي الفكرة التي تأسست عليها الشركة! يمكننا أن نرى فئتين من المنتجات تناسب هذا الوصف (`الكتاب` و`شراء الكتاب الإلكتروني الرقمي`)، لذا دعنا نُصفِّ مجموعة البيانات في كلتا اللغتين لهذه المنتجات فقط. كما رأينا في [الفصل 5](/course/chapter5)، فإن دالة `Dataset.filter()` تسمح لنا بتقسيم مجموعة البيانات بفعالية، لذا يمكننا تعريف دالة بسيطة للقيام بذلك:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

الآن عندما نطبق هذه الدالة على `english_dataset` و`spanish_dataset`، ستتضمن النتيجة فقط الصفوف المتعلقة بفئات الكتب. وقبل تطبيق التصفية، دعنا نغير تنسيق `english_dataset` من `"pandas"` إلى `"arrow"`:

```python
english_dataset.reset_format()
```

بعد ذلك يمكننا تطبيق دالة التصفية، وكفحص للسلامة، دعنا نتفحص عينة من المراجعات لنرى إن كانت بالفعل عن الكتب:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

حسنًا، يمكننا أن نرى أن المراجعات ليست فقط عن الكتب، وقد تشير إلى أشياء مثل التقاويم والتطبيقات الإلكترونية مثل OneNote. ومع ذلك، يبدو المجال مناسبًا لتدريب نموذج التلخيص. وقبل أن ننظر إلى النماذج المختلفة المناسبة لهذه المهمة، لدينا خطوة أخيرة في إعداد البيانات: دمج المراجعات الإنجليزية والإسبانية في كائن `DatasetDict` واحد. توفر مكتبة 🤗 Datasets دالة `concatenate_datasets()` مفيدة (كما يوحي الاسم) ستضع كائني `Dataset` فوق بعضهما البعض. لذا، لإنشاء مجموعة بيانات ثنائية اللغة، سنقوم بالتمرير على كل تقسيم، ودمج مجموعات البيانات لذلك التقسيم، ومزج النتيجة لضمان عدم تعلم النموذج بشكل مفرط على لغة واحدة:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# تفقد بعض الأمثلة
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÑADO'
'>> Review: Me llegó el día que tocaba, junto a otros libros que pedí, pero la caja llegó en mal estado lo cual dañó las esquinas de los libros porque venían sin protección (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

هذا بالتأكيد يبدو مزيجًا من المراجعات الإنجليزية والإسبانية! الآن بعد أن أصبح لدينا مجموعة بيانات للتدريب، هناك شيء أخير يجب التحقق منه وهو توزيع الكلمات في المراجعات وعناوينها. هذا مهم بشكل خاص في مهام التلخيص، حيث يمكن للملخصات المرجعية القصيرة في البيانات أن تحيز النموذج لإنتاج ملخصات من كلمة واحدة أو كلمتين فقط. توضح الرسوم البيانية أدناه توزيعات الكلمات، ويمكننا أن نرى أن العناوين تميل بشدة نحو كلمة واحدة أو كلمتين فقط:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="توزيعات عدد الكلمات للعناوين والمراجعات."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="توزيعات عدد الكلمات للعناوين والمراجعات."/>
</div>

للتعامل مع هذا، سنقوم بتصفية الأمثلة ذات العناوين القصيرة جدًا حتى يتمكن نموذجنا من إنتاج ملخصات أكثر إثارة للاهتمام. بما أننا نتعامل مع نصوص إنجليزية وإسبانية، يمكننا استخدام تقريب تقريبي لتقسيم العناوين على المسافات البيضاء، ثم استخدام طريقة `Dataset.filter()` الموثوقة كما يلي:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```
الآن بعد أن قمنا بتحضير مجموعة البيانات الخاصة بنا، دعنا نلقي نظرة على بعض نماذج المحول (Transformer) المحتملة التي يمكن ضبطها الدقيق على هذه المجموعة!

## نماذج لتلخيص النصوص [[models-for-text-summarization]]

إذا فكرت في الأمر، فإن تلخيص النصوص هو مهمة مشابهة لترجمة الآلة: لدينا نص مثل مراجعة نريد "ترجمتها" إلى نسخة أقصر تلتقط الميزات البارزة للنص الأصلي. وبناءً على ذلك، فإن معظم نماذج المحول لتلخيص النصوص تعتمد بنية الترميز-فك الترميز (encoder-decoder) التي تعرفنا عليها في [الفصل 1](/course/chapter1)، على الرغم من وجود بعض الاستثناءات مثل عائلة نماذج GPT التي يمكن استخدامها أيضًا لتلخيص النصوص في إعدادات قليلة التصويب. يدرج الجدول التالي بعض النماذج المُدربة مسبقًا والشائعة التي يمكن ضبطها الدقيق لتلخيص النصوص.

| نموذج المحول | الوصف                                                                                                                                                                                                    | متعدد اللغات؟ |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | على الرغم من تدريبه كنموذج لغوي تنبؤي ذاتي، يمكنك جعل GPT-2 يولد ملخصات بإضافة "TL;DR" في نهاية النص المدخل.                                                                          |      ❌       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | يستخدم هدفًا مسبقًا للتنبؤ بالجمل المقنعة في النصوص متعددة الجمل. هذا الهدف المسبق أقرب إلى تلخيص النصوص من النمذجة اللغوية التقليدية ويحقق نتائج عالية في المعايير الشائعة. |      ❌       |
|     [T5](https://huggingface.co/t5-base)      | بنية محول عالمية تصيغ جميع المهام في إطار نص-إلى-نص؛ على سبيل المثال، تنسيق الإدخال للنموذج لتلخيص وثيقة هو `summarize: ARTICLE`.                              |      ❌       |
|     [mT5](https://huggingface.co/google/mt5-base)     | نسخة متعددة اللغات من T5، مُدربة مسبقًا على مجموعة بيانات Common Crawl متعددة اللغات (mC4)، تغطي 101 لغة.                                                                                                |      ✅       |
|    [BART](https://huggingface.co/facebook/bart-base)     | بنية محول جديدة تحتوي على كل من طبقة الترميز وطبقة فك الترميز المدربتين على إعادة بناء الإدخال المشوه، وتجمع بين مخططات التدريب المسبق لنماذج BERT وGPT-2.                                    |      ❌       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | نسخة متعددة اللغات من BART، مُدربة مسبقًا على 50 لغة.                                                                                                                                                     |      ✅       |

كما ترى من هذا الجدول، فإن معظم نماذج المحول لتلخيص النصوص (ومعظم مهام معالجة اللغات الطبيعية) أحادية اللغة. هذا رائع إذا كانت مهمتك بلغة "غنية بالموارد" مثل الإنجليزية أو الألمانية، ولكن ليس جيدًا للآلاف من اللغات الأخرى المستخدمة في جميع أنحاء العالم. لحسن الحظ، هناك فئة من نماذج المحول متعددة اللغات، مثل mT5 وmBART، التي تأتي للمساعدة. هذه النماذج مُدربة مسبقًا باستخدام النمذجة اللغوية، ولكن مع اختلاف: بدلاً من التدريب على مجموعة بيانات بلغة واحدة، يتم تدريبها بشكل مشترك على نصوص بأكثر من 50 لغة في نفس الوقت!

سنركز على mT5، بنية مثيرة للاهتمام مبنية على T5 تم تدريبها مسبقًا في إطار نص-إلى-نص. في T5، يتم صياغة كل مهمة من مهام معالجة اللغات الطبيعية من حيث بادئة موجهة مثل `summarize:` التي تجعل النموذج يُكيّف النص المولد وفقًا للموجه. كما هو موضح في الشكل أدناه، هذا يجعل T5 متعدد الاستخدامات للغاية، حيث يمكنك حل العديد من المهام باستخدام نموذج واحد!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="مهام مختلفة يؤديها نموذج T5."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="مهام مختلفة يؤديها نموذج T5."/>
</div>

لا يستخدم mT5 البادئات، ولكنه يشارك الكثير من تعدد الاستخدامات مع T5 ويتمتع بميزة كونه متعدد اللغات. الآن بعد أن اخترنا نموذجًا، دعنا نلقي نظرة على تحضير بياناتنا للتدريب.


<Tip>

✏️ **جربه!** بعد الانتهاء من هذا القسم، شاهد كيف يقارن mT5 مع mBART من خلال ضبط الأخير باستخدام نفس التقنيات. وللحصول على نقاط إضافية، يمكنك أيضًا تجربة ضبط T5 على المراجعات الإنجليزية فقط. نظرًا لأن T5 لديه بادئة موجهة خاصة، ستحتاج إلى إضافة `summarize:` إلى الأمثلة المدخلة في خطوات ما قبل المعالجة أدناه.

</Tip>

## معالجة البيانات مسبقًا [[preprocessing-the-data]]

<Youtube id="1m7BerpSq8A"/>

مهمتنا التالية هي تقسيم المراجعات وعناوينها إلى رموز. كما هو معتاد، نبدأ بتحميل المقسم إلى رموز المرتبط بنقطة تفتيش النموذج المُدرب مسبقًا. سنستخدم `mt5-small` كنقطة تفتيش لنا حتى نتمكن من ضبط النموذج الدقيق في وقت معقول:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

💡 في المراحل المبكرة من مشاريع معالجة اللغات الطبيعية، من الممارسات الجيدة تدريب فئة من النماذج "الصغيرة" على عينة صغيرة من البيانات. هذا يسمح لك بتصحيح الأخطاء وتكرار العملية بشكل أسرع نحو سير عمل من البداية إلى النهاية. بمجرد أن تكون واثقًا من النتائج، يمكنك دائمًا زيادة حجم النموذج ببساطة عن طريق تغيير نقطة تفتيش النموذج!

</Tip>

دعنا نجرب مقسم الرموز mT5 على مثال صغير:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

هنا يمكننا رؤية `input_ids` و`attention_mask` المألوفين الذين تعرفنا عليهم في تجاربنا الأولى لضبط النموذج الدقيق في [الفصل 3](/course/chapter3). دعنا نقوم بفك ترميز هذه المعرّفات المدخلة باستخدام دالة `convert_ids_to_tokens()` لمقسم الرموز لمعرفة نوع مقسم الرموز الذي نتعامل معه:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '</s>']
```

الرمز الخاص Unicode `▁` ورمز نهاية السلسلة `</s>` يشيران إلى أننا نتعامل مع مقسم رموز SentencePiece، والذي يعتمد على خوارزمية التجزئة Unigram التي ناقشناها في [الفصل 6](/course/chapter6). Unigram مفيد بشكل خاص للمجموعات متعددة اللغات لأنه يسمح لـ SentencePiece بأن يكون غير متحيز للتشكيل والتنقيط وحقيقة أن العديد من اللغات، مثل اليابانية، لا تحتوي على أحرف المسافة.

لتحويل مجموعة البيانات الخاصة بنا إلى رموز، يجب أن نتعامل مع دقة مرتبطة بتلخيص النصوص: لأن علاماتنا أيضًا نص، فمن الممكن أن تتجاوز حجم السياق الأقصى للنموذج. هذا يعني أننا بحاجة إلى تطبيق التقطيع على كل من المراجعات وعناوينها لضمان عدم تمرير مدخلات طويلة بشكل مفرط إلى نموذجنا. توفر مقسمات الرموز في 🤗 Transformers حجة `text_target` مفيدة تسمح لك بتحويل العلامات إلى رموز بالتوازي مع المدخلات. فيما يلي مثال على كيفية معالجة المدخلات والعلامات لـ mT5:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```
دعونا نمر عبر هذا الكود لفهم ما يحدث. أول شيء قمنا به هو تحديد قيم لـ `max_input_length` و `max_target_length`، والتي تحدد الحدود العليا لطول المراجع والعناوين التي يمكننا استخدامها. بما أن نص المراجعة يكون عادةً أكبر بكثير من العنوان، فقد قمنا بضبط هذه القيم وفقًا لذلك.

مع `preprocess_function()`، يصبح من السهل توكينز (تقسيم النص إلى وحدات معجمية) الكوربوس (مجموعة النصوص) بالكامل باستخدام دالة `Dataset.map()` المفيدة التي استخدمناها بشكل مكثف خلال هذه الدورة:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

الآن بعد أن تم معالجة الكوربوس مسبقًا، دعونا نلقي نظرة على بعض المقاييس المستخدمة عادةً في تلخيص النصوص. كما سنرى، لا يوجد حل سحري عندما يتعلق الأمر بقياس جودة النصوص المولدة آليًا.

<Tip>

💡 ربما لاحظت أننا استخدمنا `batched=True` في دالة `Dataset.map()` أعلاه. هذا يشفر الأمثلة في مجموعات من 1,000 (القيمة الافتراضية) ويسمح لك باستخدام قدرات تعدد الخيوط في المحولات السريعة 🤗. حيثما أمكن، حاول استخدام `batched=True` للاستفادة القصوى من معالجة البيانات المسبقة!

</Tip>


## مقاييس تلخيص النصوص [[metrics-for-text-summarization]]

<Youtube id="TMshhnrEXlg"/>

بالمقارنة مع معظم المهام الأخرى التي قمنا بتغطيتها في هذه الدورة، فإن قياس أداء مهام توليد النصوص مثل التلخيص أو الترجمة ليس بالأمر السهل. على سبيل المثال، بالنظر إلى مراجعة مثل "أحببت قراءة ألعاب الجوع"، هناك ملخصات صالحة متعددة، مثل "أحببت ألعاب الجوع" أو "ألعاب الجوع قراءة رائعة". من الواضح أن تطبيق نوع من المطابقة الدقيقة بين الملخص المولد والملصق ليس حلاً جيدًا - حتى البشر سيحصلون على نتائج سيئة وفقًا لمثل هذا المقياس، لأن لكل منا أسلوبه الخاص في الكتابة.

أحد المقاييس الأكثر استخدامًا في تلخيص النصوص هو [درجة ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (اختصارًا لـ Recall-Oriented Understudy for Gisting Evaluation). الفكرة الأساسية وراء هذا المقياس هي مقارنة ملخص مولد بمجموعة من الملخصات المرجعية التي عادة ما يقوم بها البشر. لجعل هذا أكثر دقة، لنفترض أننا نريد مقارنة الملخصين التاليين:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

إحدى طرق المقارنة بينهما يمكن أن تكون بعدد الكلمات المتداخلة، والتي في هذه الحالة ستكون 6. ولكن هذه الطريقة بدائية بعض الشيء، لذلك بدلاً من ذلك، تعتمد ROUGE على حساب درجات _الدقة_ و_الاستدعاء_ للتداخل.

<Tip>

🙋 لا تقلق إذا كانت هذه هي المرة الأولى التي تسمع فيها عن الدقة والاستدعاء - سنمر عبر بعض الأمثلة الصريحة معًا لتوضيح الأمر. هذه المقاييس عادة ما يتم مواجهتها في مهام التصنيف، لذلك إذا كنت ترغب في فهم كيفية تحديد الدقة والاستدعاء في هذا السياق، فإننا نوصي بالاطلاع على [الدلائل](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) في `scikit-learn`.

</Tip>

بالنسبة لـ ROUGE، يقيس الاستدعاء مقدار الملخص المرجعي الذي تم التقاطه بواسطة الملخص المولد. إذا كنا نقارن فقط الكلمات، يمكن حساب الاستدعاء وفقًا للمعادلة التالية:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

بالنسبة لمثالنا البسيط أعلاه، تعطي هذه المعادلة استدعاءً مثاليًا 6/6 = 1؛ أي أن جميع الكلمات في الملخص المرجعي تم إنتاجها بواسطة النموذج. قد يبدو هذا رائعًا، ولكن تخيل إذا كان ملخصنا المولد هو "I really really loved reading the Hunger Games all night". سيكون لهذا أيضًا استدعاءً مثاليًا، ولكنه ملخص أسوأ لأن النص مطنب. للتعامل مع هذه السيناريوهات، نحسب أيضًا الدقة، والتي في سياق ROUGE تقيس مدى ملاءمة الملخص المولد:

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

تطبيق هذا على ملخصنا المطنب يعطي دقة 6/10 = 0.6، والتي تعتبر أسوأ بكثير من الدقة 6/7 = 0.86 التي حصلنا عليها من ملخصنا الأقصر. في الممارسة العملية، يتم حساب كل من الدقة والاستدعاء عادةً، ثم يتم الإبلاغ عن F1-score (المتوسط التوافقي للدقة والاستدعاء). يمكننا القيام بذلك بسهولة في 🤗 Datasets عن طريق تثبيت حزمة `rouge_score` أولاً:

```py
!pip install rouge_score
```

ثم تحميل مقياس ROUGE كما يلي:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

بعد ذلك، يمكننا استخدام دالة `rouge_score.compute()` لحساب جميع المقاييس مرة واحدة:

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

يا له من كم كبير من المعلومات في هذا المخرج - ماذا يعني كل هذا؟ أولاً، 🤗 Datasets يحسب فترات الثقة للدقة والاستدعاء وF1-score؛ هذه هي سمات `low` و`mid` و`high` التي يمكنك رؤيتها هنا. علاوة على ذلك، يحسب 🤗 Datasets مجموعة متنوعة من درجات ROUGE والتي تستند إلى أنواع مختلفة من دقة النص عند مقارنة الملخصات المولدة والمرجعية. تُستخدم نسخة `rouge1` للتداخل بين الكلمات المنفردة - هذه طريقة معقدة للقول إنها تداخل الكلمات وهي بالضبط المقياس الذي ناقشناه أعلاه. للتحقق من ذلك، دعنا نستخرج قيمة `mid` من درجاتنا:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

رائع، أرقام الدقة والاستدعاء تتطابق! والآن ماذا عن درجات ROUGE الأخرى؟ تقيس `rouge2` التداخل بين الكلمات المزدوجة (فكر في تداخل أزواج الكلمات)، بينما `rougeL` و`rougeLsum` تقيس أطول تسلسلات متطابقة من الكلمات عن طريق البحث عن أطول السلاسل المشتركة في الملخصات المولدة والمرجعية. يشير "sum" في `rougeLsum` إلى حقيقة أن هذا المقياس يتم حسابه على ملخص كامل، بينما يتم حساب `rougeL` كمتوسط على الجمل الفردية.

<Tip>

✏️ **جربها!** قم بإنشاء مثالك الخاص لملخص مولد وملخص مرجعي وانظر ما إذا كانت درجات ROUGE تتفق مع الحساب اليدوي بناءً على المعادلات للدقة والاستدعاء. للحصول على نقاط إضافية، قم بتقسيم النص إلى كلمات مزدوجة وقارن الدقة والاستدعاء لمقياس `rouge2`.

</Tip>

سنستخدم هذه الدرجات ROUGE لتتبع أداء نموذجنا، ولكن قبل القيام بذلك دعنا نفعل شيئًا يجب على كل ممارس جيد للـ NLP فعله: إنشاء خط أساس قوي وبسيط!

### إنشاء خط أساس قوي [[creating-a-strong-baseline]]

خط الأساس الشائع لتلخيص النصوص هو ببساطة أخذ الجمل الثلاث الأولى من مقال، والذي يُطلق عليه غالبًا _lead-3_ baseline. يمكننا استخدام النقاط الكاملة لتتبع حدود الجمل، ولكن هذا سيفشل في الاختصارات مثل "U.S." أو "U.N." - لذلك بدلاً من ذلك، سنستخدم مكتبة `nltk`، والتي تتضمن خوارزمية أفضل للتعامل مع هذه الحالات. يمكنك تثبيت الحزمة باستخدام `pip` كما يلي:

```python
!pip install nltk
```

ثم تنزيل قواعد الترقيم:

```python
import nltk

nltk.download("punkt")
```
ثم قم بتنزيل قواعد الترقيم:

```python
import nltk

nltk.download("punkt")
```

بعد ذلك، نستورد محدد الجمل من `nltk` وننشئ دالة بسيطة لاستخراج الجمل الثلاث الأولى في المراجعة. الاتفاقية في تلخيص النص هي فصل كل ملخص بسطر جديد، لذا دعنا نقوم بذلك أيضًا ونختبرها على مثال تدريبي:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

يبدو أن هذا يعمل، لذا دعنا الآن ننفذ دالة تستخرج هذه "الملخصات" من مجموعة البيانات وتحسب درجات ROUGE للخط الأساسي:

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

يمكننا بعد ذلك استخدام هذه الدالة لحساب درجات ROUGE على مجموعة التحقق وجعلها أكثر وضوحًا باستخدام Pandas:

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

يمكننا أن نرى أن درجة `rouge2` أقل بكثير من البقية؛ وهذا يعكس على الأرجح حقيقة أن عناوين المراجعات تكون عادة موجزة، لذا فإن خط الأساس المكون من ثلاث جمل يكون طويلاً جدًا. الآن بعد أن أصبح لدينا خط أساس جيد للعمل منه، دعنا نوجه انتباهنا نحو الضبط الدقيق لـ mT5!

{#if fw === 'pt'}

## الضبط الدقيق لـ mT5 باستخدام واجهة برمجة التطبيقات `Trainer`[[fine-tuning-mt5-with-the-trainer-api]]

الضبط الدقيق لنموذج للتلخيص يشبه إلى حد كبير المهام الأخرى التي قمنا بتغطيتها في هذا الفصل. أول شيء نحتاج إلى القيام به هو تحميل النموذج المسبق التدريب من نقطة التحقق `mt5-small`. بما أن التلخيص هو مهمة تسلسل إلى تسلسل، يمكننا تحميل النموذج باستخدام فئة `AutoModelForSeq2SeqLM`، والتي ستقوم تلقائيًا بتنزيل الأوزان وتخزينها مؤقتًا:

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## الضبط الدقيق لـ mT5 باستخدام Keras[[fine-tuning-mt5-with-keras]]

الضبط الدقيق لنموذج للتلخيص يشبه إلى حد كبير المهام الأخرى التي قمنا بتغطيتها في هذا الفصل. أول شيء نحتاج إلى القيام به هو تحميل النموذج المسبق التدريب من نقطة التحقق `mt5-small`. بما أن التلخيص هو مهمة تسلسل إلى تسلسل، يمكننا تحميل النموذج باستخدام فئة `TFAutoModelForSeq2SeqLM`، والتي ستقوم تلقائيًا بتنزيل الأوزان وتخزينها مؤقتًا:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

💡 إذا كنت تتساءل عن سبب عدم ظهور أي تحذيرات حول الضبط الدقيق للنموذج على مهمة أسفل النهر، فذلك لأننا بالنسبة لمهام التسلسل إلى التسلسل نحتفظ بجميع أوزان الشبكة. قارن هذا بالنموذج الخاص بنا في التصنيف النصي في [الفصل 3](/course/chapter3)، حيث تم استبدال رأس النموذج المسبق التدريب بشبكة تم تهيئتها عشوائيًا.

</Tip>

الشيء التالي الذي نحتاج إلى القيام به هو تسجيل الدخول إلى Hugging Face Hub. إذا كنت تقوم بتشغيل هذا الرمز في دفتر ملاحظات، فيمكنك القيام بذلك باستخدام دالة المساعدة التالية:

```python
from huggingface_hub import notebook_login

notebook_login()
```

والتي ستعرض أداة يمكنك من خلالها إدخال بيانات اعتمادك. أو يمكنك تشغيل هذا الأمر في طرفية النظام الخاص بك وتسجيل الدخول هناك:

```
huggingface-cli login
```

{#if fw === 'pt'}

سنحتاج إلى توليد ملخصات لحساب درجات ROUGE أثناء التدريب. لحسن الحظ، يوفر 🤗 Transformers فئات مخصصة `Seq2SeqTrainingArguments` و`Seq2SeqTrainer` التي يمكنها القيام بذلك تلقائيًا! لمعرفة كيفية عمل ذلك، دعنا نحدد أولاً فرط المعلمات والحجج الأخرى لتجاربنا:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# عرض خسارة التدريب مع كل حقبة
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

هنا، تم ضبط حجة `predict_with_generate` للإشارة إلى أنه يجب علينا توليد ملخصات أثناء التقييم بحيث يمكننا حساب درجات ROUGE لكل حقبة. كما تمت مناقشته في [الفصل 1](/course/chapter1)، يقوم فك التشفير بالاستدلال عن طريق التنبؤ بالرموز واحدًا تلو الآخر، وهذا يتم تنفيذه بواسطة طريقة `generate()` للنموذج. يخبر تعيين `predict_with_generate=True` المدرب `Seq2SeqTrainer` باستخدام تلك الطريقة للتقييم. لقد قمنا أيضًا بتعديل بعض فرط المعلمات الافتراضية، مثل معدل التعلم، وعدد الحقبات، ووزن التلاشي، وقمنا بضبط خيار `save_total_limit` لحفظ ما يصل إلى 3 نقاط تحقق فقط أثناء التدريب - وذلك لأن حتى الإصدار "الصغير" من mT5 يستخدم حوالي جيجابايت من مساحة القرص الصلب، ويمكننا توفير بعض المساحة عن طريق الحد من عدد النسخ التي نحفظها.

ستسمح لنا حجة `push_to_hub=True` بدفع النموذج إلى Hub بعد التدريب؛ ستجد المستودع تحت ملفك الشخصي في الموقع المحدد بواسطة `output_dir`. لاحظ أنه يمكنك تحديد اسم المستودع الذي تريد دفعه باستخدام حجة `hub_model_id` (على وجه الخصوص، سيتعين عليك استخدام هذه الحجة للدفع إلى منظمة). على سبيل المثال، عندما قمنا بدفع النموذج إلى منظمة [`huggingface-course`](https://huggingface.co/huggingface-course)، أضفنا `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` إلى `Seq2SeqTrainingArguments`.

الشيء التالي الذي نحتاج إلى القيام به هو تزويد المدرب بدالة `compute_metrics()` بحيث يمكننا تقييم نموذجنا أثناء التدريب. بالنسبة للتلخيص، هذا أكثر تعقيدًا من مجرد استدعاء `rouge_score.compute()` على تنبؤات النموذج، حيث نحتاج إلى _فك تشفير_ المخرجات والعلامات إلى نص قبل أن نتمكن من حساب درجات ROUGE. تقوم الدالة التالية بذلك بالضبط، وتستخدم أيضًا دالة `sent_tokenize()` من `nltk` لفصل جمل الملخص بسطور جديدة:

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # فك تشفير الملخصات المولدة إلى نص
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # استبدل -100 في العلامات حيث لا يمكننا فك تشفيرها
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # فك تشفير ملخصات المرجع إلى نص
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # يتوقع ROUGE وجود سطر جديد بعد كل جملة
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # حساب درجات ROUGE
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # استخراج الدرجات المتوسطة
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

الآن، نحتاج إلى تحديد جامع بيانات لمهامنا من تسلسل إلى تسلسل. بما أن mT5 هو نموذج محول من مشفر-فاك، فإن إحدى الدقائق في إعداد دفعاتنا هي أنه أثناء فك التشفير، نحتاج إلى تحويل الملصقات إلى اليمين بواحد. هذا مطلوب لضمان أن الفاك لا يرى سوى الملصقات الحقيقية السابقة وليس الحالية أو المستقبلية، والتي سيكون من السهل على النموذج تذكرها. هذا مشابه لكيفية تطبيق الاهتمام الذاتي المقنع على المدخلات في مهمة مثل [نمذجة اللغة السببية](/course/chapter7/6).

لحسن الحظ، يوفر 🤗 Transformers جامع بيانات `DataCollatorForSeq2Seq` الذي سيقوم تلقائيًا بملء المدخلات والملصقات. لإنشاء هذا الجامع، نحتاج ببساطة إلى توفير `tokenizer` و`model`:

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

دعنا نرى ما ينتجه هذا الجامع عند إطعامه دفعة صغيرة من الأمثلة. أولاً، نحتاج إلى إزالة الأعمدة ذات السلاسل النصية لأن جامع البيانات لن يعرف كيفية ملء هذه العناصر:

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

بما أن جامع البيانات يتوقع قائمة من `dict`s، حيث يمثل كل `dict` مثالًا واحدًا في مجموعة البيانات، نحتاج أيضًا إلى ترتيب البيانات في التنسيق المتوقع قبل تمريرها إلى جامع البيانات:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

الشيء الرئيسي الذي يجب ملاحظته هنا هو أن المثال الأول أطول من الثاني، لذا تم ملء `input_ids` و`attention_mask` للمثال الثاني على اليمين برمز `[PAD]` (الذي يكون معرفه `0`). وبالمثل، يمكننا أن نرى أن `labels` تم ملؤها برموز `-100`، للتأكد من تجاهل رموز الملء بواسطة دالة الخسارة. وأخيرًا، يمكننا أن نرى `decoder_input_ids` جديد الذي قام بتحويل الملصقات إلى اليمين بإدراج رمز `[PAD]` في الإدخال الأول.

{#if fw === 'pt'}

أخيرًا، لدينا جميع المكونات التي نحتاجها للتدريب! نحتاج الآن ببساطة إلى إنشاء المدرب بالحجج القياسية:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

ثم إطلاق عملية التدريب:

```python
trainer.train()
```

أثناء التدريب، يجب أن ترى انخفاض خسارة التدريب وزيادة درجات ROUGE مع كل حقبة. بمجرد اكتمال التدريب، يمكنك رؤية درجات ROUGE النهائية بتشغيل `Trainer.evaluate()`:

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

من الدرجات يمكننا أن نرى أن نموذجنا تفوق بسهولة على خط الأساس لدينا - رائع! الشيء الأخير الذي يجب فعله هو دفع أوزان النموذج إلى المركز، كما يلي:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

سيقوم هذا بحفظ ملفات نقطة التفتيش والتكوين إلى `output_dir`، قبل تحميل جميع الملفات إلى المركز. من خلال تحديد حجة `tags`، نضمن أيضًا أن الأداة في المركز ستكون واحدة لخط أنابيب الملخص بدلاً من أداة إنشاء النص الافتراضية المرتبطة بهندسة mT5 (لمزيد من المعلومات حول علامات النموذج، راجع [وثائق 🤗 Hub](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). المخرجات من `trainer.push_to_hub()` هي عنوان URL لرمز Git commit، لذا يمكنك بسهولة رؤية التغييرات التي تم إجراؤها على مستودع النموذج!

لإنهاء هذا القسم، دعنا نلقي نظرة على كيفية ضبط نموذج mT5 أيضًا باستخدام الميزات منخفضة المستوى التي يوفرها 🤗 Accelerate.

{:else}

نحن على استعداد للتدريب تقريبًا! نحتاج فقط إلى تحويل مجموعات البيانات الخاصة بنا إلى `tf.data.Dataset`s باستخدام جامع البيانات الذي حددناه أعلاه، ثم `compile()` و`fit()` النموذج. أولاً، مجموعات البيانات:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

الآن، نحدد فرط معلمات التدريب ونقوم بالترجمة:

```python
from transformers import create_optimizer
import tensorflow as tf

# عدد خطوات التدريب هو عدد العينات في مجموعة البيانات، مقسومًا على حجم الدفعة ثم مضروبًا
# في العدد الإجمالي للحقبات. لاحظ أن tf_train_dataset هنا هو tf.data.Dataset بمجموعات،
# وليس مجموعة بيانات Hugging Face الأصلية، لذا فإن len() الخاصة به هي بالفعل num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

وأخيرًا، نقوم بضبط النموذج. نستخدم `PushToHubCallback` لحفظ النموذج إلى المركز بعد كل حقبة، مما سيسمح لنا باستخدامه للاستدلال لاحقًا:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)

```
لقد حصلنا على بعض قيم الخسارة أثناء التدريب، ولكننا في الواقع نريد رؤية مقاييس ROUGE التي قمنا بحسابها مسبقًا. للحصول على تلك المقاييس، سنحتاج إلى توليد المخرجات من النموذج وتحويلها إلى سلاسل نصية. دعنا نبني بعض القوائم من العلامات والتنبؤات لمقارنة مقياس ROUGE (ملاحظة: إذا حصلت على أخطاء استيراد لهذا القسم، فقد تحتاج إلى `!pip install tqdm`). سنستخدم أيضًا خدعة تزيد الأداء بشكل كبير - وهي تجميع كود التوليد الخاص بنا مع [XLA](https://www.tensorflow.org/xla)، وهو المجمع المعجل للجبر الخطي من TensorFlow. يطبق XLA العديد من التحسينات على رسم الحساب للنموذج، مما يؤدي إلى تحسينات كبيرة في السرعة واستخدام الذاكرة. كما هو موضح في مدونة Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate)، يعمل XLA بشكل أفضل عندما لا تختلف أشكال الإدخال لدينا كثيرًا. لمعالجة هذا، سنقوم بملء إدخالاتنا إلى مضاعفات 128، ونصنع مجموعة بيانات جديدة مع جامع الملء، ثم سنطبق الديكور `@tf.function(jit_compile=True)` على دالة التوليد الخاصة بنا، والتي تشير إلى الدالة بأكملها للتجميع مع XLA.

```python
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

بمجرد أن نحصل على قوائمنا من السلاسل النصية للعلامات والتنبؤات، سيكون حساب درجة ROUGE سهلاً:

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```

{/if}

{#if fw === 'pt'}

## ضبط نموذج mT5 الدقيق مع 🤗 Accelerate[[fine-tuning-mt5-with-accelerate]]

ضبط نموذجنا الدقيق مع 🤗 Accelerate مشابه جدًا لمثال تصنيف النصوص الذي واجهناه في [الفصل 3](/course/chapter3). ستكون الاختلافات الرئيسية هي الحاجة إلى توليد ملخصاتنا بشكل صريح أثناء التدريب وتحديد كيفية حساب درجات ROUGE (تذكر أن `Seq2SeqTrainer` تولى عملية التوليد نيابة عنا). دعنا نلقي نظرة على كيفية تنفيذ هذين الشرطين ضمن 🤗 Accelerate!

### إعداد كل شيء للتدريب[[preparing-everything-for-training]]

أول شيء نحتاج إلى فعله هو إنشاء `DataLoader` لكل من أقسام بياناتنا. نظرًا لأن محملات بيانات PyTorch تتوقع دفعات من المصفوفات، نحتاج إلى تعيين التنسيق إلى `"torch"` في مجموعات بياناتنا:

```python
tokenized_datasets.set_format("torch")
```

الآن بعد أن حصلنا على مجموعات بيانات تتكون فقط من المصفوفات، فإن الشيء التالي الذي يجب فعله هو إنشاء مثيل لـ `DataCollatorForSeq2Seq` مرة أخرى. لهذا نحتاج إلى توفير نسخة جديدة من النموذج، لذا دعنا نحمله مرة أخرى من ذاكرة التخزين المؤقت لدينا:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

بعد ذلك، يمكننا إنشاء مثيل لجامع البيانات واستخدامه لتحديد محملات البيانات الخاصة بنا:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

الشيء التالي الذي يجب فعله هو تحديد المحسن الذي نريد استخدامه. كما في الأمثلة الأخرى، سنستخدم `AdamW`، والذي يعمل بشكل جيد لمعظم المشاكل:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

أخيرًا، نقوم بتغذية نموذجنا ومحسننا ومحملات البيانات لدينا إلى طريقة `accelerator.prepare()`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

🚨 إذا كنت تتدرب على TPU، فستحتاج إلى نقل كل الكود أعلاه إلى وظيفة تدريب مخصصة. راجع [الفصل 3](/course/chapter3) لمزيد من التفاصيل.

</Tip>

الآن بعد أن قمنا بإعداد كائناتنا، هناك ثلاثة أشياء متبقية يجب القيام بها:

* تحديد جدول معدل التعلم.
* تنفيذ وظيفة لمعالجة الملخصات بعد التدريب.
* إنشاء مستودع على Hub يمكننا دفع نموذجنا إليه.

بالنسبة لجدول معدل التعلم، سنستخدم الجدول الخطي القياسي من الأقسام السابقة:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

بالنسبة للمعالجة اللاحقة، نحتاج إلى وظيفة تقوم بتقسيم الملخصات المولدة إلى جمل مفصولة بفواصل أسطر. هذا هو التنسيق الذي يتوقعه مقياس ROUGE، ويمكننا تحقيق ذلك باستخدام مقتطف الكود التالي:

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE يتوقع وجود فاصل أسطر بعد كل جملة
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

يجب أن يبدو هذا مألوفًا لك إذا تذكرت كيف قمنا بتعريف دالة `compute_metrics()` لنموذج `Seq2SeqTrainer`.

أخيرًا، نحتاج إلى إنشاء مستودع نموذج على Hugging Face Hub. لهذا، يمكننا استخدام مكتبة 🤗 Hub المناسبة. نحتاج فقط إلى تحديد اسم لمستودعنا، وللمكتبة وظيفة فائدة لدمج معرف المستودع مع ملف تعريف المستخدم:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

الآن يمكننا استخدام اسم المستودع هذا لاستنساخ نسخة محلية إلى دليل النتائج لدينا والذي سيخزن نتائج التدريب:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

هذا سيسمح لنا بإعادة الآثار إلى المركز الرئيسي عن طريق استدعاء طريقة `repo.push_to_hub()` أثناء التدريب! دعنا الآن ننهي تحليلنا عن طريق كتابة حلقة التدريب.

### حلقة التدريب[[training-loop]]

حلقة التدريب للتلخيص مشابهة جدًا لأمثلة 🤗 Accelerate الأخرى التي واجهناها وتنقسم تقريبًا إلى أربع خطوات رئيسية:

1. تدريب النموذج عن طريق التكرار على جميع الأمثلة في `train_dataloader` لكل حقبة.
2. توليد ملخصات النموذج في نهاية كل حقبة، عن طريق توليد الرموز أولاً ثم فك ترميزها (والملخصات المرجعية) إلى نص.
3. حساب درجات ROUGE باستخدام نفس التقنيات التي رأيناها سابقًا.
4. حفظ نقاط التفتيش ودفع كل شيء إلى المركز الرئيسي. هنا نعتمد على حجة `blocking=False` المفيدة لموضوع `Repository` بحيث يمكننا دفع نقاط التفتيش لكل حقبة _بشكل غير متزامن_. يسمح لنا هذا بالاستمرار في التدريب دون الحاجة إلى الانتظار للتحميل البطيء إلى حد ما المرتبط بنموذج بحجم جيجابايت!

يمكن رؤية هذه الخطوات في كتلة الكود التالية:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

وهذا كل شيء! بمجرد تشغيل هذا، سيكون لديك نموذج ونتائج مشابهة جدًا لتلك التي حصلنا عليها مع `Trainer`.

{/if}

## استخدام النموذج المعدل لديك[[using-your-fine-tuned-model]]

بمجرد أن تدفع بالنموذج إلى المركز الرئيسي، يمكنك اللعب به إما عبر أداة الاستدلال أو مع كائن `pipeline`، كما يلي:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

يمكننا إطعام بعض الأمثلة من مجموعة الاختبار (التي لم يراها النموذج) إلى خط الأنابيب الخاص بنا للحصول على فكرة عن جودة الملخصات. دعنا ننفذ وظيفة بسيطة لعرض المراجعة والعنوان والملخص المولد معًا:

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

دعنا نلقي نظرة على أحد الأمثلة الإنجليزية التي نحصل عليها:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn’t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It’s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

هذا ليس سيئًا للغاية! يمكننا أن نرى أن نموذجنا تمكن بالفعل من أداء التلخيص الاستخلاصي عن طريق إضافة أجزاء من المراجعة بكلمات جديدة. ولعل الجانب الأروع في نموذجنا هو أنه ثنائي اللغة، لذلك يمكننا أيضًا توليد ملخصات لمراجعات باللغة الإسبانية:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

يترجم الملخص إلى "سهل القراءة جدًا" باللغة الإنجليزية، والتي يمكننا أن نرى في هذه الحالة تم استخلاصها مباشرة من المراجعة. ومع ذلك، فإن هذا يظهر مرونة نموذج mT5 وقد أعطاك فكرة عن التعامل مع مجموعة بيانات متعددة اللغات!

بعد ذلك، سنوجه انتباهنا إلى مهمة أكثر تعقيدًا قليلاً: تدريب نموذج اللغة من الصفر.