<FrameworkSwitchCourse {fw} />

# تدريب نموذج اللغة السببي من الصفر [[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

حتى الآن، كنا نستخدم بشكل أساسي نماذج مُدربة مسبقًا ونقوم بتعديلها لتناسب حالات الاستخدام الجديدة عن طريق إعادة استخدام الأوزان من التدريب المسبق. كما رأينا في [الفصل 1](/course/chapter1)، يُشار إلى هذا عادةً باسم _التعلم التحويلي_، وهي استراتيجية ناجحة للغاية لتطبيق نماذج المحول على معظم حالات الاستخدام الواقعية حيث تكون البيانات المُوسومة نادرة. في هذا الفصل، سنتخذ نهجًا مختلفًا ونقوم بتدريب نموذج جديد تمامًا من الصفر. هذا نهج جيد إذا كان لديك الكثير من البيانات وهي مختلفة جدًا عن بيانات التدريب المسبق المستخدمة للنماذج المتاحة. ومع ذلك، يتطلب أيضًا موارد حوسبة أكبر بكثير لتدريب نموذج اللغة من مجرد تعديل نموذج موجود. تشمل الأمثلة التي يمكن أن يكون من المنطقي فيها تدريب نموذج جديد مجموعات البيانات المكونة من النوتات الموسيقية، أو التسلسلات الجزيئية مثل الحمض النووي، أو لغات البرمجة. اكتسبت الأخيرة زخمًا مؤخرًا بفضل أدوات مثل TabNine و Copilot من GitHub، والتي تعمل بواسطة نموذج Codex من OpenAI، والتي يمكنها توليد تسلسلات طويلة من التعليمات البرمجية. تتم معالجة مهمة توليد النص هذه بشكل أفضل باستخدام نماذج اللغة التراجعية أو السببية مثل GPT-2.

في هذا القسم، سنقوم ببناء نسخة مصغرة من نموذج توليد التعليمات البرمجية: سنركز على الإكمال في سطر واحد بدلاً من الوظائف أو الفئات الكاملة، باستخدام مجموعة فرعية من شفرة بايثون. عند العمل مع البيانات في بايثون، فأنت على اتصال متكرر بمجموعة بايثون للعلوم، والتي تتكون من مكتبات `matplotlib` و `seaborn` و `pandas` و `scikit-learn`. عند استخدام هذه الأطر، من الشائع أن تحتاج إلى البحث عن أوامر محددة، لذا سيكون من الجيد إذا استطعنا استخدام نموذج لإكمال هذه الاستدعاءات من أجلنا.

<Youtube id="Vpjb1lu0MDk"/>

في [الفصل 6](/course/chapter6)، قمنا بإنشاء معالج كفء لمعالجة شفرة المصدر بايثون، ولكن ما زلنا نحتاج إليه هو مجموعة بيانات واسعة النطاق لتدريب نموذج عليها. هنا، سنطبق معالجنا على مجموعة من شفرة بايثون مستمدة من مستودعات GitHub. بعد ذلك، سنستخدم واجهة برمجة التطبيقات `Trainer` و 🤗 Accelerate لتدريب النموذج. هيا بنا!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

هذا في الواقع يعرض النموذج الذي تم تدريبه وتحميله إلى المركز باستخدام التعليمات البرمجية الموضحة في هذا القسم. يمكنك العثور عليه [هنا](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). لاحظ أنه نظرًا لحدوث بعض العشوائية في توليد النص، فمن المحتمل أن تحصل على نتيجة مختلفة قليلاً.

## جمع البيانات [[gathering-the-data]]

شفرة بايثون متوفرة بكثرة من مستودعات الشفرة مثل GitHub، والتي يمكننا استخدامها لإنشاء مجموعة بيانات عن طريق البحث عن كل مستودع بايثون. كان هذا هو النهج المتبع في [كتاب المحولات](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) لتدريب نموذج GPT-2 كبير. باستخدام تفريغ GitHub بحجم حوالي 180 جيجابايت يحتوي على حوالي 20 مليون ملف بايثون يسمى `codeparrot`، قام المؤلفون ببناء مجموعة بيانات قاموا بعد ذلك بمشاركتها على [Hub Hugging Face](https://huggingface.co/datasets/transformersbook/codeparrot).

ومع ذلك، فإن التدريب على المجموعة الكاملة يستغرق وقتًا ويستهلك الكثير من موارد الحوسبة، ونحن نحتاج فقط إلى جزء من مجموعة البيانات المعنية بمجموعة بايثون للعلوم. لذا، دعنا نبدأ بتصفية مجموعة بيانات `codeparrot` لجميع الملفات التي تتضمن أيًا من المكتبات في هذه المجموعة. بسبب حجم مجموعة البيانات، نريد تجنب تنزيلها؛ بدلاً من ذلك، سنستخدم ميزة البث المباشر لتصفية المجموعة أثناء التنقل. لمساعدتنا في تصفية عينات الشفرة باستخدام المكتبات التي ذكرناها سابقًا، سنستخدم الدالة التالية:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

دعنا نختبرها على مثالين:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

يمكننا استخدام هذا لإنشاء دالة ستقوم ببث مجموعة البيانات وتصفية العناصر التي نريدها:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

بعد ذلك، يمكننا ببساطة تطبيق هذه الدالة على مجموعة البيانات البث:

```py
# هذه الخلية ستستغرق وقتًا طويلاً للتنفيذ، لذا يجب عليك تخطيها والانتقال إلى
# التالي!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

هذا يترك لنا حوالي 3% من مجموعة البيانات الأصلية، والتي لا تزال كبيرة الحجم - تتكون مجموعة البيانات الناتجة من 600,000 نص برمجي بايثون بحجم 6 جيجابايت!

يمكن أن يستغرق تصفية مجموعة البيانات الكاملة 2-3 ساعات حسب جهازك وعرض النطاق الترددي. إذا كنت لا ترغب في المرور بهذه العملية الطويلة بنفسك، فإننا نوفر مجموعة البيانات المصفاة على المركز لتتمكن من تنزيلها:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```
<Tip>

سيستغرق التدريب المسبق لنموذج اللغة بعض الوقت. نقترح أن تقوم بتشغيل حلقة التدريب على عينة من البيانات عن طريق إلغاء تعليق السطرين الجزئيين أعلاه، والتأكد من اكتمال التدريب بنجاح وتخزين النماذج. لا يوجد شيء أكثر إحباطًا من فشل عملية التدريب في الخطوة الأخيرة لأنك نسيت إنشاء مجلد أو بسبب وجود خطأ مطبعي في نهاية حلقة التدريب!

</Tip>

دعنا نلقي نظرة على مثال من مجموعة البيانات. سنعرض فقط أول 200 حرف من كل حقل:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

يمكننا أن نرى أن حقل `content` يحتوي على الكود الذي نريد أن يتدرب عليه النموذج. الآن بعد أن لدينا مجموعة بيانات، نحتاج إلى إعداد النصوص بحيث تكون في تنسيق مناسب للتدريب المسبق.

## إعداد مجموعة البيانات[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

ستكون الخطوة الأولى هي تقسيم البيانات إلى رموز، بحيث يمكننا استخدامها للتدريب. بما أن هدفنا هو إكمال وظائف الاستدعاء القصيرة بشكل أساسي، يمكننا الحفاظ على حجم السياق صغيرًا نسبيًا. هذه الميزة تسمح لنا بتدريب النموذج بشكل أسرع بكثير وتتطلب ذاكرة أقل بكثير. إذا كان من المهم لتطبيقك أن يكون لديك سياق أكبر (على سبيل المثال، إذا كنت تريد أن يقوم النموذج بكتابة اختبارات الوحدة بناءً على ملف بتعريف الوظيفة)، تأكد من زيادة ذلك الرقم، ولكن ضع في اعتبارك أيضًا أن هذا يأتي مع مساحة ذاكرة GPU أكبر. دعنا الآن نثبت حجم السياق عند 128 رمز، على عكس 1,024 أو 2,048 المستخدمة في GPT-2 أو GPT-3، على التوالي.

تحتوي معظم المستندات على أكثر من 128 رمز، لذا فإن مجرد تقصير المدخلات إلى الطول الأقصى سيقضي على جزء كبير من مجموعة بياناتنا. بدلاً من ذلك، سنستخدم خيار `return_overflowing_tokens` لتقسيم الإدخال بالكامل إلى عدة أجزاء، كما فعلنا في [الفصل 6](/course/chapter6/4). سنستخدم أيضًا خيار `return_length` لإرجاع طول كل جزء تم إنشاؤه تلقائيًا. غالبًا ما يكون الجزء الأخير أصغر من حجم السياق، وسنتخلص من هذه القطع لتجنب مشكلات الحشو؛ نحن لا نحتاجها حقًا لأن لدينا الكثير من البيانات على أي حال.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="تقسيم نص كبير إلى عدة أجزاء."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="تقسيم نص كبير إلى عدة أجزاء."/>
</div>

دعنا نرى بالضبط كيف يعمل هذا عن طريق النظر إلى أول مثالين:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

يمكننا أن نرى أننا نحصل على 34 مقطعًا إجماليًا من هذين المثالين. بالنظر إلى أطوال الأجزاء، يمكننا أن نرى أن الأجزاء في نهايات كل من المستندين تحتوي على أقل من 128 رمز (117 و41، على التوالي). تمثل هذه الأجزاء مجرد جزء صغير من إجمالي الأجزاء التي لدينا، لذا يمكننا التخلص منها بأمان. باستخدام حقل `overflow_to_sample_mapping`، يمكننا أيضًا إعادة بناء الأجزاء التي تنتمي إلى عينات الإدخال.

مع هذه العملية، نستخدم ميزة مفيدة لوظيفة `Dataset.map()` في 🤗 Datasets، والتي لا تتطلب خرائط واحد إلى واحد؛ كما رأينا في [القسم 3](/course/chapter7/3)، يمكننا إنشاء دفعات تحتوي على عدد أكبر أو أقل من العناصر من الدفعة المدخلة. هذا مفيد عند إجراء عمليات مثل زيادة البيانات أو تصفية البيانات التي تغير عدد العناصر. في حالتنا، عند تقسيم كل عنصر إلى أجزاء من حجم السياق المحدد، نقوم بإنشاء العديد من العينات من كل مستند. نحن بحاجة فقط للتأكد من حذف الأعمدة الموجودة، لأنها ذات حجم متضارب. إذا أردنا الاحتفاظ بها، يمكننا تكرارها بشكل مناسب وإرجاعها ضمن مكالمة `Dataset.map()`:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

لدينا الآن 16.7 مليون مثال يحتوي كل منها على 128 رمز، وهو ما يقابل حوالي 2.1 مليار رمز في المجموع. للرجوع، تم تدريب نماذج OpenAI's GPT-3 وCodex على 300 و100 مليار رمز، على التوالي، حيث تم تهيئة نماذج Codex من نقاط تفتيش GPT-3. هدفنا في هذا القسم ليس التنافس مع هذه النماذج، والتي يمكنها توليد نصوص متماسكة طويلة، ولكن لإنشاء نسخة مصغرة توفر وظيفة إكمال تلقائي سريعة لعلماء البيانات.

الآن بعد أن أصبحت مجموعة البيانات جاهزة، دعنا نعد النموذج!

<Tip>

✏️ **جربه!** لم يكن التخلص من جميع الأجزاء التي تكون أصغر من حجم السياق مشكلة كبيرة هنا لأننا نستخدم نوافذ سياق صغيرة. كلما زاد حجم السياق (أو إذا كان لديك مجموعة من المستندات القصيرة)، سينمو أيضًا الجزء من الأجزاء التي يتم التخلص منها. هناك طريقة أكثر كفاءة لإعداد البيانات وهي دمج جميع العينات المعلمة في دفعة مع رمز `eos_token_id` بينها، ثم إجراء التقسيم على التسلسلات المدمجة. كتمرين، عدل وظيفة `tokenize()` لاستخدام هذا النهج. لاحظ أنك ستريد تعيين `truncation=False` وإزالة الحجج الأخرى من المعلم لكي تحصل على التسلسل الكامل لأرقام الرموز.

</Tip>


## تهيئة نموذج جديد[[initializing-a-new-model]]

ستكون خطوتنا الأولى هي تهيئة نموذج GPT-2 جديد. سنستخدم نفس التهيئة لنموذجنا كما هو الحال في نموذج GPT-2 الصغير، لذا نقوم بتحميل التهيئة المسبقة، والتأكد من أن حجم المعلم يطابق حجم مفردات النموذج وتمرير رموز `bos` و`eos` (بداية ونهاية التسلسل) IDs:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

مع هذه التهيئة، يمكننا تحميل نموذج جديد. لاحظ أن هذه هي المرة الأولى التي لا نستخدم فيها وظيفة `from_pretrained()`، لأننا في الواقع نهيئ نموذجًا بأنفسنا:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

مع هذا التكوين، يمكننا تحميل نموذج جديد. لاحظ أن هذه هي المرة الأولى التي لا نستخدم فيها دالة `from_pretrained()`، لأننا في الواقع نقوم بتهيئة نموذج بأنفسنا:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # بناء النموذج
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

لدى نموذجنا 124M معاملات سنقوم بضبطها. قبل أن نبدأ التدريب، نحتاج إلى إعداد جامع بيانات يقوم بإنشاء الدفعات. يمكننا استخدام جامع بيانات `DataCollatorForLanguageModeling`، والذي تم تصميمه خصيصًا للنمذجة اللغوية (كما يوحي الاسم بشكل خفي). بالإضافة إلى تكديس وتوسيد الدفعات، فإنه يقوم أيضًا بإنشاء تسميات النموذج اللغوي - في النمذجة اللغوية السببية تعمل المدخلات كعلامات أيضًا (متحولة فقط بعنصر واحد)، ويقوم جامع البيانات هذا بإنشائها أثناء التدريب حتى لا نحتاج إلى تكرار `input_ids`.

لاحظ أن `DataCollatorForLanguageModeling` يدعم كل من النمذجة اللغوية المقنعة (MLM) والنمذجة اللغوية السببية (CLM). بشكل افتراضي، يقوم بإعداد البيانات لـ MLM، ولكن يمكننا التبديل إلى CLM عن طريق تعيين وسيط `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

دعونا نلقي نظرة على مثال:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

يمكننا أن نرى أن الأمثلة تم تكديسها وأن جميع المصفوفات لها نفس الشكل.

{#if fw === 'tf'}

الآن يمكننا استخدام طريقة `prepare_tf_dataset()` لتحويل مجموعات البيانات الخاصة بنا إلى مجموعات بيانات TensorFlow باستخدام جامع البيانات الذي أنشأناه أعلاه:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

⚠️ يحدث تحويل المدخلات والعلامات لمواءمتها داخل النموذج، لذلك يقوم جامع البيانات بنسخ المدخلات لإنشاء العلامات.

</Tip>


الآن لدينا كل شيء في مكانه لتدريب نموذجنا بالفعل - لم يكن الأمر بهذا القدر من العمل بعد كل شيء! قبل أن نبدأ التدريب، يجب أن نقوم بتسجيل الدخول إلى Hugging Face. إذا كنت تعمل في دفتر ملاحظات، فيمكنك القيام بذلك باستخدام دالة المساعدة التالية:

```python
from huggingface_hub import notebook_login

notebook_login()
```

سيتم عرض عنصر واجهة مستخدم حيث يمكنك إدخال بيانات اعتماد تسجيل دخول Hugging Face الخاصة بك.

إذا لم تكن تعمل في دفتر ملاحظات، فما عليك سوى كتابة السطر التالي في المحطة الطرفية الخاصة بك:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

كل ما تبقى هو تكوين حجج التدريب وتشغيل `Trainer`. سنستخدم جدول معدل تعلم كوني مع بعض التسخين وحجم دفعة فعال يبلغ 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). يتم استخدام تراكم التدرج عندما لا تتناسب دفعة واحدة مع الذاكرة، ويبني تدريجيًا التدرج من خلال العديد من عمليات التمرير للأمام والخلف. سنرى هذا في العمل عندما ننشئ حلقة التدريب مع 🤗 Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

الآن يمكننا ببساطة تشغيل `Trainer` وانتظار اكتمال التدريب. اعتمادًا على ما إذا كنت تشغله على مجموعة التدريب الكاملة أو جزء منها، سيستغرق ذلك 20 أو ساعتين على التوالي، لذا احصل على بعض القهوة وكتاب جيد للقراءة!

```py
trainer.train()
```

بعد اكتمال التدريب، يمكننا دفع النموذج ومحلل الرموز إلى المركز:

```py
trainer.push_to_hub()
```

{:else}

كل ما تبقى هو تكوين معلمات التدريب واستدعاء `compile()` و `fit()`. سنستخدم جدول معدل تعلم مع بعض التسخين لتحسين استقرار التدريب:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# تدريب في دقة النقطة العائمة المختلطة
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

الآن يمكننا ببساطة استدعاء `model.fit()` وانتظار اكتمال التدريب. اعتمادًا على ما إذا كنت تشغله على مجموعة التدريب الكاملة أو جزء منها، سيستغرق ذلك 20 أو ساعتين على التوالي، لذا احصل على بعض القهوة وكتاب جيد للقراءة! بعد اكتمال التدريب، يمكننا دفع النموذج ومحلل الرموز إلى المركز:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

✏️ **جربها!** لقد استغرقنا فقط حوالي 30 سطرا من التعليمات البرمجية بالإضافة إلى `TrainingArguments` للانتقال من النصوص الخام إلى تدريب GPT-2. جربها مع مجموعة بياناتك الخاصة وشاهد إذا كنت تستطيع الحصول على نتائج جيدة!

</Tip>

<Tip>

{#if fw === 'pt'}

💡 إذا كان لديك إمكانية الوصول إلى جهاز به وحدات معالجة رسومية متعددة، حاول تشغيل التعليمات البرمجية هناك. يقوم `Trainer` بإدارة الأجهزة المتعددة تلقائيًا، ويمكن أن يسرع التدريب بشكل كبير.

{:else}

💡 إذا كان لديك إمكانية الوصول إلى جهاز به وحدات معالجة رسومية متعددة، يمكنك تجربة استخدام سياق `MirroredStrategy` لتسريع التدريب بشكل كبير. ستحتاج إلى إنشاء كائن `tf.distribute.MirroredStrategy`، والتأكد من أن أي أساليب `to_tf_dataset()` أو `prepare_tf_dataset()` بالإضافة إلى إنشاء النموذج والمكالمة إلى `fit()` يتم تشغيلها جميعًا في سياق `scope()` الخاص بها. يمكنك الاطلاع على الوثائق المتعلقة بهذا الأمر [هنا](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## توليد الكود باستخدام خط أنابيب [[code-generation-with-a-pipeline]]

الآن هي لحظة الحقيقة: دعنا نرى مدى جودة عمل النموذج المدرب بالفعل! يمكننا أن نرى في السجلات أن الخسارة انخفضت بثبات، ولكن لوضع النموذج تحت الاختبار دعنا نلقي نظرة على مدى جودة عمله على بعض المطالبات. للقيام بذلك، سنقوم بتغليف النموذج في خط أنابيب توليد النص، وسنضعه على وحدة معالجة الرسومات للعمليات السريعة إذا كان هناك واحد متاح:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

دعنا نبدأ بالمهمة البسيطة المتمثلة في إنشاء مخطط متفرق:

```py
txt = """\
# إنشاء بعض البيانات
x = np.random.randn(100)
y = np.random.randn(100)

# إنشاء مخطط متفرق باستخدام x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# إنشاء بعض البيانات
x = np.random.randn(100)
y = np.random.randn(100)

# إنشاء مخطط متفرق باستخدام x, y
plt.scatter(x, y)

# إنشاء مخطط متفرق
```

تبدو النتيجة صحيحة. هل يعمل أيضًا لعملية pandas؟ دعنا نرى إذا كان بإمكاننا إنشاء DataFrame من مصفوفتين:

```py
txt = """\
# إنشاء بعض البيانات
x = np.random.randn(100)
y = np.random.randn(100)

# إنشاء DataFrame من x و y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# إنشاء بعض البيانات
x = np.random.randn(100)
y = np.random.randn(100)

# إنشاء DataFrame من x و y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

جميل، هذا هو الجواب الصحيح - على الرغم من أنه بعد ذلك يقوم بإدراج العمود "x" مرة أخرى. نظرًا لأن عدد الرموز المولدة محدود، يتم قطع حلقة "for" التالية. دعنا نرى إذا كان بإمكاننا القيام بشيء أكثر تعقيدًا وجعل النموذج يساعدنا في استخدام عملية groupby:

```py
txt = """\
# dataframe مع المهنة والدخل والاسم
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# حساب متوسط الدخل لكل مهنة
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe مع المهنة والدخل والاسم
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# حساب متوسط الدخل لكل مهنة
profession = df.groupby(['profession']).mean()

# حساب
```

ليس سيئًا؛ هذه هي الطريقة الصحيحة للقيام بذلك. أخيرًا، دعنا نرى إذا كان بإمكاننا استخدامه أيضًا لـ scikit-learn وإعداد نموذج Random Forest:

```py
txt = """
# استيراد random forest regressor من scikit-learn
from sklearn.ensemble import RandomForestRegressor

# تناسب نموذج random forest مع 300 من المقدرين على X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# استيراد random forest regressor من scikit-learn
from sklearn.ensemble import RandomForestRegressor

# تناسب نموذج random forest مع 300 من المقدرين على X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

عند النظر إلى هذه الأمثلة القليلة، يبدو أن النموذج قد تعلم بعض بناء جملة مجموعة أدوات علوم البيانات في بايثون. بالطبع، سنحتاج إلى تقييم النموذج بشكل أكثر شمولية قبل نشره في العالم الحقيقي، ولكن هذا لا يزال نموذجًا أوليًا رائعًا.

{:else}

عند النظر إلى هذه الأمثلة القليلة، يبدو أن النموذج قد تعلم بعض بناء جملة مجموعة أدوات علوم البيانات في بايثون (بالطبع، سنحتاج إلى تقييمه بشكل أكثر شمولية قبل نشر النموذج في العالم الحقيقي). في بعض الأحيان، يتطلب الأمر المزيد من التخصيص لتدريب النموذج لتحقيق الأداء الضروري لحالة استخدام معينة، ومع ذلك. على سبيل المثال، ماذا لو أردنا تحديث حجم الدفعة ديناميكيًا أو وجود حلقة تدريب شرطية تخطي الأمثلة السيئة أثناء التنقل؟ أحد الخيارات هو إنشاء فئة فرعية من `Trainer` وإضافة التغييرات الضرورية، ولكن في بعض الأحيان يكون من الأبسط كتابة حلقة التدريب من الصفر. هنا يأتي دور 🤗 Accelerate.

{/if}

{#if fw === 'pt'}

## التدريب مع 🤗 Accelerate[[training-with-accelerate]]

لقد رأينا كيفية تدريب نموذج باستخدام `Trainer`، والذي يمكن أن يسمح ببعض التخصيص. ومع ذلك، في بعض الأحيان نريد التحكم الكامل في حلقة التدريب، أو نريد إجراء بعض التغييرات الغريبة. في هذه الحالة، يعد 🤗 Accelerate خيارًا رائعًا، وفي هذا القسم سنمر عبر الخطوات لاستخدامه لتدريب نموذجنا. لجعل الأمور أكثر إثارة للاهتمام، سنضيف أيضًا تحولًا إلى حلقة التدريب.

<Youtube id="Hm8_PgVTFuc"/>

نظرًا لأننا مهتمون بشكل أساسي بالاستكمال التلقائي المنطقي لمكتبات علوم البيانات، فمن المنطقي إعطاء المزيد من الوزن لعينات التدريب التي تستخدم هذه المكتبات بشكل أكبر. يمكننا تحديد هذه الأمثلة بسهولة من خلال استخدام كلمات رئيسية مثل `plt`، و`pd`، و`sk`، و`fit`، و`predict`، والتي تعد أكثر أسماء الاستيراد تكرارًا لـ `matplotlib.pyplot`، و`pandas`، و`sklearn` بالإضافة إلى نمط fit/predict للأخير. إذا تم تمثيل كل منها كرموز مفردة، فيمكننا التحقق بسهولة مما إذا كانت تحدث في تسلسل الإدخال. يمكن أن يكون للرموز بادئة مسافة بيضاء، لذا سنتحقق أيضًا من هذه الإصدارات في مفردات المحلل اللغوي. للتحقق من أنها تعمل، سنضيف رمز اختبار واحد يجب تقسيمه إلى رموز متعددة:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

رائع، يبدو أنه يعمل بشكل جيد! يمكننا الآن كتابة دالة خسارة مخصصة تأخذ تسلسل الإدخال، واللوجيتس، ورموز المفاتيح التي قمنا باختيارها للتو كمدخلات. أولاً نحتاج إلى محاذاة اللوجيتس والمدخلات: يتم تحويل تسلسل الإدخال إلى اليمين لتشكيل التصنيفات، حيث أن الرمز التالي هو التصنيف للرمز الحالي. يمكننا تحقيق ذلك عن طريق البدء بالتصنيفات من الرمز الثاني لتسلسل الإدخال، حيث أن النموذج لا يقوم بتنبؤ للرمز الأول على أي حال. ثم نقوم بقطع اللوجيت الأخير، حيث لا يوجد لدينا تصنيف للرمز الذي يتبع تسلسل الإدخال الكامل. بهذا يمكننا حساب الخسارة لكل عينة وحساب تكرار جميع الكلمات الرئيسية في كل عينة. أخيراً، نقوم بحساب المتوسط المرجح لجميع العينات باستخدام التكرارات كأوزان. حيث أننا لا نريد التخلص من جميع العينات التي لا تحتوي على كلمات رئيسية، نضيف 1 إلى الأوزان:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # تحويل الرموز بحيث أن الرموز < n تتنبأ بـ n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # حساب الخسارة لكل رمز
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # تغيير حجم ومتوسط الخسارة لكل عينة
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # حساب وتصغير الأوزان
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # حساب المتوسط المرجح
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

قبل أن نبدأ التدريب باستخدام دالة الخسارة الجديدة الرائعة هذه، نحتاج إلى إعداد بعض الأشياء:

- نحتاج إلى محملات البيانات لتحميل البيانات في مجموعات.
- نحتاج إلى ضبط معلمات انخفاض الوزن.
- من وقت لآخر نريد التقييم، لذلك من المنطقي أن نغلف كود التقييم في دالة.

دعنا نبدأ بمحملات البيانات. نحتاج فقط إلى ضبط تنسيق مجموعة البيانات إلى `"torch"`، ثم يمكننا تمريرها إلى `DataLoader` في PyTorch مع حجم المجموعة المناسب:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

بعد ذلك، نقوم بتجزئة المعلمات بحيث يعرف المحسن أي منها سيحصل على انخفاض إضافي في الوزن. عادة، يتم إعفاء جميع مصطلحات الانحياز ووزن LayerNorm من هذا؛ إليك كيف يمكننا القيام بذلك:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

حيث أننا نريد تقييم النموذج بانتظام على مجموعة التحقق أثناء التدريب، دعنا نكتب دالة لذلك أيضاً. تقوم فقط بتشغيل محمل بيانات التقييم وجمع جميع الخسائر عبر العمليات:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

باستخدام دالة `evaluate()` يمكننا الإبلاغ عن الخسارة و[perplexity](/course/chapter7/3) على فترات منتظمة. بعد ذلك، نقوم بإعادة تعريف نموذجنا للتأكد من أننا نتدرب من الصفر مرة أخرى:

```py
model = GPT2LMHeadModel(config)
```

بعد ذلك، يمكننا تعريف المحسن، باستخدام الدالة من قبل لتقسيم المعلمات لانخفاض الوزن:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

الآن دعنا نعد النموذج، والمحسن، ومحملات البيانات بحيث يمكننا البدء بالتدريب:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

🚨 إذا كنت تتدرب على TPU، فستحتاج إلى نقل كل الكود بدءاً من الخلية أعلاه إلى دالة تدريب مخصصة. راجع [الفصل 3](/course/chapter3) لمزيد من التفاصيل.

</Tip>

الآن بعد أن أرسلنا `train_dataloader` إلى `accelerator.prepare()`، يمكننا استخدام طوله لحساب عدد خطوات التدريب. تذكر أنه يجب علينا دائماً القيام بذلك بعد إعداد محمل البيانات، حيث أن هذه الطريقة ستغير طوله. نستخدم جدولًا خطيًا كلاسيكيًا من معدل التعلم إلى 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

أخيراً، لدفع نموذجنا إلى Hub، سنحتاج إلى إنشاء كائن `Repository` في مجلد عمل. قم بتسجيل الدخول إلى Hub Hugging Face، إذا لم تكن قد سجلت الدخول بالفعل. سنحدد اسم المستودع من معرف النموذج الذي نريد إعطاءه لنموذجنا (لا تتردد في استبدال `repo_name` بخيارك الخاص؛ فهو يحتاج فقط إلى احتواء اسم المستخدم الخاص بك، وهو ما تقوم به دالة `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

بعد ذلك، يمكننا استنساخ هذا المستودع في مجلد محلي. إذا كان موجودًا بالفعل، فيجب أن يكون هذا المجلد المحلي مستنسخًا موجودًا للمستودع الذي نعمل معه:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

الآن يمكننا تحميل أي شيء نقوم بحفظه في `output_dir` عن طريق استدعاء طريقة `repo.push_to_hub()`. سيساعدنا هذا في تحميل النماذج الوسيطة في نهاية كل حقبة.

قبل أن نتدرب، دعنا نجري اختبارًا سريعًا لنرى إذا كانت دالة التقييم تعمل بشكل صحيح:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

هذه قيم مرتفعة للغاية للخسارة والارتباك، ولكن ليس من المستغرب أننا لم نقم بتدريب النموذج بعد. بهذا، لدينا كل شيء جاهز لكتابة الجزء الأساسي من سيناريو التدريب: حلقة التدريب. في حلقة التدريب، نكرر العملية على مجموعة البيانات ونمرر المجموعات إلى النموذج. باستخدام القيم اللوجستية، يمكننا بعد ذلك تقييم دالة الخسارة المخصصة لدينا. نقوم بضبط الخسارة حسب عدد خطوات تراكم التدرج حتى لا نخلق خسائر أكبر عند تجميع المزيد من الخطوات. قبل التحسين، نقوم أيضًا بقص التدرجات من أجل تقارب أفضل. وأخيرًا، كل بضع خطوات نقوم بتقييم النموذج على مجموعة التقييم باستخدام دالة `evaluate()` الجديدة لدينا:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

وهذا هو -- لديك الآن حلقة التدريب المخصصة الخاصة بك للنماذج اللغوية السببية مثل GPT-2 والتي يمكنك تخصيصها أكثر حسب احتياجاتك.

<Tip>

✏️ **جربها!** إما أن تقوم بإنشاء دالة خسارة مخصصة تناسب حالتك الاستخدامية، أو أضف خطوة مخصصة أخرى إلى حلقة التدريب.

</Tip>

<Tip>

✏️ **جربها!** عند إجراء تجارب تدريب طويلة، من الجيد تسجيل المقاييس المهمة باستخدام أدوات مثل TensorBoard أو Weights & Biases. أضف تسجيلًا صحيحًا إلى حلقة التدريب حتى يمكنك دائمًا التحقق من كيفية سير التدريب.

</Tip>

{/if}