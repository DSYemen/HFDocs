<FrameworkSwitchCourse {fw} />

# ุชุฏุฑูุจ ูููุฐุฌ ุงููุบุฉ ุงูุณุจุจู ูู ุงูุตูุฑ [[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

ุญุชู ุงูุขูุ ููุง ูุณุชุฎุฏู ุจุดูู ุฃุณุงุณู ููุงุฐุฌ ููุฏุฑุจุฉ ูุณุจููุง ููููู ุจุชุนุฏูููุง ูุชูุงุณุจ ุญุงูุงุช ุงูุงุณุชุฎุฏุงู ุงูุฌุฏูุฏุฉ ุนู ุทุฑูู ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ุงูุฃูุฒุงู ูู ุงูุชุฏุฑูุจ ุงููุณุจู. ููุง ุฑุฃููุง ูู [ุงููุตู 1](/course/chapter1)ุ ููุดุงุฑ ุฅูู ูุฐุง ุนุงุฏุฉู ุจุงุณู _ุงูุชุนูู ุงูุชุญูููู_ุ ููู ุงุณุชุฑุงุชูุฌูุฉ ูุงุฌุญุฉ ููุบุงูุฉ ูุชุทุจูู ููุงุฐุฌ ุงููุญูู ุนูู ูุนุธู ุญุงูุงุช ุงูุงุณุชุฎุฏุงู ุงููุงูุนูุฉ ุญูุซ ุชููู ุงูุจูุงูุงุช ุงููููุณููุฉ ูุงุฏุฑุฉ. ูู ูุฐุง ุงููุตูุ ุณูุชุฎุฐ ููุฌูุง ูุฎุชูููุง ููููู ุจุชุฏุฑูุจ ูููุฐุฌ ุฌุฏูุฏ ุชูุงููุง ูู ุงูุตูุฑ. ูุฐุง ููุฌ ุฌูุฏ ุฅุฐุง ูุงู ูุฏูู ุงููุซูุฑ ูู ุงูุจูุงูุงุช ููู ูุฎุชููุฉ ุฌุฏูุง ุนู ุจูุงูุงุช ุงูุชุฏุฑูุจ ุงููุณุจู ุงููุณุชุฎุฏูุฉ ููููุงุฐุฌ ุงููุชุงุญุฉ. ููุน ุฐููุ ูุชุทูุจ ุฃูุถูุง ููุงุฑุฏ ุญูุณุจุฉ ุฃูุจุฑ ุจูุซูุฑ ูุชุฏุฑูุจ ูููุฐุฌ ุงููุบุฉ ูู ูุฌุฑุฏ ุชุนุฏูู ูููุฐุฌ ููุฌูุฏ. ุชุดูู ุงูุฃูุซูุฉ ุงูุชู ูููู ุฃู ูููู ูู ุงูููุทูู ูููุง ุชุฏุฑูุจ ูููุฐุฌ ุฌุฏูุฏ ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูููููุฉ ูู ุงูููุชุงุช ุงูููุณูููุฉุ ุฃู ุงูุชุณูุณูุงุช ุงูุฌุฒูุฆูุฉ ูุซู ุงูุญูุถ ุงูููููุ ุฃู ูุบุงุช ุงูุจุฑูุฌุฉ. ุงูุชุณุจุช ุงูุฃุฎูุฑุฉ ุฒุฎููุง ูุคุฎุฑูุง ุจูุถู ุฃุฏูุงุช ูุซู TabNine ู Copilot ูู GitHubุ ูุงูุชู ุชุนูู ุจูุงุณุทุฉ ูููุฐุฌ Codex ูู OpenAIุ ูุงูุชู ูููููุง ุชูููุฏ ุชุณูุณูุงุช ุทูููุฉ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ. ุชุชู ูุนุงูุฌุฉ ูููุฉ ุชูููุฏ ุงููุต ูุฐู ุจุดูู ุฃูุถู ุจุงุณุชุฎุฏุงู ููุงุฐุฌ ุงููุบุฉ ุงูุชุฑุงุฌุนูุฉ ุฃู ุงูุณุจุจูุฉ ูุซู GPT-2.

ูู ูุฐุง ุงููุณูุ ุณูููู ุจุจูุงุก ูุณุฎุฉ ูุตุบุฑุฉ ูู ูููุฐุฌ ุชูููุฏ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ: ุณูุฑูุฒ ุนูู ุงูุฅููุงู ูู ุณุทุฑ ูุงุญุฏ ุจุฏูุงู ูู ุงููุธุงุฆู ุฃู ุงููุฆุงุช ุงููุงููุฉุ ุจุงุณุชุฎุฏุงู ูุฌููุนุฉ ูุฑุนูุฉ ูู ุดูุฑุฉ ุจุงูุซูู. ุนูุฏ ุงูุนูู ูุน ุงูุจูุงูุงุช ูู ุจุงูุซููุ ูุฃูุช ุนูู ุงุชุตุงู ูุชูุฑุฑ ุจูุฌููุนุฉ ุจุงูุซูู ููุนูููุ ูุงูุชู ุชุชููู ูู ููุชุจุงุช `matplotlib` ู `seaborn` ู `pandas` ู `scikit-learn`. ุนูุฏ ุงุณุชุฎุฏุงู ูุฐู ุงูุฃุทุฑุ ูู ุงูุดุงุฆุน ุฃู ุชุญุชุงุฌ ุฅูู ุงูุจุญุซ ุนู ุฃูุงูุฑ ูุญุฏุฏุฉุ ูุฐุง ุณูููู ูู ุงูุฌูุฏ ุฅุฐุง ุงุณุชุทุนูุง ุงุณุชุฎุฏุงู ูููุฐุฌ ูุฅููุงู ูุฐู ุงูุงุณุชุฏุนุงุกุงุช ูู ุฃุฌููุง.

<Youtube id="Vpjb1lu0MDk"/>

ูู [ุงููุตู 6](/course/chapter6)ุ ูููุง ุจุฅูุดุงุก ูุนุงูุฌ ููุก ููุนุงูุฌุฉ ุดูุฑุฉ ุงููุตุฏุฑ ุจุงูุซููุ ูููู ูุง ุฒููุง ูุญุชุงุฌ ุฅููู ูู ูุฌููุนุฉ ุจูุงูุงุช ูุงุณุนุฉ ุงููุทุงู ูุชุฏุฑูุจ ูููุฐุฌ ุนูููุง. ููุงุ ุณูุทุจู ูุนุงูุฌูุง ุนูู ูุฌููุนุฉ ูู ุดูุฑุฉ ุจุงูุซูู ูุณุชูุฏุฉ ูู ูุณุชูุฏุนุงุช GitHub. ุจุนุฏ ุฐููุ ุณูุณุชุฎุฏู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `Trainer` ู ๐ค Accelerate ูุชุฏุฑูุจ ุงููููุฐุฌ. ููุง ุจูุง!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ูุฐุง ูู ุงููุงูุน ูุนุฑุถ ุงููููุฐุฌ ุงูุฐู ุชู ุชุฏุฑูุจู ูุชุญูููู ุฅูู ุงููุฑูุฒ ุจุงุณุชุฎุฏุงู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุงูููุถุญุฉ ูู ูุฐุง ุงููุณู. ููููู ุงูุนุซูุฑ ุนููู [ููุง](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). ูุงุญุธ ุฃูู ูุธุฑูุง ูุญุฏูุซ ุจุนุถ ุงูุนุดูุงุฆูุฉ ูู ุชูููุฏ ุงููุตุ ููู ุงููุญุชูู ุฃู ุชุญุตู ุนูู ูุชูุฌุฉ ูุฎุชููุฉ ููููุงู.

## ุฌูุน ุงูุจูุงูุงุช [[gathering-the-data]]

ุดูุฑุฉ ุจุงูุซูู ูุชููุฑุฉ ุจูุซุฑุฉ ูู ูุณุชูุฏุนุงุช ุงูุดูุฑุฉ ูุซู GitHubุ ูุงูุชู ูููููุง ุงุณุชุฎุฏุงููุง ูุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ุนู ุทุฑูู ุงูุจุญุซ ุนู ูู ูุณุชูุฏุน ุจุงูุซูู. ูุงู ูุฐุง ูู ุงูููุฌ ุงููุชุจุน ูู [ูุชุงุจ ุงููุญููุงุช](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) ูุชุฏุฑูุจ ูููุฐุฌ GPT-2 ูุจูุฑ. ุจุงุณุชุฎุฏุงู ุชูุฑูุบ GitHub ุจุญุฌู ุญูุงูู 180 ุฌูุฌุงุจุงูุช ูุญุชูู ุนูู ุญูุงูู 20 ููููู ููู ุจุงูุซูู ูุณูู `codeparrot`ุ ูุงู ุงููุคูููู ุจุจูุงุก ูุฌููุนุฉ ุจูุงูุงุช ูุงููุง ุจุนุฏ ุฐูู ุจูุดุงุฑูุชูุง ุนูู [Hub Hugging Face](https://huggingface.co/datasets/transformersbook/codeparrot).

ููุน ุฐููุ ูุฅู ุงูุชุฏุฑูุจ ุนูู ุงููุฌููุนุฉ ุงููุงููุฉ ูุณุชุบุฑู ููุชูุง ููุณุชููู ุงููุซูุฑ ูู ููุงุฑุฏ ุงูุญูุณุจุฉุ ููุญู ูุญุชุงุฌ ููุท ุฅูู ุฌุฒุก ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุนููุฉ ุจูุฌููุนุฉ ุจุงูุซูู ููุนููู. ูุฐุงุ ุฏุนูุง ูุจุฏุฃ ุจุชุตููุฉ ูุฌููุนุฉ ุจูุงูุงุช `codeparrot` ูุฌููุน ุงููููุงุช ุงูุชู ุชุชุถูู ุฃููุง ูู ุงูููุชุจุงุช ูู ูุฐู ุงููุฌููุนุฉ. ุจุณุจุจ ุญุฌู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุฑูุฏ ุชุฌูุจ ุชูุฒูููุงุ ุจุฏูุงู ูู ุฐููุ ุณูุณุชุฎุฏู ููุฒุฉ ุงูุจุซ ุงููุจุงุดุฑ ูุชุตููุฉ ุงููุฌููุนุฉ ุฃุซูุงุก ุงูุชููู. ููุณุงุนุฏุชูุง ูู ุชุตููุฉ ุนููุงุช ุงูุดูุฑุฉ ุจุงุณุชุฎุฏุงู ุงูููุชุจุงุช ุงูุชู ุฐูุฑูุงูุง ุณุงุจููุงุ ุณูุณุชุฎุฏู ุงูุฏุงูุฉ ุงูุชุงููุฉ:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

ุฏุนูุง ูุฎุชุจุฑูุง ุนูู ูุซุงููู:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

ูููููุง ุงุณุชุฎุฏุงู ูุฐุง ูุฅูุดุงุก ุฏุงูุฉ ุณุชููู ุจุจุซ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุชุตููุฉ ุงูุนูุงุตุฑ ุงูุชู ูุฑูุฏูุง:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

ุจุนุฏ ุฐููุ ูููููุง ุจุจุณุงุทุฉ ุชุทุจูู ูุฐู ุงูุฏุงูุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุจุซ:

```py
# ูุฐู ุงูุฎููุฉ ุณุชุณุชุบุฑู ููุชูุง ุทูููุงู ููุชูููุฐุ ูุฐุง ูุฌุจ ุนููู ุชุฎุทููุง ูุงูุงูุชูุงู ุฅูู
# ุงูุชุงูู!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

ูุฐุง ูุชุฑู ููุง ุญูุงูู 3% ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฃุตููุฉุ ูุงูุชู ูุง ุชุฒุงู ูุจูุฑุฉ ุงูุญุฌู - ุชุชููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุงุชุฌุฉ ูู 600,000 ูุต ุจุฑูุฌู ุจุงูุซูู ุจุญุฌู 6 ุฌูุฌุงุจุงูุช!

ูููู ุฃู ูุณุชุบุฑู ุชุตููุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุงููุฉ 2-3 ุณุงุนุงุช ุญุณุจ ุฌูุงุฒู ูุนุฑุถ ุงููุทุงู ุงูุชุฑุฏุฏู. ุฅุฐุง ููุช ูุง ุชุฑุบุจ ูู ุงููุฑูุฑ ุจูุฐู ุงูุนูููุฉ ุงูุทูููุฉ ุจููุณูุ ูุฅููุง ูููุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุตูุงุฉ ุนูู ุงููุฑูุฒ ูุชุชููู ูู ุชูุฒูููุง:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```
<Tip>

ุณูุณุชุบุฑู ุงูุชุฏุฑูุจ ุงููุณุจู ููููุฐุฌ ุงููุบุฉ ุจุนุถ ุงูููุช. ููุชุฑุญ ุฃู ุชููู ุจุชุดุบูู ุญููุฉ ุงูุชุฏุฑูุจ ุนูู ุนููุฉ ูู ุงูุจูุงูุงุช ุนู ุทุฑูู ุฅูุบุงุก ุชุนููู ุงูุณุทุฑูู ุงูุฌุฒุฆููู ุฃุนูุงูุ ูุงูุชุฃูุฏ ูู ุงูุชูุงู ุงูุชุฏุฑูุจ ุจูุฌุงุญ ูุชุฎุฒูู ุงูููุงุฐุฌ. ูุง ููุฌุฏ ุดูุก ุฃูุซุฑ ุฅุญุจุงุทูุง ูู ูุดู ุนูููุฉ ุงูุชุฏุฑูุจ ูู ุงูุฎุทูุฉ ุงูุฃุฎูุฑุฉ ูุฃูู ูุณูุช ุฅูุดุงุก ูุฌูุฏ ุฃู ุจุณุจุจ ูุฌูุฏ ุฎุทุฃ ูุทุจุนู ูู ููุงูุฉ ุญููุฉ ุงูุชุฏุฑูุจ!

</Tip>

ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ูุซุงู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช. ุณูุนุฑุถ ููุท ุฃูู 200 ุญุฑู ูู ูู ุญูู:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

ูููููุง ุฃู ูุฑู ุฃู ุญูู `content` ูุญุชูู ุนูู ุงูููุฏ ุงูุฐู ูุฑูุฏ ุฃู ูุชุฏุฑุจ ุนููู ุงููููุฐุฌ. ุงูุขู ุจุนุฏ ุฃู ูุฏููุง ูุฌููุนุฉ ุจูุงูุงุชุ ูุญุชุงุฌ ุฅูู ุฅุนุฏุงุฏ ุงููุตูุต ุจุญูุซ ุชููู ูู ุชูุณูู ููุงุณุจ ููุชุฏุฑูุจ ุงููุณุจู.

## ุฅุนุฏุงุฏ ูุฌููุนุฉ ุงูุจูุงูุงุช[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

ุณุชููู ุงูุฎุทูุฉ ุงูุฃููู ูู ุชูุณูู ุงูุจูุงูุงุช ุฅูู ุฑููุฒุ ุจุญูุซ ูููููุง ุงุณุชุฎุฏุงููุง ููุชุฏุฑูุจ. ุจูุง ุฃู ูุฏููุง ูู ุฅููุงู ูุธุงุฆู ุงูุงุณุชุฏุนุงุก ุงููุตูุฑุฉ ุจุดูู ุฃุณุงุณูุ ูููููุง ุงูุญูุงุธ ุนูู ุญุฌู ุงูุณูุงู ุตุบูุฑูุง ูุณุจููุง. ูุฐู ุงูููุฒุฉ ุชุณูุญ ููุง ุจุชุฏุฑูุจ ุงููููุฐุฌ ุจุดูู ุฃุณุฑุน ุจูุซูุฑ ูุชุชุทูุจ ุฐุงูุฑุฉ ุฃูู ุจูุซูุฑ. ุฅุฐุง ูุงู ูู ุงูููู ูุชุทุจููู ุฃู ูููู ูุฏูู ุณูุงู ุฃูุจุฑ (ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ููุช ุชุฑูุฏ ุฃู ูููู ุงููููุฐุฌ ุจูุชุงุจุฉ ุงุฎุชุจุงุฑุงุช ุงููุญุฏุฉ ุจูุงุกู ุนูู ููู ุจุชุนุฑูู ุงููุธููุฉ)ุ ุชุฃูุฏ ูู ุฒูุงุฏุฉ ุฐูู ุงูุฑููุ ูููู ุถุน ูู ุงุนุชุจุงุฑู ุฃูุถูุง ุฃู ูุฐุง ูุฃุชู ูุน ูุณุงุญุฉ ุฐุงูุฑุฉ GPU ุฃูุจุฑ. ุฏุนูุง ุงูุขู ูุซุจุช ุญุฌู ุงูุณูุงู ุนูุฏ 128 ุฑูุฒุ ุนูู ุนูุณ 1,024 ุฃู 2,048 ุงููุณุชุฎุฏูุฉ ูู GPT-2 ุฃู GPT-3ุ ุนูู ุงูุชูุงูู.

ุชุญุชูู ูุนุธู ุงููุณุชูุฏุงุช ุนูู ุฃูุซุฑ ูู 128 ุฑูุฒุ ูุฐุง ูุฅู ูุฌุฑุฏ ุชูุตูุฑ ุงููุฏุฎูุงุช ุฅูู ุงูุทูู ุงูุฃูุตู ุณููุถู ุนูู ุฌุฒุก ูุจูุฑ ูู ูุฌููุนุฉ ุจูุงูุงุชูุง. ุจุฏูุงู ูู ุฐููุ ุณูุณุชุฎุฏู ุฎูุงุฑ `return_overflowing_tokens` ูุชูุณูู ุงูุฅุฏุฎุงู ุจุงููุงูู ุฅูู ุนุฏุฉ ุฃุฌุฒุงุกุ ููุง ูุนููุง ูู [ุงููุตู 6](/course/chapter6/4). ุณูุณุชุฎุฏู ุฃูุถูุง ุฎูุงุฑ `return_length` ูุฅุฑุฌุงุน ุทูู ูู ุฌุฒุก ุชู ุฅูุดุงุคู ุชููุงุฆููุง. ุบุงูุจูุง ูุง ูููู ุงูุฌุฒุก ุงูุฃุฎูุฑ ุฃุตุบุฑ ูู ุญุฌู ุงูุณูุงูุ ูุณูุชุฎูุต ูู ูุฐู ุงููุทุน ูุชุฌูุจ ูุดููุงุช ุงูุญุดูุ ูุญู ูุง ูุญุชุงุฌูุง ุญููุง ูุฃู ูุฏููุง ุงููุซูุฑ ูู ุงูุจูุงูุงุช ุนูู ุฃู ุญุงู.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="ุชูุณูู ูุต ูุจูุฑ ุฅูู ุนุฏุฉ ุฃุฌุฒุงุก."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="ุชูุณูู ูุต ูุจูุฑ ุฅูู ุนุฏุฉ ุฃุฌุฒุงุก."/>
</div>

ุฏุนูุง ูุฑู ุจุงูุถุจุท ููู ูุนูู ูุฐุง ุนู ุทุฑูู ุงููุธุฑ ุฅูู ุฃูู ูุซุงููู:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ูููููุง ุฃู ูุฑู ุฃููุง ูุญุตู ุนูู 34 ููุทุนูุง ุฅุฌูุงูููุง ูู ูุฐูู ุงููุซุงููู. ุจุงููุธุฑ ุฅูู ุฃุทูุงู ุงูุฃุฌุฒุงุกุ ูููููุง ุฃู ูุฑู ุฃู ุงูุฃุฌุฒุงุก ูู ููุงูุงุช ูู ูู ุงููุณุชูุฏูู ุชุญุชูู ุนูู ุฃูู ูู 128 ุฑูุฒ (117 ู41ุ ุนูู ุงูุชูุงูู). ุชูุซู ูุฐู ุงูุฃุฌุฒุงุก ูุฌุฑุฏ ุฌุฒุก ุตุบูุฑ ูู ุฅุฌูุงูู ุงูุฃุฌุฒุงุก ุงูุชู ูุฏููุงุ ูุฐุง ูููููุง ุงูุชุฎูุต ูููุง ุจุฃูุงู. ุจุงุณุชุฎุฏุงู ุญูู `overflow_to_sample_mapping`ุ ูููููุง ุฃูุถูุง ุฅุนุงุฏุฉ ุจูุงุก ุงูุฃุฌุฒุงุก ุงูุชู ุชูุชูู ุฅูู ุนููุงุช ุงูุฅุฏุฎุงู.

ูุน ูุฐู ุงูุนูููุฉุ ูุณุชุฎุฏู ููุฒุฉ ูููุฏุฉ ููุธููุฉ `Dataset.map()` ูู ๐ค Datasetsุ ูุงูุชู ูุง ุชุชุทูุจ ุฎุฑุงุฆุท ูุงุญุฏ ุฅูู ูุงุญุฏุ ููุง ุฑุฃููุง ูู [ุงููุณู 3](/course/chapter7/3)ุ ูููููุง ุฅูุดุงุก ุฏูุนุงุช ุชุญุชูู ุนูู ุนุฏุฏ ุฃูุจุฑ ุฃู ุฃูู ูู ุงูุนูุงุตุฑ ูู ุงูุฏูุนุฉ ุงููุฏุฎูุฉ. ูุฐุง ูููุฏ ุนูุฏ ุฅุฌุฑุงุก ุนูููุงุช ูุซู ุฒูุงุฏุฉ ุงูุจูุงูุงุช ุฃู ุชุตููุฉ ุงูุจูุงูุงุช ุงูุชู ุชุบูุฑ ุนุฏุฏ ุงูุนูุงุตุฑ. ูู ุญุงูุชูุงุ ุนูุฏ ุชูุณูู ูู ุนูุตุฑ ุฅูู ุฃุฌุฒุงุก ูู ุญุฌู ุงูุณูุงู ุงููุญุฏุฏุ ูููู ุจุฅูุดุงุก ุงูุนุฏูุฏ ูู ุงูุนููุงุช ูู ูู ูุณุชูุฏ. ูุญู ุจุญุงุฌุฉ ููุท ููุชุฃูุฏ ูู ุญุฐู ุงูุฃุนูุฏุฉ ุงูููุฌูุฏุฉุ ูุฃููุง ุฐุงุช ุญุฌู ูุชุถุงุฑุจ. ุฅุฐุง ุฃุฑุฏูุง ุงูุงุญุชูุงุธ ุจูุงุ ูููููุง ุชูุฑุงุฑูุง ุจุดูู ููุงุณุจ ูุฅุฑุฌุงุนูุง ุถูู ููุงููุฉ `Dataset.map()`:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

ูุฏููุง ุงูุขู 16.7 ููููู ูุซุงู ูุญุชูู ูู ูููุง ุนูู 128 ุฑูุฒุ ููู ูุง ููุงุจู ุญูุงูู 2.1 ูููุงุฑ ุฑูุฒ ูู ุงููุฌููุน. ููุฑุฌูุนุ ุชู ุชุฏุฑูุจ ููุงุฐุฌ OpenAI's GPT-3 ูCodex ุนูู 300 ู100 ูููุงุฑ ุฑูุฒุ ุนูู ุงูุชูุงููุ ุญูุซ ุชู ุชููุฆุฉ ููุงุฐุฌ Codex ูู ููุงุท ุชูุชูุด GPT-3. ูุฏููุง ูู ูุฐุง ุงููุณู ููุณ ุงูุชูุงูุณ ูุน ูุฐู ุงูููุงุฐุฌุ ูุงูุชู ูููููุง ุชูููุฏ ูุตูุต ูุชูุงุณูุฉ ุทูููุฉุ ูููู ูุฅูุดุงุก ูุณุฎุฉ ูุตุบุฑุฉ ุชููุฑ ูุธููุฉ ุฅููุงู ุชููุงุฆู ุณุฑูุนุฉ ูุนููุงุก ุงูุจูุงูุงุช.

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุฌุงูุฒุฉุ ุฏุนูุง ูุนุฏ ุงููููุฐุฌ!

<Tip>

โ๏ธ **ุฌุฑุจู!** ูู ููู ุงูุชุฎูุต ูู ุฌููุน ุงูุฃุฌุฒุงุก ุงูุชู ุชููู ุฃุตุบุฑ ูู ุญุฌู ุงูุณูุงู ูุดููุฉ ูุจูุฑุฉ ููุง ูุฃููุง ูุณุชุฎุฏู ููุงูุฐ ุณูุงู ุตุบูุฑุฉ. ูููุง ุฒุงุฏ ุญุฌู ุงูุณูุงู (ุฃู ุฅุฐุง ูุงู ูุฏูู ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช ุงููุตูุฑุฉ)ุ ุณูููู ุฃูุถูุง ุงูุฌุฒุก ูู ุงูุฃุฌุฒุงุก ุงูุชู ูุชู ุงูุชุฎูุต ูููุง. ููุงู ุทุฑููุฉ ุฃูุซุฑ ููุงุกุฉ ูุฅุนุฏุงุฏ ุงูุจูุงูุงุช ููู ุฏูุฌ ุฌููุน ุงูุนููุงุช ุงููุนููุฉ ูู ุฏูุนุฉ ูุน ุฑูุฒ `eos_token_id` ุจูููุงุ ุซู ุฅุฌุฑุงุก ุงูุชูุณูู ุนูู ุงูุชุณูุณูุงุช ุงููุฏูุฌุฉ. ูุชูุฑููุ ุนุฏู ูุธููุฉ `tokenize()` ูุงุณุชุฎุฏุงู ูุฐุง ุงูููุฌ. ูุงุญุธ ุฃูู ุณุชุฑูุฏ ุชุนููู `truncation=False` ูุฅุฒุงูุฉ ุงูุญุฌุฌ ุงูุฃุฎุฑู ูู ุงููุนูู ููู ุชุญุตู ุนูู ุงูุชุณูุณู ุงููุงูู ูุฃุฑูุงู ุงูุฑููุฒ.

</Tip>


## ุชููุฆุฉ ูููุฐุฌ ุฌุฏูุฏ[[initializing-a-new-model]]

ุณุชููู ุฎุทูุชูุง ุงูุฃููู ูู ุชููุฆุฉ ูููุฐุฌ GPT-2 ุฌุฏูุฏ. ุณูุณุชุฎุฏู ููุณ ุงูุชููุฆุฉ ููููุฐุฌูุง ููุง ูู ุงูุญุงู ูู ูููุฐุฌ GPT-2 ุงูุตุบูุฑุ ูุฐุง ูููู ุจุชุญููู ุงูุชููุฆุฉ ุงููุณุจูุฉุ ูุงูุชุฃูุฏ ูู ุฃู ุญุฌู ุงููุนูู ูุทุงุจู ุญุฌู ููุฑุฏุงุช ุงููููุฐุฌ ูุชูุฑูุฑ ุฑููุฒ `bos` ู`eos` (ุจุฏุงูุฉ ูููุงูุฉ ุงูุชุณูุณู) IDs:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ูุน ูุฐู ุงูุชููุฆุฉุ ูููููุง ุชุญููู ูููุฐุฌ ุฌุฏูุฏ. ูุงุญุธ ุฃู ูุฐู ูู ุงููุฑุฉ ุงูุฃููู ุงูุชู ูุง ูุณุชุฎุฏู ูููุง ูุธููุฉ `from_pretrained()`ุ ูุฃููุง ูู ุงููุงูุน ูููุฆ ูููุฐุฌูุง ุจุฃููุณูุง:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ูุน ูุฐุง ุงูุชููููุ ูููููุง ุชุญููู ูููุฐุฌ ุฌุฏูุฏ. ูุงุญุธ ุฃู ูุฐู ูู ุงููุฑุฉ ุงูุฃููู ุงูุชู ูุง ูุณุชุฎุฏู ูููุง ุฏุงูุฉ `from_pretrained()`ุ ูุฃููุง ูู ุงููุงูุน ูููู ุจุชููุฆุฉ ูููุฐุฌ ุจุฃููุณูุง:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # ุจูุงุก ุงููููุฐุฌ
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

ูุฏู ูููุฐุฌูุง 124M ูุนุงููุงุช ุณูููู ุจุถุจุทูุง. ูุจู ุฃู ูุจุฏุฃ ุงูุชุฏุฑูุจุ ูุญุชุงุฌ ุฅูู ุฅุนุฏุงุฏ ุฌุงูุน ุจูุงูุงุช ูููู ุจุฅูุดุงุก ุงูุฏูุนุงุช. ูููููุง ุงุณุชุฎุฏุงู ุฌุงูุน ุจูุงูุงุช `DataCollatorForLanguageModeling`ุ ูุงูุฐู ุชู ุชุตูููู ุฎุตูุตูุง ููููุฐุฌุฉ ุงููุบููุฉ (ููุง ููุญู ุงูุงุณู ุจุดูู ุฎูู). ุจุงูุฅุถุงูุฉ ุฅูู ุชูุฏูุณ ูุชูุณูุฏ ุงูุฏูุนุงุชุ ูุฅูู ูููู ุฃูุถูุง ุจุฅูุดุงุก ุชุณููุงุช ุงููููุฐุฌ ุงููุบูู - ูู ุงูููุฐุฌุฉ ุงููุบููุฉ ุงูุณุจุจูุฉ ุชุนูู ุงููุฏุฎูุงุช ูุนูุงูุงุช ุฃูุถูุง (ูุชุญููุฉ ููุท ุจุนูุตุฑ ูุงุญุฏ)ุ ููููู ุฌุงูุน ุงูุจูุงูุงุช ูุฐุง ุจุฅูุดุงุฆูุง ุฃุซูุงุก ุงูุชุฏุฑูุจ ุญุชู ูุง ูุญุชุงุฌ ุฅูู ุชูุฑุงุฑ `input_ids`.

ูุงุญุธ ุฃู `DataCollatorForLanguageModeling` ูุฏุนู ูู ูู ุงูููุฐุฌุฉ ุงููุบููุฉ ุงููููุนุฉ (MLM) ูุงูููุฐุฌุฉ ุงููุบููุฉ ุงูุณุจุจูุฉ (CLM). ุจุดูู ุงูุชุฑุงุถูุ ูููู ุจุฅุนุฏุงุฏ ุงูุจูุงูุงุช ูู MLMุ ูููู ูููููุง ุงูุชุจุฏูู ุฅูู CLM ุนู ุทุฑูู ุชุนููู ูุณูุท `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ูุซุงู:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

ูููููุง ุฃู ูุฑู ุฃู ุงูุฃูุซูุฉ ุชู ุชูุฏูุณูุง ูุฃู ุฌููุน ุงููุตูููุงุช ููุง ููุณ ุงูุดูู.

{#if fw === 'tf'}

ุงูุขู ูููููุง ุงุณุชุฎุฏุงู ุทุฑููุฉ `prepare_tf_dataset()` ูุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ุฅูู ูุฌููุนุงุช ุจูุงูุงุช TensorFlow ุจุงุณุชุฎุฏุงู ุฌุงูุน ุงูุจูุงูุงุช ุงูุฐู ุฃูุดุฃูุงู ุฃุนูุงู:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

โ๏ธ ูุญุฏุซ ุชุญููู ุงููุฏุฎูุงุช ูุงูุนูุงูุงุช ูููุงุกูุชูุง ุฏุงุฎู ุงููููุฐุฌุ ูุฐูู ูููู ุฌุงูุน ุงูุจูุงูุงุช ุจูุณุฎ ุงููุฏุฎูุงุช ูุฅูุดุงุก ุงูุนูุงูุงุช.

</Tip>


ุงูุขู ูุฏููุง ูู ุดูุก ูู ููุงูู ูุชุฏุฑูุจ ูููุฐุฌูุง ุจุงููุนู - ูู ููู ุงูุฃูุฑ ุจูุฐุง ุงููุฏุฑ ูู ุงูุนูู ุจุนุฏ ูู ุดูุก! ูุจู ุฃู ูุจุฏุฃ ุงูุชุฏุฑูุจุ ูุฌุจ ุฃู ูููู ุจุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face. ุฅุฐุง ููุช ุชุนูู ูู ุฏูุชุฑ ููุงุญุธุงุชุ ูููููู ุงูููุงู ุจุฐูู ุจุงุณุชุฎุฏุงู ุฏุงูุฉ ุงููุณุงุนุฏุฉ ุงูุชุงููุฉ:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ุณูุชู ุนุฑุถ ุนูุตุฑ ูุงุฌูุฉ ูุณุชุฎุฏู ุญูุซ ููููู ุฅุฏุฎุงู ุจูุงูุงุช ุงุนุชูุงุฏ ุชุณุฌูู ุฏุฎูู Hugging Face ุงูุฎุงุตุฉ ุจู.

ุฅุฐุง ูู ุชูู ุชุนูู ูู ุฏูุชุฑ ููุงุญุธุงุชุ ููุง ุนููู ุณูู ูุชุงุจุฉ ุงูุณุทุฑ ุงูุชุงูู ูู ุงููุญุทุฉ ุงูุทุฑููุฉ ุงูุฎุงุตุฉ ุจู:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

ูู ูุง ุชุจูู ูู ุชูููู ุญุฌุฌ ุงูุชุฏุฑูุจ ูุชุดุบูู `Trainer`. ุณูุณุชุฎุฏู ุฌุฏูู ูุนุฏู ุชุนูู ูููู ูุน ุจุนุถ ุงูุชุณุฎูู ูุญุฌู ุฏูุนุฉ ูุนุงู ูุจูุบ 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). ูุชู ุงุณุชุฎุฏุงู ุชุฑุงูู ุงูุชุฏุฑุฌ ุนูุฏูุง ูุง ุชุชูุงุณุจ ุฏูุนุฉ ูุงุญุฏุฉ ูุน ุงูุฐุงูุฑุฉุ ููุจูู ุชุฏุฑูุฌููุง ุงูุชุฏุฑุฌ ูู ุฎูุงู ุงูุนุฏูุฏ ูู ุนูููุงุช ุงูุชูุฑูุฑ ููุฃูุงู ูุงูุฎูู. ุณูุฑู ูุฐุง ูู ุงูุนูู ุนูุฏูุง ููุดุฆ ุญููุฉ ุงูุชุฏุฑูุจ ูุน ๐ค Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

ุงูุขู ูููููุง ุจุจุณุงุทุฉ ุชุดุบูู `Trainer` ูุงูุชุธุงุฑ ุงูุชูุงู ุงูุชุฏุฑูุจ. ุงุนุชูุงุฏูุง ุนูู ูุง ุฅุฐุง ููุช ุชุดุบูู ุนูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉ ุฃู ุฌุฒุก ูููุงุ ุณูุณุชุบุฑู ุฐูู 20 ุฃู ุณุงุนุชูู ุนูู ุงูุชูุงููุ ูุฐุง ุงุญุตู ุนูู ุจุนุถ ุงููููุฉ ููุชุงุจ ุฌูุฏ ูููุฑุงุกุฉ!

```py
trainer.train()
```

ุจุนุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ูููููุง ุฏูุน ุงููููุฐุฌ ููุญูู ุงูุฑููุฒ ุฅูู ุงููุฑูุฒ:

```py
trainer.push_to_hub()
```

{:else}

ูู ูุง ุชุจูู ูู ุชูููู ูุนููุงุช ุงูุชุฏุฑูุจ ูุงุณุชุฏุนุงุก `compile()` ู `fit()`. ุณูุณุชุฎุฏู ุฌุฏูู ูุนุฏู ุชุนูู ูุน ุจุนุถ ุงูุชุณุฎูู ูุชุญุณูู ุงุณุชูุฑุงุฑ ุงูุชุฏุฑูุจ:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ุชุฏุฑูุจ ูู ุฏูุฉ ุงูููุทุฉ ุงูุนุงุฆูุฉ ุงููุฎุชูุทุฉ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ุงูุขู ูููููุง ุจุจุณุงุทุฉ ุงุณุชุฏุนุงุก `model.fit()` ูุงูุชุธุงุฑ ุงูุชูุงู ุงูุชุฏุฑูุจ. ุงุนุชูุงุฏูุง ุนูู ูุง ุฅุฐุง ููุช ุชุดุบูู ุนูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉ ุฃู ุฌุฒุก ูููุงุ ุณูุณุชุบุฑู ุฐูู 20 ุฃู ุณุงุนุชูู ุนูู ุงูุชูุงููุ ูุฐุง ุงุญุตู ุนูู ุจุนุถ ุงููููุฉ ููุชุงุจ ุฌูุฏ ูููุฑุงุกุฉ! ุจุนุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ูููููุง ุฏูุน ุงููููุฐุฌ ููุญูู ุงูุฑููุฒ ุฅูู ุงููุฑูุฒ:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ููุฏ ุงุณุชุบุฑููุง ููุท ุญูุงูู 30 ุณุทุฑุง ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุจุงูุฅุถุงูุฉ ุฅูู `TrainingArguments` ููุงูุชูุงู ูู ุงููุตูุต ุงูุฎุงู ุฅูู ุชุฏุฑูุจ GPT-2. ุฌุฑุจูุง ูุน ูุฌููุนุฉ ุจูุงูุงุชู ุงูุฎุงุตุฉ ูุดุงูุฏ ุฅุฐุง ููุช ุชุณุชุทูุน ุงูุญุตูู ุนูู ูุชุงุฆุฌ ุฌูุฏุฉ!

</Tip>

<Tip>

{#if fw === 'pt'}

๐ก ุฅุฐุง ูุงู ูุฏูู ุฅููุงููุฉ ุงููุตูู ุฅูู ุฌูุงุฒ ุจู ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณูููุฉ ูุชุนุฏุฏุฉุ ุญุงูู ุชุดุบูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ููุงู. ูููู `Trainer` ุจุฅุฏุงุฑุฉ ุงูุฃุฌูุฒุฉ ุงููุชุนุฏุฏุฉ ุชููุงุฆููุงุ ููููู ุฃู ูุณุฑุน ุงูุชุฏุฑูุจ ุจุดูู ูุจูุฑ.

{:else}

๐ก ุฅุฐุง ูุงู ูุฏูู ุฅููุงููุฉ ุงููุตูู ุฅูู ุฌูุงุฒ ุจู ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณูููุฉ ูุชุนุฏุฏุฉุ ููููู ุชุฌุฑุจุฉ ุงุณุชุฎุฏุงู ุณูุงู `MirroredStrategy` ูุชุณุฑูุน ุงูุชุฏุฑูุจ ุจุดูู ูุจูุฑ. ุณุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุงุฆู `tf.distribute.MirroredStrategy`ุ ูุงูุชุฃูุฏ ูู ุฃู ุฃู ุฃุณุงููุจ `to_tf_dataset()` ุฃู `prepare_tf_dataset()` ุจุงูุฅุถุงูุฉ ุฅูู ุฅูุดุงุก ุงููููุฐุฌ ูุงูููุงููุฉ ุฅูู `fit()` ูุชู ุชุดุบูููุง ุฌููุนูุง ูู ุณูุงู `scope()` ุงูุฎุงุต ุจูุง. ููููู ุงูุงุทูุงุน ุนูู ุงููุซุงุฆู ุงููุชุนููุฉ ุจูุฐุง ุงูุฃูุฑ [ููุง](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## ุชูููุฏ ุงูููุฏ ุจุงุณุชุฎุฏุงู ุฎุท ุฃูุงุจูุจ [[code-generation-with-a-pipeline]]

ุงูุขู ูู ูุญุธุฉ ุงูุญูููุฉ: ุฏุนูุง ูุฑู ูุฏู ุฌูุฏุฉ ุนูู ุงููููุฐุฌ ุงููุฏุฑุจ ุจุงููุนู! ูููููุง ุฃู ูุฑู ูู ุงูุณุฌูุงุช ุฃู ุงูุฎุณุงุฑุฉ ุงูุฎูุถุช ุจุซุจุงุชุ ูููู ููุถุน ุงููููุฐุฌ ุชุญุช ุงูุงุฎุชุจุงุฑ ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ูุฏู ุฌูุฏุฉ ุนููู ุนูู ุจุนุถ ุงููุทุงูุจุงุช. ููููุงู ุจุฐููุ ุณูููู ุจุชุบููู ุงููููุฐุฌ ูู ุฎุท ุฃูุงุจูุจ ุชูููุฏ ุงููุตุ ูุณูุถุนู ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ููุนูููุงุช ุงูุณุฑูุนุฉ ุฅุฐุง ูุงู ููุงู ูุงุญุฏ ูุชุงุญ:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

ุฏุนูุง ูุจุฏุฃ ุจุงููููุฉ ุงูุจุณูุทุฉ ุงููุชูุซูุฉ ูู ุฅูุดุงุก ูุฎุทุท ูุชูุฑู:

```py
txt = """\
# ุฅูุดุงุก ุจุนุถ ุงูุจูุงูุงุช
x = np.random.randn(100)
y = np.random.randn(100)

# ุฅูุดุงุก ูุฎุทุท ูุชูุฑู ุจุงุณุชุฎุฏุงู x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# ุฅูุดุงุก ุจุนุถ ุงูุจูุงูุงุช
x = np.random.randn(100)
y = np.random.randn(100)

# ุฅูุดุงุก ูุฎุทุท ูุชูุฑู ุจุงุณุชุฎุฏุงู x, y
plt.scatter(x, y)

# ุฅูุดุงุก ูุฎุทุท ูุชูุฑู
```

ุชุจุฏู ุงููุชูุฌุฉ ุตุญูุญุฉ. ูู ูุนูู ุฃูุถูุง ูุนูููุฉ pandasุ ุฏุนูุง ูุฑู ุฅุฐุง ูุงู ุจุฅููุงููุง ุฅูุดุงุก DataFrame ูู ูุตูููุชูู:

```py
txt = """\
# ุฅูุดุงุก ุจุนุถ ุงูุจูุงูุงุช
x = np.random.randn(100)
y = np.random.randn(100)

# ุฅูุดุงุก DataFrame ูู x ู y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# ุฅูุดุงุก ุจุนุถ ุงูุจูุงูุงุช
x = np.random.randn(100)
y = np.random.randn(100)

# ุฅูุดุงุก DataFrame ูู x ู y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

ุฌูููุ ูุฐุง ูู ุงูุฌูุงุจ ุงูุตุญูุญ - ุนูู ุงูุฑุบู ูู ุฃูู ุจุนุฏ ุฐูู ูููู ุจุฅุฏุฑุงุฌ ุงูุนููุฏ "x" ูุฑุฉ ุฃุฎุฑู. ูุธุฑูุง ูุฃู ุนุฏุฏ ุงูุฑููุฒ ุงููููุฏุฉ ูุญุฏูุฏุ ูุชู ูุทุน ุญููุฉ "for" ุงูุชุงููุฉ. ุฏุนูุง ูุฑู ุฅุฐุง ูุงู ุจุฅููุงููุง ุงูููุงู ุจุดูุก ุฃูุซุฑ ุชุนููุฏูุง ูุฌุนู ุงููููุฐุฌ ูุณุงุนุฏูุง ูู ุงุณุชุฎุฏุงู ุนูููุฉ groupby:

```py
txt = """\
# dataframe ูุน ุงููููุฉ ูุงูุฏุฎู ูุงูุงุณู
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# ุญุณุงุจ ูุชูุณุท ุงูุฏุฎู ููู ูููุฉ
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe ูุน ุงููููุฉ ูุงูุฏุฎู ูุงูุงุณู
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# ุญุณุงุจ ูุชูุณุท ุงูุฏุฎู ููู ูููุฉ
profession = df.groupby(['profession']).mean()

# ุญุณุงุจ
```

ููุณ ุณูุฆูุงุ ูุฐู ูู ุงูุทุฑููุฉ ุงูุตุญูุญุฉ ููููุงู ุจุฐูู. ุฃุฎูุฑูุงุ ุฏุนูุง ูุฑู ุฅุฐุง ูุงู ุจุฅููุงููุง ุงุณุชุฎุฏุงูู ุฃูุถูุง ูู scikit-learn ูุฅุนุฏุงุฏ ูููุฐุฌ Random Forest:

```py
txt = """
# ุงุณุชูุฑุงุฏ random forest regressor ูู scikit-learn
from sklearn.ensemble import RandomForestRegressor

# ุชูุงุณุจ ูููุฐุฌ random forest ูุน 300 ูู ุงูููุฏุฑูู ุนูู X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# ุงุณุชูุฑุงุฏ random forest regressor ูู scikit-learn
from sklearn.ensemble import RandomForestRegressor

# ุชูุงุณุจ ูููุฐุฌ random forest ูุน 300 ูู ุงูููุฏุฑูู ุนูู X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

ุนูุฏ ุงููุธุฑ ุฅูู ูุฐู ุงูุฃูุซูุฉ ุงูููููุฉุ ูุจุฏู ุฃู ุงููููุฐุฌ ูุฏ ุชุนูู ุจุนุถ ุจูุงุก ุฌููุฉ ูุฌููุนุฉ ุฃุฏูุงุช ุนููู ุงูุจูุงูุงุช ูู ุจุงูุซูู. ุจุงูุทุจุนุ ุณูุญุชุงุฌ ุฅูู ุชูููู ุงููููุฐุฌ ุจุดูู ุฃูุซุฑ ุดููููุฉ ูุจู ูุดุฑู ูู ุงูุนุงูู ุงูุญููููุ ูููู ูุฐุง ูุง ูุฒุงู ูููุฐุฌูุง ุฃููููุง ุฑุงุฆุนูุง.

{:else}

ุนูุฏ ุงููุธุฑ ุฅูู ูุฐู ุงูุฃูุซูุฉ ุงูููููุฉุ ูุจุฏู ุฃู ุงููููุฐุฌ ูุฏ ุชุนูู ุจุนุถ ุจูุงุก ุฌููุฉ ูุฌููุนุฉ ุฃุฏูุงุช ุนููู ุงูุจูุงูุงุช ูู ุจุงูุซูู (ุจุงูุทุจุนุ ุณูุญุชุงุฌ ุฅูู ุชููููู ุจุดูู ุฃูุซุฑ ุดููููุฉ ูุจู ูุดุฑ ุงููููุฐุฌ ูู ุงูุนุงูู ุงูุญูููู). ูู ุจุนุถ ุงูุฃุญูุงูุ ูุชุทูุจ ุงูุฃูุฑ ุงููุฒูุฏ ูู ุงูุชุฎุตูุต ูุชุฏุฑูุจ ุงููููุฐุฌ ูุชุญููู ุงูุฃุฏุงุก ุงูุถุฑูุฑู ูุญุงูุฉ ุงุณุชุฎุฏุงู ูุนููุฉุ ููุน ุฐูู. ุนูู ุณุจูู ุงููุซุงูุ ูุงุฐุง ูู ุฃุฑุฏูุง ุชุญุฏูุซ ุญุฌู ุงูุฏูุนุฉ ุฏููุงูููููุง ุฃู ูุฌูุฏ ุญููุฉ ุชุฏุฑูุจ ุดุฑุทูุฉ ุชุฎุทู ุงูุฃูุซูุฉ ุงูุณูุฆุฉ ุฃุซูุงุก ุงูุชูููุ ุฃุญุฏ ุงูุฎูุงุฑุงุช ูู ุฅูุดุงุก ูุฆุฉ ูุฑุนูุฉ ูู `Trainer` ูุฅุถุงูุฉ ุงูุชุบููุฑุงุช ุงูุถุฑูุฑูุฉุ ูููู ูู ุจุนุถ ุงูุฃุญูุงู ูููู ูู ุงูุฃุจุณุท ูุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ูู ุงูุตูุฑ. ููุง ูุฃุชู ุฏูุฑ ๐ค Accelerate.

{/if}

{#if fw === 'pt'}

## ุงูุชุฏุฑูุจ ูุน ๐ค Accelerate[[training-with-accelerate]]

ููุฏ ุฑุฃููุง ููููุฉ ุชุฏุฑูุจ ูููุฐุฌ ุจุงุณุชุฎุฏุงู `Trainer`ุ ูุงูุฐู ูููู ุฃู ูุณูุญ ุจุจุนุถ ุงูุชุฎุตูุต. ููุน ุฐููุ ูู ุจุนุถ ุงูุฃุญูุงู ูุฑูุฏ ุงูุชุญูู ุงููุงูู ูู ุญููุฉ ุงูุชุฏุฑูุจุ ุฃู ูุฑูุฏ ุฅุฌุฑุงุก ุจุนุถ ุงูุชุบููุฑุงุช ุงูุบุฑูุจุฉ. ูู ูุฐู ุงูุญุงูุฉุ ูุนุฏ ๐ค Accelerate ุฎูุงุฑูุง ุฑุงุฆุนูุงุ ููู ูุฐุง ุงููุณู ุณููุฑ ุนุจุฑ ุงูุฎุทูุงุช ูุงุณุชุฎุฏุงูู ูุชุฏุฑูุจ ูููุฐุฌูุง. ูุฌุนู ุงูุฃููุฑ ุฃูุซุฑ ุฅุซุงุฑุฉ ููุงูุชูุงูุ ุณูุถูู ุฃูุถูุง ุชุญูููุง ุฅูู ุญููุฉ ุงูุชุฏุฑูุจ.

<Youtube id="Hm8_PgVTFuc"/>

ูุธุฑูุง ูุฃููุง ููุชููู ุจุดูู ุฃุณุงุณู ุจุงูุงุณุชููุงู ุงูุชููุงุฆู ุงูููุทูู ูููุชุจุงุช ุนููู ุงูุจูุงูุงุชุ ููู ุงูููุทูู ุฅุนุทุงุก ุงููุฒูุฏ ูู ุงููุฒู ูุนููุงุช ุงูุชุฏุฑูุจ ุงูุชู ุชุณุชุฎุฏู ูุฐู ุงูููุชุจุงุช ุจุดูู ุฃูุจุฑ. ูููููุง ุชุญุฏูุฏ ูุฐู ุงูุฃูุซูุฉ ุจุณูููุฉ ูู ุฎูุงู ุงุณุชุฎุฏุงู ูููุงุช ุฑุฆูุณูุฉ ูุซู `plt`ุ ู`pd`ุ ู`sk`ุ ู`fit`ุ ู`predict`ุ ูุงูุชู ุชุนุฏ ุฃูุซุฑ ุฃุณูุงุก ุงูุงุณุชูุฑุงุฏ ุชูุฑุงุฑูุง ูู `matplotlib.pyplot`ุ ู`pandas`ุ ู`sklearn` ุจุงูุฅุถุงูุฉ ุฅูู ููุท fit/predict ููุฃุฎูุฑ. ุฅุฐุง ุชู ุชูุซูู ูู ูููุง ูุฑููุฒ ููุฑุฏุฉุ ููููููุง ุงูุชุญูู ุจุณูููุฉ ููุง ุฅุฐุง ูุงูุช ุชุญุฏุซ ูู ุชุณูุณู ุงูุฅุฏุฎุงู. ูููู ุฃู ูููู ููุฑููุฒ ุจุงุฏุฆุฉ ูุณุงูุฉ ุจูุถุงุกุ ูุฐุง ุณูุชุญูู ุฃูุถูุง ูู ูุฐู ุงูุฅุตุฏุงุฑุงุช ูู ููุฑุฏุงุช ุงููุญูู ุงููุบูู. ููุชุญูู ูู ุฃููุง ุชุนููุ ุณูุถูู ุฑูุฒ ุงุฎุชุจุงุฑ ูุงุญุฏ ูุฌุจ ุชูุณููู ุฅูู ุฑููุฒ ูุชุนุฏุฏุฉ:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

ุฑุงุฆุนุ ูุจุฏู ุฃูู ูุนูู ุจุดูู ุฌูุฏ! ูููููุง ุงูุขู ูุชุงุจุฉ ุฏุงูุฉ ุฎุณุงุฑุฉ ูุฎุตุตุฉ ุชุฃุฎุฐ ุชุณูุณู ุงูุฅุฏุฎุงูุ ูุงูููุฌูุชุณุ ูุฑููุฒ ุงูููุงุชูุญ ุงูุชู ูููุง ุจุงุฎุชูุงุฑูุง ููุชู ููุฏุฎูุงุช. ุฃููุงู ูุญุชุงุฌ ุฅูู ูุญุงุฐุงุฉ ุงูููุฌูุชุณ ูุงููุฏุฎูุงุช: ูุชู ุชุญููู ุชุณูุณู ุงูุฅุฏุฎุงู ุฅูู ุงููููู ูุชุดููู ุงูุชุตูููุงุชุ ุญูุซ ุฃู ุงูุฑูุฒ ุงูุชุงูู ูู ุงูุชุตููู ููุฑูุฒ ุงูุญุงูู. ูููููุง ุชุญููู ุฐูู ุนู ุทุฑูู ุงูุจุฏุก ุจุงูุชุตูููุงุช ูู ุงูุฑูุฒ ุงูุซุงูู ูุชุณูุณู ุงูุฅุฏุฎุงูุ ุญูุซ ุฃู ุงููููุฐุฌ ูุง ูููู ุจุชูุจุค ููุฑูุฒ ุงูุฃูู ุนูู ุฃู ุญุงู. ุซู ูููู ุจูุทุน ุงูููุฌูุช ุงูุฃุฎูุฑุ ุญูุซ ูุง ููุฌุฏ ูุฏููุง ุชุตููู ููุฑูุฒ ุงูุฐู ูุชุจุน ุชุณูุณู ุงูุฅุฏุฎุงู ุงููุงูู. ุจูุฐุง ูููููุง ุญุณุงุจ ุงูุฎุณุงุฑุฉ ููู ุนููุฉ ูุญุณุงุจ ุชูุฑุงุฑ ุฌููุน ุงููููุงุช ุงูุฑุฆูุณูุฉ ูู ูู ุนููุฉ. ุฃุฎูุฑุงูุ ูููู ุจุญุณุงุจ ุงููุชูุณุท ุงููุฑุฌุญ ูุฌููุน ุงูุนููุงุช ุจุงุณุชุฎุฏุงู ุงูุชูุฑุงุฑุงุช ูุฃูุฒุงู. ุญูุซ ุฃููุง ูุง ูุฑูุฏ ุงูุชุฎูุต ูู ุฌููุน ุงูุนููุงุช ุงูุชู ูุง ุชุญุชูู ุนูู ูููุงุช ุฑุฆูุณูุฉุ ูุถูู 1 ุฅูู ุงูุฃูุฒุงู:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # ุชุญููู ุงูุฑููุฒ ุจุญูุซ ุฃู ุงูุฑููุฒ < n ุชุชูุจุฃ ุจู n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # ุญุณุงุจ ุงูุฎุณุงุฑุฉ ููู ุฑูุฒ
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # ุชุบููุฑ ุญุฌู ููุชูุณุท ุงูุฎุณุงุฑุฉ ููู ุนููุฉ
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # ุญุณุงุจ ูุชุตุบูุฑ ุงูุฃูุฒุงู
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # ุญุณุงุจ ุงููุชูุณุท ุงููุฑุฌุญ
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

ูุจู ุฃู ูุจุฏุฃ ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุฌุฏูุฏุฉ ุงูุฑุงุฆุนุฉ ูุฐูุ ูุญุชุงุฌ ุฅูู ุฅุนุฏุงุฏ ุจุนุถ ุงูุฃุดูุงุก:

- ูุญุชุงุฌ ุฅูู ูุญููุงุช ุงูุจูุงูุงุช ูุชุญููู ุงูุจูุงูุงุช ูู ูุฌููุนุงุช.
- ูุญุชุงุฌ ุฅูู ุถุจุท ูุนููุงุช ุงูุฎูุงุถ ุงููุฒู.
- ูู ููุช ูุขุฎุฑ ูุฑูุฏ ุงูุชููููุ ูุฐูู ูู ุงูููุทูู ุฃู ูุบูู ููุฏ ุงูุชูููู ูู ุฏุงูุฉ.

ุฏุนูุง ูุจุฏุฃ ุจูุญููุงุช ุงูุจูุงูุงุช. ูุญุชุงุฌ ููุท ุฅูู ุถุจุท ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู `"torch"`ุ ุซู ูููููุง ุชูุฑูุฑูุง ุฅูู `DataLoader` ูู PyTorch ูุน ุญุฌู ุงููุฌููุนุฉ ุงูููุงุณุจ:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

ุจุนุฏ ุฐููุ ูููู ุจุชุฌุฒุฆุฉ ุงููุนููุงุช ุจุญูุซ ูุนุฑู ุงููุญุณู ุฃู ูููุง ุณูุญุตู ุนูู ุงูุฎูุงุถ ุฅุถุงูู ูู ุงููุฒู. ุนุงุฏุฉุ ูุชู ุฅุนูุงุก ุฌููุน ูุตุทูุญุงุช ุงูุงูุญูุงุฒ ููุฒู LayerNorm ูู ูุฐุงุ ุฅููู ููู ูููููุง ุงูููุงู ุจุฐูู:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

ุญูุซ ุฃููุง ูุฑูุฏ ุชูููู ุงููููุฐุฌ ุจุงูุชุธุงู ุนูู ูุฌููุนุฉ ุงูุชุญูู ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุฏุนูุง ููุชุจ ุฏุงูุฉ ูุฐูู ุฃูุถุงู. ุชููู ููุท ุจุชุดุบูู ูุญูู ุจูุงูุงุช ุงูุชูููู ูุฌูุน ุฌููุน ุงูุฎุณุงุฆุฑ ุนุจุฑ ุงูุนูููุงุช:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `evaluate()` ูููููุง ุงูุฅุจูุงุบ ุนู ุงูุฎุณุงุฑุฉ ู[perplexity](/course/chapter7/3) ุนูู ูุชุฑุงุช ููุชุธูุฉ. ุจุนุฏ ุฐููุ ูููู ุจุฅุนุงุฏุฉ ุชุนุฑูู ูููุฐุฌูุง ููุชุฃูุฏ ูู ุฃููุง ูุชุฏุฑุจ ูู ุงูุตูุฑ ูุฑุฉ ุฃุฎุฑู:

```py
model = GPT2LMHeadModel(config)
```

ุจุนุฏ ุฐููุ ูููููุง ุชุนุฑูู ุงููุญุณูุ ุจุงุณุชุฎุฏุงู ุงูุฏุงูุฉ ูู ูุจู ูุชูุณูู ุงููุนููุงุช ูุงูุฎูุงุถ ุงููุฒู:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ุงูุขู ุฏุนูุง ูุนุฏ ุงููููุฐุฌุ ูุงููุญุณูุ ููุญููุงุช ุงูุจูุงูุงุช ุจุญูุซ ูููููุง ุงูุจุฏุก ุจุงูุชุฏุฑูุจ:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

๐จ ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู TPUุ ูุณุชุญุชุงุฌ ุฅูู ููู ูู ุงูููุฏ ุจุฏุกุงู ูู ุงูุฎููุฉ ุฃุนูุงู ุฅูู ุฏุงูุฉ ุชุฏุฑูุจ ูุฎุตุตุฉ. ุฑุงุฌุน [ุงููุตู 3](/course/chapter3) ููุฒูุฏ ูู ุงูุชูุงุตูู.

</Tip>

ุงูุขู ุจุนุฏ ุฃู ุฃุฑุณููุง `train_dataloader` ุฅูู `accelerator.prepare()`ุ ูููููุง ุงุณุชุฎุฏุงู ุทููู ูุญุณุงุจ ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ. ุชุฐูุฑ ุฃูู ูุฌุจ ุนูููุง ุฏุงุฆูุงู ุงูููุงู ุจุฐูู ุจุนุฏ ุฅุนุฏุงุฏ ูุญูู ุงูุจูุงูุงุชุ ุญูุซ ุฃู ูุฐู ุงูุทุฑููุฉ ุณุชุบูุฑ ุทููู. ูุณุชุฎุฏู ุฌุฏูููุง ุฎุทููุง ููุงุณููููุง ูู ูุนุฏู ุงูุชุนูู ุฅูู 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

ุฃุฎูุฑุงูุ ูุฏูุน ูููุฐุฌูุง ุฅูู Hubุ ุณูุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุงุฆู `Repository` ูู ูุฌูุฏ ุนูู. ูู ุจุชุณุฌูู ุงูุฏุฎูู ุฅูู Hub Hugging Faceุ ุฅุฐุง ูู ุชูู ูุฏ ุณุฌูุช ุงูุฏุฎูู ุจุงููุนู. ุณูุญุฏุฏ ุงุณู ุงููุณุชูุฏุน ูู ูุนุฑู ุงููููุฐุฌ ุงูุฐู ูุฑูุฏ ุฅุนุทุงุกู ููููุฐุฌูุง (ูุง ุชุชุฑุฏุฏ ูู ุงุณุชุจุฏุงู `repo_name` ุจุฎูุงุฑู ุงูุฎุงุตุ ููู ูุญุชุงุฌ ููุท ุฅูู ุงุญุชูุงุก ุงุณู ุงููุณุชุฎุฏู ุงูุฎุงุต ุจูุ ููู ูุง ุชููู ุจู ุฏุงูุฉ `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ุจุนุฏ ุฐููุ ูููููุง ุงุณุชูุณุงุฎ ูุฐุง ุงููุณุชูุฏุน ูู ูุฌูุฏ ูุญูู. ุฅุฐุง ูุงู ููุฌูุฏูุง ุจุงููุนูุ ููุฌุจ ุฃู ูููู ูุฐุง ุงููุฌูุฏ ุงููุญูู ูุณุชูุณุฎูุง ููุฌูุฏูุง ูููุณุชูุฏุน ุงูุฐู ูุนูู ูุนู:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ุงูุขู ูููููุง ุชุญููู ุฃู ุดูุก ูููู ุจุญูุธู ูู `output_dir` ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `repo.push_to_hub()`. ุณูุณุงุนุฏูุง ูุฐุง ูู ุชุญููู ุงูููุงุฐุฌ ุงููุณูุทุฉ ูู ููุงูุฉ ูู ุญูุจุฉ.

ูุจู ุฃู ูุชุฏุฑุจุ ุฏุนูุง ูุฌุฑู ุงุฎุชุจุงุฑูุง ุณุฑูุนูุง ููุฑู ุฅุฐุง ูุงูุช ุฏุงูุฉ ุงูุชูููู ุชุนูู ุจุดูู ุตุญูุญ:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

ูุฐู ููู ูุฑุชูุนุฉ ููุบุงูุฉ ููุฎุณุงุฑุฉ ูุงูุงุฑุชุจุงูุ ูููู ููุณ ูู ุงููุณุชุบุฑุจ ุฃููุง ูู ููู ุจุชุฏุฑูุจ ุงููููุฐุฌ ุจุนุฏ. ุจูุฐุงุ ูุฏููุง ูู ุดูุก ุฌุงูุฒ ููุชุงุจุฉ ุงูุฌุฒุก ุงูุฃุณุงุณู ูู ุณููุงุฑูู ุงูุชุฏุฑูุจ: ุญููุฉ ุงูุชุฏุฑูุจ. ูู ุญููุฉ ุงูุชุฏุฑูุจุ ููุฑุฑ ุงูุนูููุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูููุฑุฑ ุงููุฌููุนุงุช ุฅูู ุงููููุฐุฌ. ุจุงุณุชุฎุฏุงู ุงูููู ุงูููุฌุณุชูุฉุ ูููููุง ุจุนุฏ ุฐูู ุชูููู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงููุฎุตุตุฉ ูุฏููุง. ูููู ุจุถุจุท ุงูุฎุณุงุฑุฉ ุญุณุจ ุนุฏุฏ ุฎุทูุงุช ุชุฑุงูู ุงูุชุฏุฑุฌ ุญุชู ูุง ูุฎูู ุฎุณุงุฆุฑ ุฃูุจุฑ ุนูุฏ ุชุฌููุน ุงููุฒูุฏ ูู ุงูุฎุทูุงุช. ูุจู ุงูุชุญุณููุ ูููู ุฃูุถูุง ุจูุต ุงูุชุฏุฑุฌุงุช ูู ุฃุฌู ุชูุงุฑุจ ุฃูุถู. ูุฃุฎูุฑูุงุ ูู ุจุถุน ุฎุทูุงุช ูููู ุจุชูููู ุงููููุฐุฌ ุนูู ูุฌููุนุฉ ุงูุชูููู ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `evaluate()` ุงูุฌุฏูุฏุฉ ูุฏููุง:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

ููุฐุง ูู -- ูุฏูู ุงูุขู ุญููุฉ ุงูุชุฏุฑูุจ ุงููุฎุตุตุฉ ุงูุฎุงุตุฉ ุจู ููููุงุฐุฌ ุงููุบููุฉ ุงูุณุจุจูุฉ ูุซู GPT-2 ูุงูุชู ููููู ุชุฎุตูุตูุง ุฃูุซุฑ ุญุณุจ ุงุญุชูุงุฌุงุชู.

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ุฅูุง ุฃู ุชููู ุจุฅูุดุงุก ุฏุงูุฉ ุฎุณุงุฑุฉ ูุฎุตุตุฉ ุชูุงุณุจ ุญุงูุชู ุงูุงุณุชุฎุฏุงููุฉุ ุฃู ุฃุถู ุฎุทูุฉ ูุฎุตุตุฉ ุฃุฎุฑู ุฅูู ุญููุฉ ุงูุชุฏุฑูุจ.

</Tip>

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ุนูุฏ ุฅุฌุฑุงุก ุชุฌุงุฑุจ ุชุฏุฑูุจ ุทูููุฉุ ูู ุงูุฌูุฏ ุชุณุฌูู ุงูููุงููุณ ุงููููุฉ ุจุงุณุชุฎุฏุงู ุฃุฏูุงุช ูุซู TensorBoard ุฃู Weights & Biases. ุฃุถู ุชุณุฌูููุง ุตุญูุญูุง ุฅูู ุญููุฉ ุงูุชุฏุฑูุจ ุญุชู ููููู ุฏุงุฆููุง ุงูุชุญูู ูู ููููุฉ ุณูุฑ ุงูุชุฏุฑูุจ.

</Tip>

{/if}