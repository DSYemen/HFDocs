ุจุงูุชุฃููุฏุ ุณุฃุจุฏุฃ ุงูุชุฑุฌูุฉ ูู ุงูุชุนููู ุงูุชุงูู: 

## ุฌูุน ุงูุจูุงูุงุช 
ูุชููุฑ ููุฏ Python ุจููุฑุฉ ูู ูุณุชูุฏุนุงุช ุงูููุฏ ูุซู GitHubุ ูุงูุชู ูููููุง ุงุณุชุฎุฏุงููุง ูุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ุนู ุทุฑูู ุงูุจุญุซ ูู ูู ูุณุชูุฏุน Python. ูุงู ูุฐุง ูู ุงูููุฌ ุงููุชุจุน ูู ูุชุงุจ "Transformers" ูุชุนููู ูููุฐุฌ GPT-2 ูุจูุฑ. ุจุงุณุชุฎุฏุงู ุชูุฑูุบ GitHub ุจุญุฌู ุญูุงูู 180 ุฌูุฌุงุจุงูุช ูุญุชูู ุนูู ุญูุงูู 20 ููููู ููู Python ูุณูู "codeparrot"ุ ูุงู ุงููุคูููู ุจุจูุงุก ูุฌููุนุฉ ุจูุงูุงุช ูุงููุง ุจุนุฏ ุฐูู ุจูุดุงุฑูุชูุง ุนูู Hugging Face Hub. 

ููุน ุฐููุ ูุฅู ุงูุชุฏุฑูุจ ุนูู ุงููุฌููุนุฉ ุงููุงููุฉ ูุณุชุบุฑู ููุชูุง ุทูููุงู ููุณุชููู ุงููุซูุฑ ูู ููุงุฑุฏ ุงูุญูุณุจุฉุ ููุง ูุญุชุงุฌ ุณูู ุฅูู ุฌุฒุก ูู ุงููุฌููุนุฉ ุงููุนููุฉ ุจูุฌููุนุฉ Python ููุนููู. ูุฐููุ ุฏุนูุง ูุจุฏุฃ ุจุชุตููุฉ ูุฌููุนุฉ ุจูุงูุงุช "codeparrot" ููุญุตูู ุนูู ุฌููุน ุงููููุงุช ุงูุชู ุชุชุถูู ุฃููุง ูู ุงูููุชุจุงุช ุงูููุฌูุฏุฉ ูู ูุฐู ุงููุฌููุนุฉ. ูุธุฑูุง ูุญุฌู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุฑูุฏ ุชุฌูุจ ุชูุฒูููุงุ ุจุฏูุงู ูู ุฐููุ ุณูุณุชุฎุฏู ููุฒุฉ ุงูุจุซ ูุชุตููุฉ ุงูุจูุงูุงุช ุฃุซูุงุก ุงูุชููู. ููุณุงุนุฏุชูุง ูู ุชุตููุฉ ุนููุงุช ุงูููุฏ ุจุงุณุชุฎุฏุงู ุงูููุชุจุงุช ุงูุชู ุฐูุฑูุงูุง ุณุงุจููุงุ ุณูุณุชุฎุฏู ุงูุฏุงูุฉ ุงูุชุงููุฉ: 

ูุนุฏ ูุฐุง ูุซุงููุง ุฑุงุฆุนูุง ุนูู ููููุฉ ุงูุงุณุชูุงุฏุฉ ูู ุงููุญุชูู ุงููุชุงุญ ุจุญุฑูุฉ ุนูู ุงูููุจ ูุฅูุดุงุก ูุฌููุนุงุช ุจูุงูุงุช ูุฎุตุตุฉ ูุชุฏุฑูุจ ููุงุฐุฌ ุงููุบุฉ.
## ุฅุนุฏุงุฏ ูุฌููุนุฉ ุงูุจูุงูุงุช 

ุณุชููู ุงูุฎุทูุฉ ุงูุฃููู ูู ุชูุญูุฏ ุจูุงูุงุชูุงุ ุจุญูุซ ูููููุง ุงุณุชุฎุฏุงููุง ููุชุฏุฑูุจ. ูุธุฑูุง ูุฃู ูุฏููุง ูู ุงุณุชููุงู ุงุณุชุฏุนุงุกุงุช ุงูุฏูุงู ุงููุตูุฑุฉ ุจุดูู ุฃุณุงุณูุ ูููููุง ุงูุญูุงุธ ุนูู ุญุฌู ุงูุณูุงู ุตุบูุฑูุง ูุณุจููุง. ุชุชูุซู ููุฒุฉ ุฐูู ูู ุฃูู ูููููุง ุชุฏุฑูุจ ุงููููุฐุฌ ุจุดูู ุฃุณุฑุน ุจูุซูุฑุ ููุง ุฃูู ูุชุทูุจ ุฐุงูุฑุฉ ุฃูู ุจูุซูุฑ. ุฅุฐุง ูุงู ูู ุงูููู ูุชุทุจููู ุฃู ูููู ูู ุณูุงู ุฃูุจุฑ (ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ููุช ุชุฑูุฏ ุฃู ูููู ุงููููุฐุฌ ุจูุชุงุจุฉ ุงุฎุชุจุงุฑุงุช ุงููุญุฏุฉ ุจูุงุกู ุนูู ููู ุจุชุนุฑูู ุงูุฏุงูุฉ)ุ ูุชุฃูุฏ ูู ุฒูุงุฏุฉ ูุฐุง ุงูุฑููุ ูููู ุถุน ูู ุงุนุชุจุงุฑู ุฃูุถูุง ุฃู ูุฐุง ูุฃุชู ูุน ุจุตูุฉ ุฐุงูุฑุฉ GPU ุฃูุจุฑ. ุงูุขูุ ุฏุนููุง ูููู ุจุชุซุจูุช ุญุฌู ุงูุณูุงู ุนูุฏ 128 ุฑูุฒูุงุ ุนูู ุนูุณ 1024 ุฃู 2048 ุงููุณุชุฎุฏูุฉ ูู GPT-2 ุฃู GPT-3ุ ุนูู ุงูุชูุงูู.

ุชุญุชูู ูุนุธู ุงููุณุชูุฏุงุช ุนูู ุฃูุซุฑ ูู 128 ุฑูุฒูุงุ ูุฐุง ูุฅู ุงูุชุตุงุต ุงููุฏุฎูุงุช ุจุจุณุงุทุฉ ุฅูู ุงูุทูู ุงูุฃูุตู ูู ุดุฃูู ุฃู ููุถู ุนูู ุฌุฒุก ูุจูุฑ ูู ูุฌููุนุฉ ุจูุงูุงุชูุง. ุจุฏูุงู ูู ุฐููุ ุณูุณุชุฎุฏู ุฎูุงุฑ `return_overflowing_tokens` ูุชูุญูุฏ ุงูุฅุฏุฎุงู ุจุงููุงูู ูุชูุณููู ุฅูู ุนุฏุฉ ุฃุฌุฒุงุกุ ููุง ูุนููุง ูู [ุงููุตู 6](/course/chapter6/4). ุณูุณุชุฎุฏู ุฃูุถูุง ุฎูุงุฑ `return_length` ูุฅุฑุฌุงุน ุทูู ูู ุฌุฒุก ุชู ุฅูุดุงุคู ุชููุงุฆููุง. ุบุงูุจูุง ูุง ูููู ุงูุฌุฒุก ุงูุฃุฎูุฑ ุฃุตุบุฑ ูู ุญุฌู ุงูุณูุงูุ ูุณูุชุฎูุต ูู ูุฐู ุงููุทุน ูุชุฌูุจ ูุดููุงุช ุงูุญุดูุ ูุญู ูุง ูุญุชุงุฌูุง ุญููุง ูุฃู ูุฏููุง ุงููุซูุฑ ูู ุงูุจูุงูุงุช ุนูู ุฃู ุญุงู.

ุฏุนููุง ูุฑู ุจุงูุถุจุท ููู ูุนูู ูุฐุง ูู ุฎูุงู ุงููุธุฑ ูู ุฃูู ูุซุงููู:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
raw_datasets["train"][:2]["content"],
truncation=True,
max_length=context_length,
return_overflowing_tokens=True,
return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ูููููุง ุฃู ูุฑู ุฃููุง ูุญุตู ุนูู 34 ูุทุนุฉ ูู ุงููุฌููุน ูู ูุฐูู ุงููุซุงููู. ุจุงููุธุฑ ุฅูู ุฃุทูุงู ุงูุฃุฌุฒุงุกุ ูููููุง ุฃู ูุฑู ุฃู ุงูุฃุฌุฒุงุก ุงูููุฌูุฏุฉ ูู ููุงูุงุช ููุง ุงููุณุชูุฏูู ุชุญุชูู ุนูู ุฃูู ูู 128 ุฑูุฒูุง (117 ู41ุ ุนูู ุงูุชูุงูู). ุชูุซู ูุฐู ูุณุจุฉ ุตุบูุฑุฉ ุฌุฏูุง ูู ุฅุฌูุงูู ุงูุฃุฌุฒุงุก ุงูุชู ูุฏููุงุ ูุฐุง ูููููุง ุงูุชุฎูุต ูููุง ุจุฃูุงู. ุจุงุณุชุฎุฏุงู ุญูู `overflow_to_sample_mapping`ุ ูููููุง ุฃูุถูุง ุฅุนุงุฏุฉ ุจูุงุก ุงูุฃุฌุฒุงุก ุงูุชู ุชูุชูู ุฅูู ุนููุงุช ุงูุฅุฏุฎุงู.

ูุน ูุฐู ุงูุนูููุฉุ ูุณุชุฎุฏู ููุฒุฉ ูููุฏุฉ ููุธููุฉ `Dataset.map()` ูู ููุชุจุฉ ๐ค Datasetsุ ูุงูุชู ุชุชูุซู ูู ุฃููุง ูุง ุชุชุทูุจ ุฎุฑุงุฆุท ูุงุญุฏ ููุงุญุฏุ ููุง ุฑุฃููุง ูู [ุงููุณู 3](/course/chapter7/3)ุ ูููููุง ุฅูุดุงุก ุฏูุนุงุช ุชุญุชูู ุนูู ุนุฏุฏ ุฃูุจุฑ ุฃู ุฃูู ูู ุงูุนูุงุตุฑ ูู ุฏูุนุฉ ุงูุฅุฏุฎุงู. ูุฐุง ูููุฏ ุนูุฏ ุฅุฌุฑุงุก ุนูููุงุช ูุซู ุฒูุงุฏุฉ ุงูุจูุงูุงุช ุฃู ุชุตููุฉ ุงูุจูุงูุงุช ุงูุชู ุชุบูุฑ ุนุฏุฏ ุงูุนูุงุตุฑ. ูู ุญุงูุชูุงุ ุนูุฏ ุชูุญูุฏ ูู ุนูุตุฑ ุฅูู ุฃุฌุฒุงุก ุจุญุฌู ุงูุณูุงู ุงููุญุฏุฏุ ูููู ุจุฅูุดุงุก ุงูุนุฏูุฏ ูู ุงูุนููุงุช ูู ูู ูุณุชูุฏ. ูุญู ุจุญุงุฌุฉ ููุท ุฅูู ุงูุชุฃูุฏ ูู ุญุฐู ุงูุฃุนูุฏุฉ ุงูููุฌูุฏุฉุ ูุฃููุง ุฐุงุช ุญุฌู ูุชุนุงุฑุถ. ุฅุฐุง ุฃุฑุฏูุง ุงูุงุญุชูุงุธ ุจูุงุ ููููููุง ุชูุฑุงุฑูุง ุจุดูู ููุงุณุจ ูุฅุนุงุฏุชูุง ุฏุงุฎู ููุงููุฉ `Dataset.map()`

```py
def tokenize(element):
outputs = tokenizer(
element["content"],
truncation=True,
max_length=context_length,
return_overflowing_tokens=True,
return_length=True,
)
input_batch = []
for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
if length == context_length:
input_batch.append(input_ids)
return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
train: Dataset({
features: ['input_ids'],
num_rows: 16702061
})
valid: Dataset({
features: ['input_ids'],
num_rows: 93164
})
})
```

ุงูุขู ูุฏููุง 16.7 ููููู ูุซุงู ูุญุชูู ูู ูููุง ุนูู 128 ุฑูุฒูุงุ ููู ูุง ูุชูุงูู ูุน ุญูุงูู 2.1 ูููุงุฑ ุฑูุฒ ูู ุงููุฌููุน. ููููุงุฑูุฉุ ุชู ุชุฏุฑูุจ ููุงุฐุฌ GPT-3 ูCodex ูู OpenAI ุนูู 300 ู100 ูููุงุฑ ุฑูุฒ ุนูู ุงูุชูุงููุ ุญูุซ ุชู ุชููุฆุฉ ููุงุฐุฌ Codex ูู ููุงุท ุชูุชูุด GPT-3. ูุฏููุง ูู ูุฐุง ุงููุณู ููุณ ููุงูุณุฉ ูุฐู ุงูููุงุฐุฌุ ูุงูุชู ูููููุง ุฅูุดุงุก ูุตูุต ุทูููุฉ ููุชูุงุณูุฉุ ูููู ูุฅูุดุงุก ุฅุตุฏุงุฑ ูุตุบุฑ ูููุฑ ูุธููุฉ ุงุณุชููุงู ุชููุงุฆู ุณุฑูุนุฉ ูุนููุงุก ุงูุจูุงูุงุช.

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุฌุงูุฒุฉุ ุฏุนููุง ูููู ุจุฅุนุฏุงุฏ ุงููููุฐุฌ!

โ๏ธ **ุฌุฑุจู!** ูู ููู ุงูุชุฎูุต ูู ุฌููุน ุงูุฃุฌุฒุงุก ุงูุชู ุชููู ุฃุตุบุฑ ูู ุญุฌู ุงูุณูุงู ูุดููุฉ ูุจูุฑุฉ ููุง ูุฃููุง ูุณุชุฎุฏู ููุงูุฐ ุณูุงู ุตุบูุฑุฉ. ูุน ุฒูุงุฏุฉ ุญุฌู ุงูุณูุงู (ุฃู ุฅุฐุง ูุงู ูุฏูู ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช ุงููุตูุฑุฉ)ุ ูุฅู ูุณุจุฉ ุงูุฃุฌุฒุงุก ุงูุชู ูุชู ุงูุชุฎูุต ูููุง ุณุชุฒุฏุงุฏ ุฃูุถูุง. ููุงู ุทุฑููุฉ ุฃูุซุฑ ููุงุกุฉ ูุฅุนุฏุงุฏ ุงูุจูุงูุงุช ุชุชูุซู ูู ุถู ุฌููุน ุงูุนููุงุช ุงูููุญุฏุฉ ูู ุฏูุนุฉ ุจุงุณุชุฎุฏุงู ุฑูุฒ `eos_token_id` ุจูููุงุ ุซู ุฅุฌุฑุงุก ุงูุชูุณูู ุนูู ุงูุชุณูุณูุงุช ุงููุฏูุฌุฉ. ูููุงุฑุณุฉุ ุนุฏููู ูุธููุฉ `tokenize()` ูุงุณุชุฎุฏุงู ูุฐุง ุงูููุฌ. ูุงุญุธ ุฃูู ุชุฑูุฏ ุชุนููู `truncation=False` ูุฅุฒุงูุฉ ุงูุญุฌุฌ ุงูุฃุฎุฑู ูู ุงููุญูู ุงูุจุฑูุฌู ููุญุตูู ุนูู ุงูุชุณูุณู ุงููุงูู ููุนุฑูุงุช ุงูุฑููุฒ.
## ุชููุฆุฉ ูููุฐุฌ ุฌุฏูุฏ

ุชุชูุซู ุฎุทูุชูุง ุงูุฃููู ูู ุชููุฆุฉ ูููุฐุฌ GPT-2 ุฌุฏูุฏ ุชูุงููุง. ุณูุณุชุฎุฏู ููุณ ุงูุชููุฆุฉ ููููุฐุฌูุง ููุง ูู ุงููููุฐุฌ ุงูุตุบูุฑ ูู GPT-2ุ ูุฐุง ุณูููู ุจุชุญููู ุงูุชููุฆุฉ ุงููุณุจูุฉุ ูุงูุชุฃูุฏ ูู ุฃู ุญุฌู ุงูุฑููุฒ ููุงุซู ูุญุฌู ููุฑุฏุงุช ุงููููุฐุฌุ ูุฅุฑุณุงู ูุนุฑูุงุช ุงูุฑููุฒ "bos" ู"eos" (ุจุฏุงูุฉ ูููุงูุฉ ุงูุชุณูุณู):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
"gpt2",
vocab_size=len(tokenizer),
n_ctx=context_length,
bos_token_id=tokenizer.bos_token_id,
eos_token_id=tokenizer.eos_token_id,
)
```

ุจูุฐู ุงูุชููุฆุฉุ ูููููุง ุชุญููู ูููุฐุฌ ุฌุฏูุฏ. ูุงุญุธ ุฃู ูุฐู ูู ุงููุฑุฉ ุงูุฃููู ุงูุชู ูุง ูุณุชุฎุฏู ูููุง ุงูุฏุงูุฉ `from_pretrained()`ุ ูุฃููุง ูููู ุจุชููุฆุฉ ูููุฐุฌ ุจุฃููุณูุง:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
"gpt2",
vocab_size=len(tokenizer),
n_ctx=context_length,
bos_token_id=tokenizer.bos_token_id,
eos_token_id=tokenizer.eos_token_id,
)
```

ุจูุฐู ุงูุชููุฆุฉุ ูููููุง ุชุญููู ูููุฐุฌ ุฌุฏูุฏ. ูุงุญุธ ุฃู ูุฐู ูู ุงููุฑุฉ ุงูุฃููู ุงูุชู ูุง ูุณุชุฎุฏู ูููุง ุงูุฏุงูุฉ `from_pretrained()`ุ ูุฃููุง ูููู ุจุชููุฆุฉ ูููุฐุฌ ุจุฃููุณูุง:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

ูุญุชูู ูููุฐุฌูุง ุนูู 124 ููููู ูุนููุฉ ูุฌุจ ุถุจุทูุง. ูุจู ุฃู ูุชููู ูู ุจุฏุก ุงูุชุฏุฑูุจุ ูุญุชุงุฌ ุฅูู ุฅุนุฏุงุฏ ุฌุงูุน ุจูุงูุงุช ูููู ุจุฅูุดุงุก ุงูุฏูุนุงุช. ูููููุง ุงุณุชุฎุฏุงู ุฌุงูุน ุงูุจูุงูุงุช `DataCollatorForLanguageModeling`ุ ุงููุตูู ุฎุตูุตูุง ููุถุน ููุงุฐุฌ ุงููุบุฉ (ููุง ููุญู ุงูุงุณู ุจุดูู ุฎูู). ุจุงูุฅุถุงูุฉ ุฅูู ุชูุฏูุณ ุงูุฏูุนุงุช ููุณุงุฏุงุชูุงุ ูุฅูู ูููู ุฃูุถูุง ุจุฅูุดุงุก ุชุณููุงุช ูููุฐุฌ ุงููุบุฉ - ูู ูุถุน ุงููุบุฉ ุงูุณุจุจูุ ุชุนูู ุงูุฅุฏุฎุงูุงุช ูุนูุงูุงุช ุฃูุถูุง (ูุชุญููุฉ ุจููุฏุงุฑ ุนูุตุฑ ูุงุญุฏ)ุ ูููุดุฆ ุฌุงูุน ุงูุจูุงูุงุช ูุฐุง ุฃุซูุงุก ุงูุชุฏุฑูุจ ุญุชู ูุง ูุญุชุงุฌ ุฅูู ุชูุฑุงุฑ `input_ids`.

ูุงุญุธ ุฃู `DataCollatorForLanguageModeling` ูุฏุนู ูู ูู ูุถุน ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ (MLM) ููุถุน ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ (CLM). ุจุดูู ุงูุชุฑุงุถูุ ูุฅูู ูุนุฏ ุงูุจูุงูุงุช ูู MLMุ ูููู ูููููุง ุงูุชุจุฏูู ุฅูู CLM ุนู ุทุฑูู ุชุนููู ูุณูุท `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ูุซุงู:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

ูููููุง ุฃู ูุฑู ุฃู ุงูุฃูุซูุฉ ูุฏ ุชู ุชูุฏูุณูุง ูุฃู ุฌููุน ุงููุตูููุงุช ููุง ููุณ ุงูุดูู.

{#if fw === 'tf'}

ุงูุขู ูููููุง ุงุณุชุฎุฏุงู ุทุฑููุฉ `prepare_tf_dataset()` ูุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ุฅูู ูุฌููุนุงุช ุจูุงูุงุช TensorFlow ุจุงุณุชุฎุฏุงู ุฌุงูุน ุงูุจูุงูุงุช ุงูุฐู ุฃูุดุฃูุงู ุฃุนูุงู:

```python
tf_train_dataset = model.prepare_tf_dataset(
tokenized_datasets["train"],
collate_fn=data_collator,
shuffle=True,
batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
tokenized_datasets["valid"],
collate_fn=data_collator,
shuffle=False,
batch_size=32,
)
```

{/if}

<Tip warning={true}>

โ๏ธ ูุญุฏุซ ุชุญููู ุงูุฅุฏุฎุงูุงุช ูุงูุชุณููุงุช ูููุงุกูุชูุง ุฏุงุฎู ุงููููุฐุฌุ ูุฐุง ูุฅู ุฌุงูุน ุงูุจูุงูุงุช ูููู ููุท ุจูุณุฎ ุงูุฅุฏุฎุงูุงุช ูุฅูุดุงุก ุงูุชุณููุงุช.

</Tip>

ุงูุขู ูุฏููุง ูู ุดูุก ูู ููุงูู ูุชุฏุฑูุจ ูููุฐุฌูุง ุจุงููุนู - ูู ููู ุงูุฃูุฑ ุจูุฐุง ุงููุฏุฑ ูู ุงูุนูู ุจุนุฏ ูู ุดูุก! ูุจู ุฃู ูุจุฏุฃ ุงูุชุฏุฑูุจุ ูุฌุจ ุฃู ูููู ุจุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face. ุฅุฐุง ููุช ุชุนูู ูู ุฏูุชุฑ ููุงุญุธุงุชุ ูููููู ุงูููุงู ุจุฐูู ุจุงุณุชุฎุฏุงู ุฏุงูุฉ ุงููุณุงุนุฏุฉ ูุฐู:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ุณูุคุฏู ูุฐุง ุฅูู ุนุฑุถ ูุฑุจุน ุญูุงุฑ ููููู ูู ุฎูุงูู ุฅุฏุฎุงู ุจูุงูุงุช ุงุนุชูุงุฏ ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face.

ุฅุฐุง ููุช ูุง ุชุนูู ูู ุฏูุชุฑ ุงูููุงุญุธุงุชุ ููุง ุนููู ุณูู ูุชุงุจุฉ ุงูุณุทุฑ ุงูุชุงูู ูู ุงููุญุทุฉ ุงูุทุฑููุฉ ุงูุฎุงุตุฉ ุจู:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

ูู ูุง ุชุจูู ูู ุชูููู ุงูุญุฌุฌ ุงูุชุฏุฑูุจ ูุชุดุบูู `Trainer`. ุณูุณุชุฎุฏู ุฌุฏูู ุชุนูู ุฐุงุชู ุจูุนุฏู ุชุนูู ุฏูุฑู ูุน ุจุนุถ ุงูุงุญูุงุก ูุญุฌู ุฏูุนุฉ ูุนุงู ูุจูุบ 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). ูุณุชุฎุฏู ุชุฑุงูู ุงูุชุฏุฑุฌุงุช ุนูุฏูุง ูุง ุชุชูุงุณุจ ุฏูุนุฉ ูุงุญุฏุฉ ูุน ุงูุฐุงูุฑุฉุ ููุจูู ุชุฏุฑูุฌููุง ุงูุชุฏุฑุฌ ูู ุฎูุงู ุนุฏุฉ ุชูุฑูุฑุงุช ููุฃูุงู ูุงูุฎูู. ุณูุฑู ูุฐุง ูู ุงูุนูู ุนูุฏ ุฅูุดุงุก ุญููุฉ ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ๐ค Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
output_dir="codeparrot-ds",
per_device_train_batch_size=32,
per_device_eval_batch_size=32,
evaluation_strategy="steps",
eval_steps=5_000,
logging_steps=5_000,
gradient_accumulation_steps=8,
num_train_epochs=1,
weight_decay=0.1,
warmup_steps=1_000,
lr_scheduler_type="cosine",
learning_rate=5e-4,
save_steps=5_000,
fp16=True,
push_to_hub=True,
)

trainer = Trainer(
model=model,
tokenizer=tokenizer,
args=args,
data_collator=data_collator,
train_dataset=tokenized_datasets["train"],
eval_dataset=tokenized_datasets["valid"],
)
```

ุงูุขู ูููููุง ุจุจุณุงุทุฉ ุจุฏุก ุชุดุบูู `Trainer` ูุงูุชุธุฑ ุญุชู ููุชูู ุงูุชุฏุฑูุจ. ุงุนุชูุงุฏูุง ุนูู ูุง ุฅุฐุง ููุช ุชุดุบูู ุนูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉ ุฃู ุฌุฒุก ูููุงุ ุณูุณุชุบุฑู ูุฐุง 20 ุฃู ุณุงุนุชูู ุนูู ุงูุชูุงููุ ูุฐุง ุงุญุตู ุนูู ุจุนุถ ุงููููุฉ ููุชุงุจ ุฌูุฏ ูููุฑุงุกุฉ!

```py
trainer.train()
```

ุจุนุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ูููููุง ุฏูุน ุงููููุฐุฌ ูุงููุญูู ุฅูู ุงููุฑูุฒ:

```py
trainer.push_to_hub()
```

{:else}

ูู ูุง ุชุจูู ูู ุชูููู ูุฑุท ุงููุนููุงุช ุงูุชุฏุฑูุจ ูุงุณุชุฏุนุงุก `compile()` ู`fit()`. ุณูุณุชุฎุฏู ุฌุฏูู ุชุนูู ุฐุงุชู ุจูุนุฏู ุชุนูู ุฏูุฑู ูุน ุจุนุถ ุงูุงุญูุงุก ูุชุญุณูู ุงุณุชูุฑุงุฑ ุงูุชุฏุฑูุจ:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
init_lr=5e-5,
num_warmup_steps=1_000,
num_train_steps=num_train_steps,
weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ุงูุขู ูููููุง ุจุจุณุงุทุฉ ุงุณุชุฏุนุงุก `model.fit()` ูุงูุชุธุฑ ุญุชู ููุชูู ุงูุชุฏุฑูุจ. ุงุนุชูุงุฏูุง ุนูู ูุง ุฅุฐุง ููุช ุชุดุบูู ุนูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉ ุฃู ุฌุฒุก ูููุงุ ุณูุณุชุบุฑู ูุฐุง 20 ุฃู ุณุงุนุชูู ุนูู ุงูุชูุงููุ ูุฐุง ุงุญุตู ุนูู ุจุนุถ ุงููููุฉ ููุชุงุจ ุฌูุฏ ูููุฑุงุกุฉ! ุจุนุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ูููููุง ุฏูุน ุงููููุฐุฌ ูุงููุญูู ุฅูู ุงููุฑูุฒ:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

โ๏ธ **ุฌุฑุจู!** ุงุณุชุบุฑู ุงูุฃูุฑ ููุง ุญูุงูู 30 ุณุทุฑูุง ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุจุงูุฅุถุงูุฉ ุฅูู `TrainingArguments` ููุงูุชูุงู ูู ุงููุตูุต ุงูุฎุงู ุฅูู ุชุฏุฑูุจ GPT-2. ุฌุฑุจู ุจุงุณุชุฎุฏุงู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ูุดุงูุฏ ูุง ุฅุฐุง ูุงู ุจุฅููุงูู ุงูุญุตูู ุนูู ูุชุงุฆุฌ ุฌูุฏุฉ!

</Tip>

<Tip>

{#if fw === 'pt'}

๐ก ุฅุฐุง ูุงู ูุฏูู ุฅููุงููุฉ ุงููุตูู ุฅูู ุฌูุงุฒ ุจู ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณููุงุช ูุชุนุฏุฏุฉุ ูุญุงูู ุชุดุบูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ููุงู. ูููู `Trainer` ุจุฅุฏุงุฑุฉ ุฃุฌูุฒุฉ ูุชุนุฏุฏุฉ ุชููุงุฆููุงุ ููููู ุฃู ูุณุฑุน ุงูุชุฏุฑูุจ ุจุดูู ูุจูุฑ.

{:else}

๐ก ุฅุฐุง ูุงู ูุฏูู ุฅููุงููุฉ ุงููุตูู ุฅูู ุฌูุงุฒ ุจู ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณููุงุช ูุชุนุฏุฏุฉุ ูููููู ุชุฌุฑุจุฉ ุงุณุชุฎุฏุงู ุณูุงู `MirroredStrategy` ููุชุณุฑูุน ุจุดูู ูุจูุฑ ูู ุงูุชุฏุฑูุจ. ุณุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุงุฆู `tf.distribute.MirroredStrategy`ุ ูุงูุชุฃูุฏ ูู ุชุดุบูู ุฃู ุทุฑู `to_tf_dataset()` ุฃู `prepare_tf_dataset()` ุจุงูุฅุถุงูุฉ ุฅูู ุฅูุดุงุก ุงููููุฐุฌ ูุงุณุชุฏุนุงุก `fit()` ุฌููุนูุง ูู ุณูุงู `scope()` ุงูุฎุงุต ุจู. ููููู ุงูุงุทูุงุน ุนูู ุงููุซุงุฆู ุงููุชุนููุฉ ุจูุฐุง ุงูุฃูุฑ [ููุง](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## ุชูููุฏ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ูุน ุฎุท ุฃูุงุจูุจ

ุงูุขู ูู ูุญุธุฉ ุงูุญูููุฉ: ุฏุนููุง ูุฑู ูุฏู ูุฌุงุญ ุงููููุฐุฌ ุงููุฏุฑุจ ุจุงููุนู! ูููููุง ุฃู ูุฑู ูู ุงูุณุฌูุงุช ุฃู ุงูุฎุณุงุฑุฉ ุงูุฎูุถุช ุจุซุจุงุชุ ูููู ููุถุน ุงููููุฐุฌ ุนูู ุงููุญูุ ุฏุนููุง ูุฑู ูุฏู ูุฌุงุญู ูู ุจุนุถ ุงููุทุงูุจุงุช. ููููุงู ุจุฐููุ ุณูููู ุจุชุบููู ุงููููุฐุฌ ูู ุฎุท ุฃูุงุจูุจ ุชูููุฏ ุงููุตุ ูุณูุถุนู ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ููุญุตูู ุนูู ุฃุฌูุงู ุณุฑูุนุฉ ุฅุฐุง ูุงู ููุงู ูุญุฏุฉ ูุนุงูุฌุฉ ุฑุณููุงุช ูุชุงุญุฉ:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
"text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
"text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

ุฏุนููุง ูุจุฏุฃ ุจุงููููุฉ ุงูุจุณูุทุฉ ุงููุชูุซูุฉ ูู ุฅูุดุงุก ูุฎุทุท ุชุดุชุช:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

ุชุจุฏู ุงููุชูุฌุฉ ุตุญูุญุฉ. ูู ูุนูู ุฃูุถูุง ูุนูููุฉ `pandas`ุ ุฏุนููุง ูุฑู ูุง ุฅุฐุง ูุงู ุจุฅููุงููุง ุฅูุดุงุก `DataFrame` ูู ุตููููู:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

ุฑุงุฆุนุ ูุฐุง ูู ุงูุฌูุงุจ ุงูุตุญูุญ - ุนูู ุงูุฑุบู ูู ุฃูู ุจุนุฏ ุฐูู ูููู ุจุฅุฏุฑุงุฌ ุนููุฏ "x" ูุฑุฉ ุฃุฎุฑู. ูุธุฑูุง ูุฃู ุนุฏุฏ ุงูุฑููุฒ ุงููููุฏุฉ ูุญุฏูุฏุ ูุชู ูุทุน ุญููุฉ "for" ุงูุชุงููุฉ. ุฏุนููุง ูุฑู ูุง ุฅุฐุง ูุงู ุจุฅููุงููุง ุงูููุงู ุจุดูุก ุฃูุซุฑ ุชุนููุฏูุง ูุฌุนู ุงููููุฐุฌ ูุณุงุนุฏูุง ูู ุงุณุชุฎุฏุงู ุนูููุฉ `groupby`:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

ููุณ ุณูุฆูุงุ ูุฐู ูู ุงูุทุฑููุฉ ุงูุตุญูุญุฉ ููููุงู ุจุฐูู. ุฃุฎูุฑูุงุ ุฏุนููุง ูุฑู ูุง ุฅุฐุง ูุงู ุจุฅููุงููุง ุฃูุถูุง ุงุณุชุฎุฏุงูู ูู `scikit-learn` ูุฅุนุฏุงุฏ ูููุฐุฌ Random Forest:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

ูู ุฎูุงู ุงููุธุฑ ูู ูุฐู ุงูุฃูุซูุฉ ุงูููููุฉุ ูุจุฏู ุฃู ุงููููุฐุฌ ูุฏ ุชุนูู ุจุนุถูุง ูู ุจูุงุก ุฌููุฉ ูุฌููุนุฉ ุฃุฏูุงุช ุนูู ุงูุจูุงูุงุช ูู Python. ุจุงูุทุจุนุ ุณูุชุนูู ุนูููุง ุชูููู ุงููููุฐุฌ ุจุดูู ุฃูุซุฑ ุดูููุงู ูุจู ูุดุฑู ูู ุงูุนุงูู ุงูุญููููุ ูููู ูุฐุง ูููุฐุฌ ุฃููู ูุซูุฑ ููุฅุนุฌุงุจ.

{:else}

ูู ุฎูุงู ุงููุธุฑ ูู ูุฐู ุงูุฃูุซูุฉ ุงูููููุฉุ ูุจุฏู ุฃู ุงููููุฐุฌ ูุฏ ุชุนูู ุจุนุถูุง ูู ุจูุงุก ุฌููุฉ ูุฌููุนุฉ ุฃุฏูุงุช ุนูู ุงูุจูุงูุงุช ูู Python (ุจุงูุทุจุนุ ุณ
## ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ๐ค Accelerate

ููุฏ ุฑุฃููุง ููู ูููู ุจุชุฏุฑูุจ ูููุฐุฌ ุจุงุณุชุฎุฏุงู `Trainer`ุ ูุงูุฐู ูุณูุญ ุจุจุนุถ ุงูุชุฎุตูุต. ููุน ุฐููุ ูู ุจุนุถ ุงูุฃุญูุงู ูุฑูุฏ ุงูุชุญูู ุงููุงูู ูู ุญููุฉ ุงูุชุฏุฑูุจุ ุฃู ูุฑูุฏ ุฅุฌุฑุงุก ุจุนุถ ุงูุชุบููุฑุงุช ุงูุบุฑูุจุฉ. ูู ูุฐู ุงูุญุงูุฉุ ูุนุฏ ๐ค Accelerate ุฎูุงุฑูุง ุฑุงุฆุนูุงุ ููู ูุฐุง ุงููุณูุ ุณููุฑ ุนุจุฑ ุงูุฎุทูุงุช ุงููุงุฒูุฉ ูุงุณุชุฎุฏุงูู ูุชุฏุฑูุจ ูููุฐุฌูุง. ูุฌุนู ุงูุฃููุฑ ุฃูุซุฑ ุฅุซุงุฑุฉ ููุงูุชูุงูุ ุณูุถูู ุฃูุถูุง ููุฉ ุฅูู ุญููุฉ ุงูุชุฏุฑูุจ.

<Youtube id="Hm8_PgVTFuc"/>

ูุธุฑูุง ูุฃููุง ููุชููู ุจุดูู ุฃุณุงุณู ุจุงูุงูุชูุงู ุงูุชููุงุฆู ุงููุนููู ูููุชุจุงุช ุนููู ุงูุจูุงูุงุชุ ููู ุงูููุทูู ุฅุนุทุงุก ูุฒู ุฃูุจุฑ ูุนููุงุช ุงูุชุฏุฑูุจ ุงูุชู ุชุณุชุฎุฏู ูุฐู ุงูููุชุจุงุช ุจุดูู ุฃูุจุฑ. ูููููุง ุงูุชุนุฑู ุจุณูููุฉ ุนูู ูุฐู ุงูุฃูุซูุฉ ูู ุฎูุงู ุงุณุชุฎุฏุงู ูููุงุช ุฑุฆูุณูุฉ ูุซู `plt` ู`pd` ู`sk` ู`fit` ู`predict`ุ ูุงูุชู ุชุนุฏ ุฃูุซุฑ ุฃุณูุงุก ุงูุงุณุชูุฑุงุฏ ุดููุนูุง ูู `matplotlib.pyplot` ู`pandas` ู`sklearn`ุ ุจุงูุฅุถุงูุฉ ุฅูู ููุท fit/predict ุงูุฃุฎูุฑ. ุฅุฐุง ุชู ุชูุซูู ูู ูููุง ูุฑููุฒ ูููุฒุฉ ูุงุญุฏุฉุ ููููููุง ุงูุชุญูู ุจุณูููุฉ ููุง ุฅุฐุง ูุงูุช ุชุญุฏุซ ูู ุชุณูุณู ุงูุฅุฏุฎุงู. ูููู ุฃู ูููู ููุฑููุฒ ุงููููุฒุฉ ุจุงุฏุฆุฉ ูุณุงูุฉ ุจูุถุงุกุ ูุฐุง ูุณูุชุญูู ุฃูุถูุง ูู ูุฐู ุงูุฅุตุฏุงุฑุงุช ูู ูุงููุณ ููุฑุฏุงุช ุงููุญูู ุงููุบูู. ููุชุญูู ูู ุฃููุง ุชุนููุ ุณูุถูู ุฑูุฒูุง ูููุฒูุง ูุงุญุฏูุง ููุงุฎุชุจุงุฑ ูุฌุจ ุชูุณููู ุฅูู ุนุฏุฉ ุฑููุฒ ูููุฒุฉ:

```py
keytoken_ids = []
for keyword in [
"plt",
"pd",
"sk",
"fit",
"predict",
" plt",
" pd",
" sk",
" fit",
" predict",
"testtest",
]:
ids = tokenizer([keyword]).input_ids[0]
if len(ids) == 1:
keytoken_ids.append(ids[0])
else:
print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

ุฑุงุฆุนุ ูุจุฏู ุฃู ูุฐุง ูุนูู ุจุดูู ุฌูุฏ! ุงูุขู ูููููุง ูุชุงุจุฉ ุฏุงูุฉ ุฎุณุงุฑุฉ ูุฎุตุตุฉ ุชุฃุฎุฐ ุชุณูุณู ุงูุฅุฏุฎุงูุ ูุงูุงุญุชูุงูุงุชุ ูุงูุฑููุฒ ุงููููุฒุฉ ุงูุฑุฆูุณูุฉ ุงูุชู ุญุฏุฏูุงูุง ููุฏุฎูุงุช. ุฃููุงูุ ูุญุชุงุฌ ุฅูู ูุญุงุฐุงุฉ ุงูุงุญุชูุงูุงุช ูุงููุฏุฎูุงุช: ูุดูู ุชุณูุณู ุงูุฅุฏุฎุงู ุงูููุฒุงุญ ุฅูู ุงููููู ุจููุฏุงุฑ ูุงุญุฏ ุงูุชุณููุงุชุ ูุธุฑูุง ูุฃู ุงูุฑูุฒ ุงููููุฒ ุงูุชุงูู ูู ุงูุชุณููุฉ ููุฑูุฒ ุงููููุฒ ุงูุญุงูู. ูููููุง ุชุญููู ุฐูู ุนู ุทุฑูู ุจุฏุก ุงูุชุณููุงุช ูู ุงูุฑูุฒ ุงููููุฒ ุงูุซุงูู ูุชุณูุณู ุงูุฅุฏุฎุงูุ ูุธุฑูุง ูุฃู ุงููููุฐุฌ ูุง ูููู ุจุชููุน ููุฑูุฒ ุงููููุฒ ุงูุฃูู ุนูู ุฃู ุญุงู. ุซู ููุทุน ุงูุงุญุชูุงู ุงูุฃุฎูุฑุ ุญูุซ ูุง ุชูุฌุฏ ุชุณููุฉ ููุฑูุฒ ุงููููุฒ ุงูุฐู ูุชุจุน ุชุณูุณู ุงูุฅุฏุฎุงู ุงููุงูู. ุจูุฐู ุงูุทุฑููุฉุ ูููููุง ุญุณุงุจ ุงูุฎุณุงุฑุฉ ููู ุนููุฉ ูุญุณุงุจ ุชูุฑุงุฑ ุฌููุน ุงููููุงุช ุงูุฑุฆูุณูุฉ ูู ูู ุนููุฉ. ุฃุฎูุฑูุงุ ูุญุณุจ ุงููุชูุณุท ุงููุฑุฌุญ ูุฌููุน ุงูุนููุงุช ุจุงุณุชุฎุฏุงู ุงูุชูุฑุงุฑุงุช ูุฃูุฒุงู. ูุธุฑูุง ูุฃููุง ูุง ูุฑูุฏ ุงูุชุฎูุต ูู ุฌููุน ุงูุนููุงุช ุงูุชู ูุง ุชุญุชูู ุนูู ูููุงุช ุฑุฆูุณูุฉุ ููุถูู 1 ุฅูู ุงูุฃูุฒุงู:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
# Shift so that tokens < n predict n
shift_labels = inputs[..., 1:].contiguous()
shift_logits = logits[..., :-1, :].contiguous()
# Calculate per-token loss
loss_fct = CrossEntropyLoss(reduce=False)
loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
# Resize and average loss per sample
loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
# Calculate and scale weighting
weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
axis=[0, 2]
)
weights = alpha * (1.0 + weights)
# Calculate weighted average
weighted_loss = (loss_per_sample * weights).mean()
return weighted_loss
```

ูุจู ุฃู ูุชููู ูู ุจุฏุก ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุฌุฏูุฏุฉ ุงูุฑุงุฆุนุฉ ูุฐูุ ููุฒู ุฅุนุฏุงุฏ ุจุนุถ ุงูุฃููุฑ:

- ูุญุชุงุฌ ุฅูู ูุญููุงุช ุงูุจูุงูุงุช ูุชุญููู ุงูุจูุงูุงุช ูู ุฏูุนุงุช.
- ูุญุชุงุฌ ุฅูู ุฅุนุฏุงุฏ ูุนููุงุช ุงูุญูุงู ุงููุฒู.
- ูุฑูุฏ ุงูุชูููู ูู ููุช ูุขุฎุฑุ ูุฐุง ููู ุงูููุทูู ูู ูุธููุฉ ุงูุชูููู ูู ุฏุงูุฉ.

ููุจุฏุฃ ุจูุญููุงุช ุงูุจูุงูุงุช. ูู ูุง ูุญุชุงุฌู ูู ุชุนููู ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู `"torch"`ุ ุซู ูููููุง ุชูุฑูุฑู ุฅูู PyTorch `DataLoader` ุจุญุฌู ุงูุฏูุนุฉ ุงูููุงุณุจ:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

ุจุนุฏ ุฐููุ ูููู ุจุชุฌูุน ุงููุนููุงุช ุจุญูุซ ูุนุฑู ุงููุญุณู ุฃููุง ุณูุญุตู ุนูู ุงูุญูุงู ูุฒู ุฅุถุงูู. ุนุงุฏุฉุ ูุชู ุฅุนูุงุก ุฌููุน ูุตุทูุญุงุช ุงูุงูุญูุงุฒ ูLayerNorm weights ูู ูุฐุงุ ุฅููู ููููุฉ ุงูููุงู ุจุฐูู:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
params_with_wd, params_without_wd = [], []
for n, p in model.named_parameters():
if any(nd in n for nd in no_decay):
params_without_wd.append(p)
else:
params_with_wd.append(p)
return [
{"params": params_with_wd, "weight_decay": weight_decay},
{"params": params_without_wd, "weight_decay": 0.0},
]
```

ูุธุฑูุง ูุฃููุง ูุฑูุฏ ุชูููู ุงููููุฐุฌ ุจุงูุชุธุงู ุนูู ูุฌููุนุฉ ุงูุชุญูู ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุฏุนููุง ููุชุจ ุฏุงูุฉ ูุฐูู ุฃูุถูุง. ููู ูููู ุจุจุณุงุทุฉ ุจุชุดุบูู ูุญูู ุจูุงูุงุช ุงูุชูููู ูุฌูุน ุฌููุน ุงูุฎุณุงุฆุฑ ุนุจุฑ ุงูุนูููุงุช:

```py
def evaluate():
model.eval()
losses = []
for step, batch in enumerate(eval_dataloader):
with torch.no_grad():
outputs = model(batch["input_ids"], labels=batch["input_ids"])

losses.append(accelerator.gather(outputs.loss))
loss = torch.mean(torch.cat(losses))
try:
perplexity = torch.exp(loss)
except OverflowError:
perplexity = float("inf")
return loss.item(), perplexity.item()
```

ูุน ุฏุงูุฉ `evaluate()`ุ ูููููุง ุงูุฅุจูุงุบ ุนู ุงูุฎุณุงุฑุฉ ู [perplexity](/course/chapter7/3) ูู ูุชุฑุงุช ููุชุธูุฉ. ุจุนุฏ ุฐููุ ูุนูุฏ ุชุนุฑูู ูููุฐุฌูุง ููุชุฃูุฏ ูู ุฃููุง ูุชุฏุฑุจ ูู ุงูุตูุฑ ูุฑุฉ ุฃุฎุฑู:

```py
model = GPT2LMHeadModel(config)
```

ุจุนุฏ ุฐููุ ูููููุง ุชุนุฑูู ูุญุณููุงุ ุจุงุณุชุฎุฏุงู ุงูุฏุงูุฉ ูู ูุจู ูุชูุณูู ุงููุนููุงุช ูุงูุญูุงู ุงููุฒู:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ุงูุขู ุฏุนููุง ูุนุฏ ุงููููุฐุฌ ูุงููุญุณู ููุญููุงุช ุงูุจูุงูุงุช ุญุชู ูุชููู ูู ุจุฏุก ุงูุชุฏุฑูุจ:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>
๐จ ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู TPUุ ูุณุชุญุชุงุฌ ุฅูู ููู ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุจุฏุกูุง ูู ุงูุฎููุฉ ุฃุนูุงู ุฅูู ุฏุงูุฉ ุชุฏุฑูุจ ูุฎุตุตุฉ. ุฑุงุฌุน [ุงููุตู 3](/course/chapter3) ููุฒูุฏ ูู ุงูุชูุงุตูู.
</Tip>

ุงูุขู ุจุนุฏ ุฃู ุฃุฑุณููุง `train_dataloader` ุฅูู `accelerator.prepare()`ุ ูููููุง ุงุณุชุฎุฏุงู ุทููู ูุญุณุงุจ ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ. ุชุฐูุฑ ุฃูู ูุฌุจ ุนูููุง ุฏุงุฆููุง ุงูููุงู ุจุฐูู ุจุนุฏ ุฅุนุฏุงุฏ ูุญูู ุงูุจูุงูุงุชุ ุญูุซ ุณุชุบูุฑ ูุฐู ุงูุทุฑููุฉ ุทููู. ูุณุชุฎุฏู ุฌุฏูููุง ุฎุทููุง ููุงุณููููุง ูู ูุนุฏู ุงูุชุนูู ุฅูู 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
name="linear",
optimizer=optimizer,
num_warmup_steps=1_000,
num_training_steps=num_training_steps,
)
```

ุฃุฎูุฑูุงุ ูุฏูุน ูููุฐุฌูุง ุฅูู Hubุ ุณูุชุนูู ุนูููุง ุฅูุดุงุก ูุงุฆู `Repository` ูู ูุฌูุฏ ุนูู. ูู ุจุชุณุฌูู ุงูุฏุฎูู ุฃููุงู ุฅูู Hub Hugging Faceุ ุฅุฐุง ูู ุชูู ูุฏ ุณุฌูุช ุงูุฏุฎูู ุจุงููุนู. ุณูุญุฏุฏ ุงุณู ุงููุณุชูุฏุน ูู ูุนุฑู ุงููููุฐุฌ ุงูุฐู ูุฑูุฏ ููุญู ููููุฐุฌูุง (ูุง ุชุชุฑุฏุฏ ูู ุงุณุชุจุฏุงู `repo_name` ุจุฎูุงุฑู ุงูุฎุงุตุ ูู ูุง ูุญุชุงุฌู ูู ุงุญุชูุงุก ุงุณู ุงููุณุชุฎุฏู ุงูุฎุงุต ุจูุ ููู ูุง ุชูุนูู ูุธููุฉ `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ุจุนุฏ ุฐููุ ูููููุง ุงุณุชูุณุงุฎ ูุฐุง ุงููุณุชูุฏุน ูู ูุฌูุฏ ูุญูู. ุฅุฐุง ูุงู ููุฌูุฏูุง ุจุงููุนูุ ููุฌุจ ุฃู ูููู ูุฐุง ุงููุฌูุฏ ุงููุญูู ูุณุชูุณุฎูุง ููุฌูุฏูุง ูููุณุชูุฏุน ุงูุฐู ูุนูู ูุนู:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ุงูุขู ูููููุง ุชุญููู ุฃู ุดูุก ูููู ุจุญูุธู ูู `output_dir` ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `repo.push_to_hub()`. ุณูุณุงุนุฏูุง ูุฐุง ูู ุชุญููู ุงูููุงุฐุฌ ุงููุชูุณุทุฉ ูู ููุงูุฉ ูู ูุชุฑุฉ.

ูุจู ุงูุชุฏุฑูุจุ ุฏุนููุง ูุฌุฑู ุงุฎุชุจุงุฑูุง ุณุฑูุนูุง ููุนุฑูุฉ ูุง ุฅุฐุง ูุงูุช ุฏุงูุฉ ุงูุชูููู ุชุนูู ุจุดูู ุตุญูุญ:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

ุชูู ููู ุนุงููุฉ ุฌุฏูุง ููุฎุณุงุฑุฉ ูุงูุงุฑุชุจุงูุ ูููู ููุณ ูู ุงููุณุชุบุฑุจ ุฃููุง ูู ููู ุจุชุฏุฑูุจ ุงููููุฐุฌ ุจุนุฏ. ุจูุฐุงุ ุฃุตุจุญ ูู ุดูุก ุฌุงูุฒูุง ููุชุงุจุฉ ุงูุฌุฒุก ุงูุฃุณุงุณู ูู ูุต ุงูุจุฑูุงูุฌ ุงููุตู ููุชุฏุฑูุจ: ุญููุฉ ุงูุชุฏุฑูุจ. ูู ุญููุฉ ุงูุชุฏุฑูุจุ ูููู ุจุงูุชุนููู ุนูู ูุญูู ุงูุจูุงูุงุช ูููุฑุฑ ุงูุฏูุนุงุช ุฅูู ุงููููุฐุฌ. ุจุงุณุชุฎุฏุงู ุงูุงุญุชูุงูุงุชุ ูููููุง ุจุนุฏ ุฐูู ุชูููู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงููุฎุตุตุฉ ูุฏููุง. ูููู ุจุถุจุท ุงูุฎุณุงุฑุฉ ุนู ุทุฑูู ุนุฏุฏ ุฎุทูุงุช ุชุฌููุน ุงูุชุฏุฑุฌุงุช ุญุชู ูุง ูููู ุจุฅูุดุงุก ุฎุณุงุฆุฑ ุฃูุจุฑ ุนูุฏ ุชุฌููุน ุงููุฒูุฏ ูู ุงูุฎุทูุงุช. ูุจู ุงูุชุญุณููุ ูููู ุฃูุถูุง ุจูุต ุงูุชุฏุฑุฌุงุช ููุชูุงุฑุจ ุงูุฃูุถู. ูุฃุฎูุฑูุงุ ูููู ุจุชูููู ุงููููุฐุฌ ุนูู ูุฌููุนุฉ ุงูุชูููู ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `evaluate()` ุงูุฌุฏูุฏุฉ ูู ุจุถุน ุฎุทูุงุช:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
for step, batch in tqdm(
enumerate(train_dataloader, start=1), total=num_training_steps
):
logits = model(batch["input_ids"]).logits
loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
if step % 100 == 0:
accelerator.print(
{
"samples": step * samples_per_step,
"steps": completed_steps,
"loss/train": loss.item() * gradient_accumulation_steps,
}
)
loss = loss / gradient_accumulation_steps
accelerator.backward(loss)
if step % gradient_accumulation_steps == 0:
accelerator.clip_grad_norm_(model.parameters(), 1.0)
optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
completed_steps += 1
if (step % (eval_steps * gradient_accumulation_steps)) == 0:
eval_loss, perplexity = evaluate()
accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
model.train()
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
if accelerator.is_main_process:
tokenizer.save_pretrained(output_dir)
repo.push_to_hub(
commit_message=f"Training in progress step {step}", blocking=False
)
```

ููุฐุง ูู ุดูุก - ูุฏูู ุงูุขู ุญููุฉ ุชุฏุฑูุจ ูุฎุตุตุฉ ุฎุงุตุฉ ุจู ููููุงุฐุฌ ุงููุบููุฉ ุงูุณุจุจูุฉ ูุซู GPT-2 ูุงูุชู ููููู ุชุฎุตูุตูุง ุฃูุซุฑ ูุชูุจูุฉ ุงุญุชูุงุฌุงุชู.

<Tip>
โ๏ธ **ุฌุฑุจู!** ุฅูุง ุฅูุดุงุก ุฏุงูุฉ ุฎุณุงุฑุฉ ูุฎุตุตุฉ ุฎุงุตุฉ ุจู ูุตููุฉ ูุญุงูุชู ุงูุงุณุชุฎุฏุงูุ ุฃู ุฅุถุงูุฉ ุฎุทูุฉ ูุฎุตุตุฉ ุฃุฎุฑู ุฅูู ุญููุฉ ุงูุชุฏุฑูุจ.
</Tip>

<Tip>
โ๏ธ **ุฌุฑุจู!** ุนูุฏ ุชุดุบูู ุชุฌุงุฑุจ ุงูุชุฏุฑูุจ ุงูุทูููุฉุ ูู ุงูุฌูุฏ ุชุณุฌูู ุงูููุงููุณ ุงููููุฉ ุจุงุณุชุฎุฏุงู ุฃุฏูุงุช ูุซู TensorBoard ุฃู Weights & Biases. ุฃุถู ุชุณุฌูู ุงูุฏุฎูู ุงูููุงุณุจ ุฅูู ุญููุฉ ุงูุชุฏุฑูุจ ุญุชู ุชุชููู ุฏุงุฆููุง ูู ุงูุชุญูู ูู ููููุฉ ุณูุฑ ุงูุชุฏุฑูุจ.
</Tip>

{/if}