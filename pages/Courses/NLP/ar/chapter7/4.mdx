<FrameworkSwitchCourse {fw} />

# الترجمة [[translation]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
]} />

{/if}

دعونا الآن نغوص في الترجمة. هذه هي مهمة أخرى [من تسلسل إلى تسلسل](/course/chapter1/7)، مما يعني أنها مشكلة يمكن صياغتها على أنها الانتقال من تسلسل إلى آخر. بهذا المعنى، فإن المشكلة قريبة جدًا من [التلخيص](/course/chapter7/6)، ويمكنك تكييف ما سنراه هنا مع مشاكل أخرى من تسلسل إلى تسلسل مثل:

- **نقل الأسلوب**: إنشاء نموذج يقوم ب*ترجمة* النصوص المكتوبة بأسلوب معين إلى أسلوب آخر (على سبيل المثال، من الرسمي إلى العادي أو من الإنجليزية الشكسبيرية إلى الإنجليزية الحديثة)
- **الإجابة على الأسئلة التوليدية**: إنشاء نموذج يقوم بتوليد إجابات للأسئلة، بالنظر إلى السياق

<Youtube id="1JvfrvZgi6c"/>

إذا كان لديك مجموعة كبيرة بما يكفي من النصوص بلغتين (أو أكثر)، يمكنك تدريب نموذج ترجمة جديد من الصفر مثلما سنفعل في القسم الخاص بـ [نمذجة اللغة السببية](/course/chapter7/6). ومع ذلك، سيكون من الأسرع ضبط نموذج ترجمة موجود، سواء كان متعدد اللغات مثل mT5 أو mBART الذي تريد ضبطه لزوج لغوي محدد، أو حتى نموذج متخصص في الترجمة من لغة إلى أخرى تريد ضبطه لمجموعتك المحددة.

في هذا القسم، سنقوم بضبط نموذج Marian مسبق التدريب للترجمة من الإنجليزية إلى الفرنسية (حيث يتحدث العديد من موظفي Hugging Face كلتا اللغتين) على مجموعة بيانات [KDE4](https://huggingface.co/datasets/kde4)، وهي مجموعة بيانات للملفات الموضعية لتطبيقات [KDE](https://apps.kde.org/). تم تدريب النموذج الذي سنستخدمه مسبقًا على مجموعة كبيرة من النصوص الفرنسية والإنجليزية المأخوذة من مجموعة بيانات [Opus](https://opus.nlpl.eu/)، والتي تحتوي بالفعل على مجموعة بيانات KDE4. ولكن حتى إذا كانت مجموعة البيانات التي نستخدمها مسبقة التدريب قد شاهدت تلك البيانات أثناء تدريبها المسبق، فسنرى أنه يمكننا الحصول على نسخة أفضل منها بعد الضبط.

بمجرد الانتهاء، سنحصل على نموذج قادر على تقديم تنبؤات مثل هذا:

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

كما في الأقسام السابقة، يمكنك العثور على النموذج الفعلي الذي سنقوم بتدريبه وتحميله إلى Hub باستخدام الكود أدناه والتحقق من تنبؤاته [هنا](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.).

## إعداد البيانات [[preparing-the-data]]

لضبط أو تدريب نموذج ترجمة من الصفر، سنحتاج إلى مجموعة بيانات مناسبة للمهمة. كما ذكرنا سابقًا، سنستخدم مجموعة بيانات [KDE4](https://huggingface.co/datasets/kde4) في هذا القسم، ولكن يمكنك تكييف الكود لاستخدام بياناتك الخاصة بسهولة، طالما لديك أزواج من الجمل في اللغتين اللتين تريد الترجمة منهما وإليهما. راجع [الفصل 5](/course/chapter5) إذا كنت بحاجة إلى تذكير بكيفية تحميل بياناتك المخصصة في `Dataset`.

### مجموعة بيانات KDE4 [[the-kde4-dataset]]

كما هو معتاد، نقوم بتنزيل مجموعة البيانات الخاصة بنا باستخدام دالة `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

إذا كنت تريد العمل مع زوج مختلف من اللغات، يمكنك تحديدها من خلال رموزها. هناك ما مجموعه 92 لغة متاحة لهذه المجموعة من البيانات؛ يمكنك الاطلاع عليها جميعًا من خلال توسيع العلامات اللغوية على [بطاقة مجموعة البيانات](https://huggingface.co/datasets/kde4) الخاصة بها.

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

دعونا نلقي نظرة على مجموعة البيانات:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

لدينا 210,173 زوج من الجمل، ولكن في تقسيم واحد فقط، لذلك سنحتاج إلى إنشاء مجموعة التحقق الخاصة بنا. كما رأينا في [الفصل 5](/course/chapter5)، تحتوي `Dataset` على طريقة `train_test_split()` التي يمكن أن تساعدنا. سنقدم بذرة للتناسق:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

يمكننا إعادة تسمية مفتاح "test" إلى "validation" على النحو التالي:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

الآن دعونا نلقي نظرة على عنصر واحد من مجموعة البيانات:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par défaut, développer les fils de discussion'}
```

نحصل على قاموس بجملتين في زوج اللغات الذي طلبناه. إحدى خصائص هذه المجموعة من البيانات المليئة بمصطلحات علوم الكمبيوتر التقنية هي أنها مترجمة بالكامل إلى الفرنسية. ومع ذلك، يترك المهندسون الفرنسيون معظم الكلمات الخاصة بعلوم الكمبيوتر باللغة الإنجليزية عندما يتحدثون. هنا، على سبيل المثال، قد تظهر كلمة "threads" في جملة فرنسية، خاصة في محادثة تقنية؛ ولكن في هذه المجموعة من البيانات تم ترجمتها إلى "fils de discussion" الأكثر صحة. النموذج مسبق التدريب الذي نستخدمه، والذي تم تدريبه مسبقًا على مجموعة أكبر من الجمل الفرنسية والإنجليزية، يأخذ الخيار الأسهل وهو ترك الكلمة كما هي:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par défaut pour les threads élargis'}]
```

يمكن رؤية مثال آخر على هذا السلوك مع كلمة "plugin"، والتي ليست كلمة فرنسية رسمية ولكن معظم المتحدثين الأصليين سيفهمونها ولن يضطروا إلى ترجمتها.
في مجموعة بيانات KDE4 تمت ترجمة هذه الكلمة إلى الفرنسية إلى "module d'extension" الأكثر رسمية:

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```
ومع ذلك، فإن نموذجنا المُدرب مسبقًا يلتزم بالكلمة الإنجليزية المألوفة والمختصرة:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': 'تعذر استيراد %1 باستخدام المكون الإضافي لاستيراد OFX. هذا الملف ليس بالتنسيق الصحيح.'}]
```

سيكون من المثير للاهتمام أن نرى ما إذا كان نموذجنا المُدرب سيستفيد من تلك الخصائص لمجموعة البيانات (تنبيه: سيفعل ذلك).

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

✏️ **دورك!** هناك كلمة إنجليزية أخرى تُستخدم غالبًا في اللغة الفرنسية وهي "email". ابحث عن أول عينة في مجموعة البيانات التدريبية التي تستخدم هذه الكلمة. كيف تُترجم؟ وكيف يترجم النموذج المُدرب مسبقًا نفس الجملة الإنجليزية؟

</Tip>

### معالجة البيانات[[processing-the-data]]

<Youtube id="XAR8jnZZuUs"/>

يجب أن تكون على دراية بالعملية الآن: تحتاج جميع النصوص إلى تحويلها إلى مجموعات من معرّفات الرموز حتى يتمكن النموذج من فهمها. بالنسبة لهذه المهمة، سنحتاج إلى تقسيم كل من المدخلات والأهداف إلى رموز. مهمتنا الأولى هي إنشاء كائن `tokenizer`. كما ذكرنا سابقًا، سنستخدم نموذجًا مُدربًا مسبقًا من Marian للترجمة من الإنجليزية إلى الفرنسية. إذا كنت تحاول استخدام هذا الكود مع زوج آخر من اللغات، تأكد من تكييف نقطة توقف النموذج. توفر منظمة [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) أكثر من ألف نموذج في لغات متعددة.

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

يمكنك أيضًا استبدال `model_checkpoint` بأي نموذج آخر تفضله من [Hub](https://huggingface.co/models)، أو مجلد محلي قمت بحفظ نموذج مُدرب مسبقًا و tokenizer فيه.

<Tip>

💡 إذا كنت تستخدم tokenizer متعدد اللغات مثل mBART أو mBART-50 أو M2M100، فستحتاج إلى تعيين رموز اللغة للمدخلات والأهداف في tokenizer من خلال تعيين `tokenizer.src_lang` و `tokenizer.tgt_lang` للقيم الصحيحة.

</Tip>

إعداد بياناتنا مباشر للغاية. هناك شيء واحد فقط يجب تذكره؛ تحتاج إلى التأكد من أن tokenizer يعالج الأهداف في لغة الإخراج (الفرنسية هنا). يمكنك القيام بذلك عن طريق تمرير الأهداف إلى حجة `text_targets` لطريقة `__call__` الخاصة بـ tokenizer.

لمعرفة كيفية عمل ذلك، دعنا نعالج عينة واحدة من كل لغة في مجموعة التدريب:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

كما نرى، يحتوي الإخراج على معرّفات المدخلات المرتبطة بالجملة الإنجليزية، في حين يتم تخزين المعرّفات المرتبطة بالجملة الفرنسية في حقل `labels`. إذا نسيت الإشارة إلى أنك تقوم بتقسيم الرموز، فسيتم تقسيمها بواسطة tokenizer المدخلات، والذي في حالة نموذج Marian لن يكون جيدًا على الإطلاق:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']
['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']
```

كما نرى، يؤدي استخدام tokenizer الإنجليزية لمعالجة جملة فرنسية إلى المزيد من الرموز، حيث لا يعرف tokenizer أي كلمات فرنسية (باستثناء تلك التي تظهر أيضًا في اللغة الإنجليزية، مثل "discussion").

بما أن `inputs` عبارة عن قاموس بمفاتيحنا المعتادة (معرّفات المدخلات، قناع الانتباه، إلخ)، فإن الخطوة الأخيرة هي تحديد دالة المعالجة المسبقة التي سنطبقها على مجموعات البيانات:

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

لاحظ أننا نحدد نفس الطول الأقصى لمدخلاتنا ومخرجاتنا. نظرًا لأن النصوص التي نتعامل معها تبدو قصيرة جدًا، نستخدم 128.

<Tip>

💡 إذا كنت تستخدم نموذج T5 (على وجه التحديد، إحدى نقاط توقف `t5-xxx`)، فسيُتوقع النموذج أن تحتوي المدخلات النصية على بادئة تشير إلى المهمة قيد التنفيذ، مثل `translate: English to French:`.

</Tip>

<Tip warning={true}>

⚠️ لا نولي اهتمامًا لقناع الانتباه للأهداف، حيث لن يتوقعه النموذج. بدلاً من ذلك، يجب تعيين العلامات المقابلة لرموز الحشو إلى `-100` حتى يتم تجاهلها في حساب الخسارة. سيتم ذلك بواسطة جامع البيانات الخاص بنا لاحقًا حيث نطبق الحشو الديناميكي، ولكن إذا كنت تستخدم الحشو هنا، فيجب عليك تكييف دالة المعالجة المسبقة لتعيين جميع العلامات التي تقابل رمز الحشو إلى `-100`.

</Tip>

الآن يمكننا تطبيق المعالجة المسبقة في خطوة واحدة على جميع أقسام مجموعة البيانات الخاصة بنا:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

الآن بعد معالجة البيانات، نحن مستعدون لتدريب نموذجنا المُدرب مسبقًا!

{#if fw === 'pt'}

## تدريب النموذج باستخدام واجهة برمجة التطبيقات `Trainer`[[fine-tuning-the-model-with-the-trainer-api]]

سيكون الكود الفعلي باستخدام `Trainer` هو نفسه كما كان من قبل، مع تغيير بسيط واحد: نستخدم هنا [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer)، وهو فئة فرعية من `Trainer` والتي ستسمح لنا بالتعامل بشكل صحيح مع التقييم، باستخدام طريقة `generate()` للتنبؤ بالمخرجات من المدخلات. سنغوص في ذلك بمزيد من التفصيل عندما نتحدث عن حساب المقياس.

أولاً وقبل كل شيء، نحتاج إلى نموذج فعلي لتدريبه. سنستخدم واجهة برمجة التطبيقات `AutoModel` المعتادة:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## تدريب النموذج باستخدام Keras[[fine-tuning-the-model-with-keras]]

أولاً وقبل كل شيء، نحتاج إلى نموذج فعلي لتدريبه. سنستخدم واجهة برمجة التطبيقات `AutoModel` المعتادة:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

💡 نقطة التحقق 'Helsinki-NLP/opus-mt-en-fr' تحتوي فقط على أوزان PyTorch، لذلك ستحصل على خطأ إذا حاولت تحميل النموذج بدون استخدام الحجة 'from_pt=True' في طريقة 'from_pretrained()'. عند تحديد 'from_pt=True'، سيقوم المكتبة تلقائيًا بتنزيل وتحويل أوزان PyTorch لك. كما ترى، من السهل جدًا التبديل بين الأطر في 🤗 Transformers!

</Tip>

{/if}

لاحظ أننا هذه المرة نستخدم نموذجًا تم تدريبه على مهمة ترجمة ويمكن استخدامه بالفعل، لذلك لا يوجد تحذير بشأن الأوزان المفقودة أو تلك التي تم تهيئتها حديثًا.

### تجميع البيانات [[data-collation]]

سنحتاج إلى مجمع بيانات للتعامل مع التوسيد للدفعات الديناميكية. لا يمكننا ببساطة استخدام 'DataCollatorWithPadding' كما في [الفصل 3](/course/chapter3) في هذه الحالة، لأن ذلك يقوم فقط بتوسيد المدخلات (معرّفات الإدخال، قناع الانتباه، ومعرّفات نوع الرمز). يجب أيضًا توسيد التصنيفات إلى الطول الأقصى الذي تم العثور عليه في التصنيفات. وكما ذكرنا سابقًا، يجب أن تكون قيمة التوسيد المستخدمة لتوسيد التصنيفات -100 وليس رمز التوسيد للمحلل اللغوي، للتأكد من تجاهل تلك القيم الموسدة في حساب الخسارة.

يتم ذلك كله بواسطة ['DataCollatorForSeq2Seq'](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). مثل 'DataCollatorWithPadding'، فإنه يأخذ 'tokenizer' المستخدم لمعالجة المدخلات مسبقًا، ولكنه يأخذ أيضًا 'model'. هذا لأن مجمع البيانات هذا سيكون مسؤولاً أيضًا عن إعداد معرّفات إدخال فك التشفير، والتي هي إصدارات منزاحة من التصنيفات مع رمز خاص في البداية. نظرًا لأن هذا التحول يتم بشكل مختلف قليلاً للهندسات المعمارية المختلفة، يحتاج 'DataCollatorForSeq2Seq' إلى معرفة كائن 'model':

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

لاختبار هذا على بعض العينات، ما علينا سوى استدعائه على قائمة من الأمثلة من مجموعة بياناتنا المعلمة:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

يمكننا التحقق من توسيد التصنيفات إلى الطول الأقصى للدفعة، باستخدام -100:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

ويمكننا أيضًا إلقاء نظرة على معرّفات إدخال فك التشفير، لمعرفة أنها إصدارات منزاحة من التصنيفات:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

هذه هي التصنيفات للعنصر الأول والثاني في مجموعة بياناتنا:

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

سنمرر هذا 'data_collator' إلى 'Seq2SeqTrainer'. بعد ذلك، دعنا نلقي نظرة على المقياس.

{:else}

يمكننا الآن استخدام هذا 'data_collator' لتحويل كل مجموعة بياناتنا إلى 'tf.data.Dataset'، جاهزة للتدريب:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### المقاييس [[metrics]]

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

تضيف ميزة 'Seq2SeqTrainer' إلى فئة 'Trainer' الرئيسية القدرة على استخدام طريقة 'generate()' أثناء التقييم أو التنبؤ. أثناء التدريب، سيستخدم النموذج 'decoder_input_ids' مع قناع الانتباه للتأكد من أنه لا يستخدم الرموز بعد الرمز الذي يحاول التنبؤ به، لتسريع التدريب. أثناء الاستدلال، لن نتمكن من استخدام تلك الرموز لأننا لن نملك التصنيفات، لذلك فمن الجيد تقييم نموذجنا بنفس الإعداد.

كما رأينا في [الفصل 1](/course/chapter1/6)، يقوم فك التشفير بالاستدلال عن طريق التنبؤ بالرموز واحدًا تلو الآخر - وهو شيء يتم تنفيذه خلف الكواليس في 🤗 Transformers بواسطة طريقة 'generate()'. سيسمح لنا 'Seq2SeqTrainer' باستخدام تلك الطريقة للتقييم إذا قمنا بتعيين 'predict_with_generate=True'.

{/if}

المقياس التقليدي المستخدم للترجمة هو [درجة BLEU](https://en.wikipedia.org/wiki/BLEU)، التي تم تقديمها في [مقال عام 2002](https://aclanthology.org/P02-1040.pdf) بواسطة كيشور بابينيني وآخرين. تقيّم درجة BLEU مدى قرب الترجمات من تصنيفاتها. لا تقيس قابلية الفهم أو الصحة النحوية للنواتج المولدة للنموذج، ولكنها تستخدم قواعد إحصائية للتأكد من أن جميع الكلمات في النواتج المولدة تظهر أيضًا في الأهداف. بالإضافة إلى ذلك، هناك قواعد تعاقب التكرارات لنفس الكلمات إذا لم تكن أيضًا متكررة في الأهداف (لتجنب إخراج النموذج لجمل مثل "the the the the the") والجمل الأقصر من تلك الموجودة في الأهداف (لتجنب إخراج النموذج لجمل مثل "the").

أحد أوجه الضعف في BLEU هو أنه يتوقع أن يكون النص معلمًا بالفعل، مما يجعل من الصعب مقارنة الدرجات بين النماذج التي تستخدم محللات لغوية مختلفة. لذلك بدلاً من ذلك، فإن المقياس الأكثر استخدامًا اليوم لمقارنة نماذج الترجمة هو [SacreBLEU](https://github.com/mjpost/sacrebleu)، والذي يعالج هذا الضعف (وغيره) من خلال توحيد خطوة المعالجة. لاستخدام هذا المقياس، نحتاج أولاً إلى تثبيت مكتبة SacreBLEU:

```py
!pip install sacrebleu
```

بعد ذلك يمكننا تحميله عبر 'evaluate.load()' كما فعلنا في [الفصل 3](/course/chapter3):

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

هذا المقياس سيأخذ النصوص كمدخلات وأهداف. تم تصميمه لقبول عدة أهداف مقبولة، حيث غالبًا ما تكون هناك ترجمات مقبولة متعددة لنفس الجملة - توفر مجموعة البيانات التي نستخدمها واحدة فقط، ولكن من غير غير المعتاد في NLP العثور على مجموعات بيانات تعطي عدة جمل كتصنيفات. لذلك، يجب أن تكون التنبؤات قائمة من الجمل، ولكن المراجع يجب أن تكون قائمة من قوائم الجمل.

دعنا نجرب مثال:

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

هذا يحصل على درجة BLEU تبلغ 46.75، وهو جيد جدًا - للرجوع إليه، حقق النموذج الأصلي للمحول في ورقة ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) درجة BLEU تبلغ 41.8 على مهمة ترجمة مماثلة بين الإنجليزية والفرنسية! (لمزيد من المعلومات حول المقاييس الفردية، مثل 'counts' و 'bp'، راجع [مستودع SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) من ناحية أخرى، إذا حاولنا مع نوعي التنبؤات السيئين (الكثير من التكرارات أو الأقصر من اللازم) التي غالبًا ما تخرج من نماذج الترجمة، فسنحصل على درجات BLEU سيئة جدًا:

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

يمكن أن يتراوح التقييم من 0 إلى 100، وكلما كان أعلى كان أفضل.

{#if fw === 'tf'}

للحصول على النصوص التي يمكن أن تستخدمها المترجمة من مخرجات النموذج، سنستخدم طريقة `tokenizer.batch_decode()`. كل ما علينا فعله هو تنظيف جميع `-100`s في التصنيفات؛ سيقوم المحلل الرمزي تلقائيًا بنفس الشيء بالنسبة لرموز الحشو. دعنا نحدد دالة تأخذ نموذجنا ومجموعة بياناتنا وتحسب المقاييس عليها. سنستخدم أيضًا خدعة تزيد الأداء بشكل كبير - تجميع كود التوليد الخاص بنا مع [XLA](https://www.tensorflow.org/xla)، وهو مترجم جبر خطي متسارع من TensorFlow. يطبق XLA تحسينات مختلفة على رسم الحوسبة للنموذج، ويؤدي إلى تحسينات كبيرة في السرعة واستخدام الذاكرة. كما هو موضح في مدونة Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate)، يعمل XLA بشكل أفضل عندما لا تختلف أشكال الإدخال لدينا كثيرًا. للتعامل مع هذا، سنقوم بملء إدخالاتنا إلى مضاعفات 128، وسنقوم بإنشاء مجموعة بيانات جديدة مع مجمع الحشو، ثم سنطبق الديكور `@tf.function(jit_compile=True)` على دالة التوليد الخاصة بنا، والتي تشير إلى الدالة بأكملها للتجميع مع XLA.

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )


def compute_metrics():
    all_preds = []
    all_labels = []

    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

للحصول على النصوص التي يمكن أن تستخدمها المترجمة من مخرجات النموذج، سنستخدم طريقة `tokenizer.batch_decode()`. كل ما علينا فعله هو تنظيف جميع `-100`s في التصنيفات (سيقوم المحلل الرمزي تلقائيًا بنفس الشيء بالنسبة لرموز الحشو):

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # في حالة عودة النموذج لأكثر من احتمالات التنبؤ
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # استبدال -100s في التصنيفات حيث لا يمكننا فك تشفيرها
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # بعض المعالجة البسيطة
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

الآن بعد أن انتهينا من ذلك، نحن مستعدون لضبط نموذجنا الدقيق!

### ضبط دقة النموذج[[fine-tuning-the-model]]

الخطوة الأولى هي تسجيل الدخول إلى Hugging Face، حتى تتمكن من تحميل نتائجك إلى Model Hub. هناك دالة ملائمة لمساعدتك في ذلك في دفتر الملاحظات:

```python
from huggingface_hub import notebook_login

notebook_login()
```

سيتم عرض عنصر واجهة مستخدم حيث يمكنك إدخال بيانات اعتماد تسجيل دخول Hugging Face الخاصة بك.

إذا لم تكن تعمل في دفتر ملاحظات، فما عليك سوى كتابة السطر التالي في طرفيتك:

```bash
huggingface-cli login
```

{#if fw === 'tf'}

قبل أن نبدأ، دعنا نرى ما هي النتائج التي نحصل عليها من نموذجنا بدون أي تدريب:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

بمجرد الانتهاء من ذلك، يمكننا إعداد كل ما نحتاجه لتجميع وتدريب نموذجنا. لاحظ استخدام `tf.keras.mixed_precision.set_global_policy("mixed_float16")` - سيخبر هذا Keras بالتدريب باستخدام float16، والذي يمكن أن يعطي تسريعًا كبيرًا على وحدات معالجة الرسومات التي تدعمه (Nvidia 20xx/V100 أو أحدث).

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# عدد خطوات التدريب هو عدد العينات في مجموعة البيانات، مقسومة على حجم الدفعة ثم مضروبة
# بعدد الفترات الإجمالية. لاحظ أن tf_train_dataset هنا هو tf.data.Dataset ذو دفعات،
# وليس مجموعة بيانات Hugging Face الأصلية، لذا فإن len() الخاصة به هي بالفعل num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# تدريب في float16 مختلط الدقة
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

بعد ذلك، نحدد `PushToHubCallback` لتحميل نموذجنا إلى Hub أثناء التدريب، كما رأينا في [القسم 2]((/course/chapter7/2))، ثم نقوم ببساطة بتناسب النموذج مع هذا الاستدعاء:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

لاحظ أنه يمكنك تحديد اسم المستودع الذي تريد دفعه باستخدام حجة `hub_model_id` (على وجه الخصوص، سيتعين عليك استخدام هذه الحجة للدفع إلى منظمة). على سبيل المثال، عندما قمنا بدفع النموذج إلى منظمة [`huggingface-course` organization](https://huggingface.co/huggingface-course)، أضفنا `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` إلى `Seq2SeqTrainingArguments`. بشكل افتراضي، سيتم استخدام المستودع الموجود في مساحة الاسم الخاصة بك والمسمى باسم دليل الإخراج الذي قمت بتعيينه، لذا سيكون هنا `"sgugger/marian-finetuned-kde4-en-to-fr"` (وهو النموذج الذي قمنا بربطه في بداية هذا القسم).

<Tip>

💡 إذا كان دليل الإخراج الذي تستخدمه موجودًا بالفعل، فيجب أن يكون مستنسخًا محليًا للمستودع الذي تريد دفعه إليه. إذا لم يكن الأمر كذلك، فستحصل على خطأ عند استدعاء `model.fit()` وسيتعين عليك تعيين اسم جديد.

</Tip>

أخيرًا، دعنا نرى كيف تبدو مقاييسنا الآن بعد انتهاء التدريب:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

في هذه المرحلة، يمكنك استخدام أداة الاستدلال على منصة النماذج لاختبار نموذجك ومشاركته مع أصدقائك. لقد قمت بضبط نموذج بنجاح على مهمة الترجمة - تهانينا!

{:else}

بمجرد الانتهاء من ذلك، يمكننا تحديد `Seq2SeqTrainingArguments`. مثل `Trainer`، نستخدم فئة فرعية من `TrainingArguments` تحتوي على بعض الحقول الإضافية:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

بخلاف المعلمات المعتادة (مثل معدل التعلم، وعدد العصور، وحجم الدفعة، وبعض التلاشي الوزني)، هناك بعض التغييرات مقارنة بما رأيناه في الأقسام السابقة:

- لا نقوم بضبط أي تقييم منتظم، حيث يستغرق التقييم بعض الوقت؛ سنقوم فقط بتقييم نموذجنا مرة واحدة قبل التدريب وبعده.
- نحدد `fp16=True`، مما يسرع التدريب على وحدات معالجة الرسومات الحديثة.
- نحدد `predict_with_generate=True`، كما ناقشنا أعلاه.
- نستخدم `push_to_hub=True` لتحميل النموذج إلى المنصة في نهاية كل عصر.

لاحظ أنه يمكنك تحديد الاسم الكامل للمستودع الذي تريد تحميله باستخدام حجة `hub_model_id` (على وجه الخصوص، سيتعين عليك استخدام هذه الحجة للتحميل إلى منظمة). على سبيل المثال، عندما قمنا بتحميل النموذج إلى منظمة [huggingface-course](https://huggingface.co/huggingface-course)، أضفنا `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` إلى `Seq2SeqTrainingArguments`. بشكل افتراضي، سيتم استخدام المستودع في مساحة اسمك ويتم تسميته وفقًا لدليل الإخراج الذي قمت بتعيينه، لذا في حالتنا سيكون `"sgugger/marian-finetuned-kde4-en-to-fr"` (وهو النموذج الذي قمنا بربطه في بداية هذا القسم).

<Tip>

💡 إذا كان دليل الإخراج الذي تستخدمه موجودًا بالفعل، فيجب أن يكون مستنسخًا محليًا للمستودع الذي تريد تحميله. إذا لم يكن كذلك، فستحصل على خطأ عند تحديد `Seq2SeqTrainer` وسيتعين عليك تعيين اسم جديد.

</Tip>

أخيرًا، نقوم فقط بتمرير كل شيء إلى `Seq2SeqTrainer`:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

قبل التدريب، سنلقي نظرة أولاً على النتيجة التي يحصل عليها نموذجنا، للتأكد من أننا لا نجعل الأمور أسوأ مع الضبط الدقيق. ستستغرق هذه الأوامر بعض الوقت، لذا يمكنك تناول القهوة أثناء التنفيذ:

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

درجة BLEU 39 ليست سيئة للغاية، مما يعكس حقيقة أن نموذجنا جيد بالفعل في ترجمة الجمل الإنجليزية إلى الفرنسية.

التالي هو التدريب، والذي سيستغرق أيضًا بعض الوقت:

```python
trainer.train()
```

لاحظ أنه أثناء حدوث التدريب، كلما تم حفظ النموذج (هنا، كل عصر) يتم تحميله إلى المنصة في الخلفية. بهذه الطريقة، ستتمكن من استئناف تدريبك على آلة أخرى إذا لزم الأمر.

بمجرد الانتهاء من التدريب، نقوم بتقييم نموذجنا مرة أخرى - نأمل أن نرى بعض التحسن في درجة BLEU!

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

هذا تحسن بنحو 14 نقطة، وهو أمر رائع.

أخيرًا، نستخدم طريقة `push_to_hub()` للتأكد من تحميل أحدث إصدار من النموذج. يقوم `Trainer` أيضًا بصياغة بطاقة نموذج بجميع نتائج التقييم وتحميلها. تحتوي بطاقة النموذج هذه على بيانات وصفية تساعد منصة النماذج على اختيار أداة الاستدلال لتجربة الاستدلال. عادة، لا توجد حاجة لقول أي شيء حيث يمكنه استنتاج الأداة الصحيحة من فئة النموذج، ولكن في هذه الحالة، يمكن استخدام نفس فئة النموذج لجميع أنواع المشكلات التسلسلية، لذا نحدد أنها نموذج ترجمة:

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

تعيد هذه الأوامر عنوان URL للالتزام الذي قام به للتو، إذا كنت تريد فحصه:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

في هذه المرحلة، يمكنك استخدام أداة الاستدلال على منصة النماذج لاختبار نموذجك ومشاركته مع أصدقائك. لقد قمت بضبط نموذج بنجاح على مهمة الترجمة - تهانينا!

إذا كنت تريد الغوص بشكل أعمق قليلاً في حلقة التدريب، فسنريكم الآن كيفية القيام بنفس الشيء باستخدام 🤗 Accelerate.

{/if}

{#if fw === 'pt'}

## حلقة تدريب مخصصة [[a-custom-training-loop]]

دعونا نلقي نظرة الآن على حلقة التدريب الكاملة، بحيث يمكنك تخصيص الأجزاء التي تحتاجها. ستبدو مشابهة لما فعلناه في [القسم 2](/course/chapter7/2) و[الفصل 3](/course/chapter3/4).

### إعداد كل شيء للتدريب [[preparing-everything-for-training]]

لقد رأيت كل هذا عدة مرات الآن، لذا سنمر عبر الكود بسرعة. أولاً سنبني `DataLoader`s من مجموعات البيانات الخاصة بنا، بعد ضبط مجموعات البيانات على تنسيق `"torch"` حتى نحصل على تنسورات PyTorch:

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

بعد ذلك، سنعيد إنشاء نموذجنا، للتأكد من أننا لا نواصل الضبط الدقيق من قبل ولكن نبدأ من النموذج المسبق التدريب مرة أخرى:

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ثم سنحتاج إلى محسن:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

بمجرد حصولنا على كل هذه الأشياء، يمكننا إرسالها إلى طريقة `accelerator.prepare()`. تذكر أنه إذا كنت تريد التدريب على وحدات معالجة الرسومات في دفتر ملاحظات Colab، فسيتعين عليك نقل كل هذا الكود إلى وظيفة تدريب، ولا يجب أن تنفذ أي خلية تنشئ `Accelerator`.

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

الآن بعد أن أرسلنا `train_dataloader` إلى `accelerator.prepare()`، يمكننا استخدام طوله لحساب عدد خطوات التدريب. تذكر أنه يجب علينا دائمًا القيام بذلك بعد إعداد أداة التحميل، حيث ستغير هذه الطريقة طول `DataLoader`. نستخدم جدولًا خطيًا كلاسيكيًا من معدل التعلم إلى 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

أخيرًا، لتحميل نموذجنا إلى المنصة، سنحتاج إلى إنشاء كائن `Repository` في مجلد العمل. قم بتسجيل الدخول إلى منصة Hugging Face، إذا لم تكن مسجلاً بالفعل. سنحدد اسم المستودع من معرف النموذج الذي نريد إعطاءه لنموذجنا (لا تتردد في استبدال `repo_name` بخيارك الخاص؛ يجب أن يحتوي فقط على اسم المستخدم الخاص بك، وهو ما تفعله وظيفة `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

ثم يمكننا استنساخ المستودع هذا في مجلد محلي. إذا كان موجودًا بالفعل، يجب أن يكون هذا المجلد المحلي مستنسخًا من المستودع الذي نعمل عليه:

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

الآن يمكننا تحميل أي شيء نحفظه في `output_dir` عن طريق استدعاء طريقة `repo.push_to_hub()`. سيساعدنا هذا في تحميل النماذج الوسيطة في نهاية كل دورة.

### حلقة التدريب[[training-loop]]

نحن الآن مستعدون لكتابة حلقة التدريب الكاملة. لتبسيط الجزء التقييمي، نحدد هذه الدالة `postprocess()` التي تأخذ التنبؤات والعلامات وتحولها إلى قوائم من السلاسل النصية التي يتوقعها كائن `metric`:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # استبدال -100 في العلامات لأننا لا نستطيع فك تشفيرها.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # بعض المعالجة البسيطة
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

تبدو حلقة التدريب مشابهة جدًا لتلك الموجودة في [القسم 2](/course/chapter7/2) و[الفصل 3](/course/chapter3)، مع بعض الاختلافات في الجزء التقييمي -- لذا دعونا نركز على ذلك!

أول شيء يجب ملاحظته هو أننا نستخدم طريقة `generate()` لحساب التنبؤات، ولكن هذه الطريقة هي على نموذجنا الأساسي، وليس النموذج الملفوف 🤗 Accelerate الذي تم إنشاؤه في طريقة `prepare()`. لهذا السبب نقوم بإلغاء لف النموذج أولاً، ثم استدعاء هذه الطريقة.

الشيء الثاني هو أنه، مثل [تصنيف الرموز](/course/chapter7/2)، قد تكون عمليتان قد أضافتا حشوًا إلى المدخلات والعلامات بأشكال مختلفة، لذا نستخدم `accelerator.pad_across_processes()` لجعل التنبؤات والعلامات بنفس الشكل قبل استدعاء طريقة `gather()`. إذا لم نفعل ذلك، فإن التقييم إما أن يتعطل أو يعلق إلى الأبد.

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # التدريب
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # التقييم
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # ضروري لإضافة حشو للتنبؤات والعلامات لجمعها
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # الحفظ والتحميل
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

بمجرد الانتهاء من ذلك، يجب أن يكون لديك نموذج ذو نتائج مشابهة جدًا للنموذج الذي تم تدريبه باستخدام `Seq2SeqTrainer`. يمكنك التحقق من النموذج الذي قمنا بتدريبه باستخدام هذا الكود في [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). وإذا كنت ترغب في اختبار أي تعديلات على حلقة التدريب، يمكنك تنفيذها مباشرة عن طريق تعديل الكود الموضح أعلاه!

{/if}

## استخدام النموذج المدرب جيدًا[[using-the-fine-tuned-model]]

لقد أظهرنا لك بالفعل كيف يمكنك استخدام النموذج الذي قمنا بتدريبه جيدًا على Model Hub باستخدام أداة التقييم. لاستخدامه محليًا في `pipeline`، يجب علينا فقط تحديد معرّف النموذج الصحيح:

```py
from transformers import pipeline

# استبدل هذا بمعرّف نقطة التحقق الخاصة بك
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par défaut, développer les fils de discussion'}]
```

كما هو متوقع، قام نموذجنا المسبق التدريب بتكييف معرفته مع مجموعة البيانات التي قمنا بتدريبه عليها، وبدلاً من ترك الكلمة الإنجليزية "threads" بمفردها، فإنه يترجمها الآن إلى النسخة الفرنسية الرسمية. وينطبق الأمر نفسه على كلمة "plugin":

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

مثال رائع آخر على تكيف المجال!

<Tip>

✏️ **دورك!** ماذا يعيد النموذج على العينة التي تحتوي على الكلمة "email" التي حددتها سابقًا؟

</Tip>