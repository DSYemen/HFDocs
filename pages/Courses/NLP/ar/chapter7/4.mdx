<FrameworkSwitchCourse {fw} />

# ุงูุชุฑุฌูุฉ [[translation]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
]} />

{/if}

ุฏุนููุง ุงูุขู ูุบูุต ูู ุงูุชุฑุฌูุฉ. ูุฐู ูู ูููุฉ ุฃุฎุฑู [ูู ุชุณูุณู ุฅูู ุชุณูุณู](/course/chapter1/7)ุ ููุง ูุนูู ุฃููุง ูุดููุฉ ูููู ุตูุงุบุชูุง ุนูู ุฃููุง ุงูุงูุชูุงู ูู ุชุณูุณู ุฅูู ุขุฎุฑ. ุจูุฐุง ุงููุนููุ ูุฅู ุงููุดููุฉ ูุฑูุจุฉ ุฌุฏูุง ูู [ุงูุชูุฎูุต](/course/chapter7/6)ุ ูููููู ุชูููู ูุง ุณูุฑุงู ููุง ูุน ูุดุงูู ุฃุฎุฑู ูู ุชุณูุณู ุฅูู ุชุณูุณู ูุซู:

- **ููู ุงูุฃุณููุจ**: ุฅูุดุงุก ูููุฐุฌ ูููู ุจ*ุชุฑุฌูุฉ* ุงููุตูุต ุงูููุชูุจุฉ ุจุฃุณููุจ ูุนูู ุฅูู ุฃุณููุจ ุขุฎุฑ (ุนูู ุณุจูู ุงููุซุงูุ ูู ุงูุฑุณูู ุฅูู ุงูุนุงุฏู ุฃู ูู ุงูุฅูุฌููุฒูุฉ ุงูุดูุณุจูุฑูุฉ ุฅูู ุงูุฅูุฌููุฒูุฉ ุงูุญุฏูุซุฉ)
- **ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงูุชูููุฏูุฉ**: ุฅูุดุงุก ูููุฐุฌ ูููู ุจุชูููุฏ ุฅุฌุงุจุงุช ููุฃุณุฆูุฉุ ุจุงููุธุฑ ุฅูู ุงูุณูุงู

<Youtube id="1JvfrvZgi6c"/>

ุฅุฐุง ูุงู ูุฏูู ูุฌููุนุฉ ูุจูุฑุฉ ุจูุง ูููู ูู ุงููุตูุต ุจูุบุชูู (ุฃู ุฃูุซุฑ)ุ ููููู ุชุฏุฑูุจ ูููุฐุฌ ุชุฑุฌูุฉ ุฌุฏูุฏ ูู ุงูุตูุฑ ูุซููุง ุณููุนู ูู ุงููุณู ุงูุฎุงุต ุจู [ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](/course/chapter7/6). ููุน ุฐููุ ุณูููู ูู ุงูุฃุณุฑุน ุถุจุท ูููุฐุฌ ุชุฑุฌูุฉ ููุฌูุฏุ ุณูุงุก ูุงู ูุชุนุฏุฏ ุงููุบุงุช ูุซู mT5 ุฃู mBART ุงูุฐู ุชุฑูุฏ ุถุจุทู ูุฒูุฌ ูุบูู ูุญุฏุฏุ ุฃู ุญุชู ูููุฐุฌ ูุชุฎุตุต ูู ุงูุชุฑุฌูุฉ ูู ูุบุฉ ุฅูู ุฃุฎุฑู ุชุฑูุฏ ุถุจุทู ููุฌููุนุชู ุงููุญุฏุฏุฉ.

ูู ูุฐุง ุงููุณูุ ุณูููู ุจุถุจุท ูููุฐุฌ Marian ูุณุจู ุงูุชุฏุฑูุจ ููุชุฑุฌูุฉ ูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงููุฑูุณูุฉ (ุญูุซ ูุชุญุฏุซ ุงูุนุฏูุฏ ูู ููุธูู Hugging Face ููุชุง ุงููุบุชูู) ุนูู ูุฌููุนุฉ ุจูุงูุงุช [KDE4](https://huggingface.co/datasets/kde4)ุ ููู ูุฌููุนุฉ ุจูุงูุงุช ูููููุงุช ุงูููุถุนูุฉ ูุชุทุจููุงุช [KDE](https://apps.kde.org/). ุชู ุชุฏุฑูุจ ุงููููุฐุฌ ุงูุฐู ุณูุณุชุฎุฏูู ูุณุจููุง ุนูู ูุฌููุนุฉ ูุจูุฑุฉ ูู ุงููุตูุต ุงููุฑูุณูุฉ ูุงูุฅูุฌููุฒูุฉ ุงููุฃุฎูุฐุฉ ูู ูุฌููุนุฉ ุจูุงูุงุช [Opus](https://opus.nlpl.eu/)ุ ูุงูุชู ุชุญุชูู ุจุงููุนู ุนูู ูุฌููุนุฉ ุจูุงูุงุช KDE4. ูููู ุญุชู ุฅุฐุง ูุงูุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ูุณุชุฎุฏููุง ูุณุจูุฉ ุงูุชุฏุฑูุจ ูุฏ ุดุงูุฏุช ุชูู ุงูุจูุงูุงุช ุฃุซูุงุก ุชุฏุฑูุจูุง ุงููุณุจูุ ูุณูุฑู ุฃูู ูููููุง ุงูุญุตูู ุนูู ูุณุฎุฉ ุฃูุถู ูููุง ุจุนุฏ ุงูุถุจุท.

ุจูุฌุฑุฏ ุงูุงูุชูุงุกุ ุณูุญุตู ุนูู ูููุฐุฌ ูุงุฏุฑ ุนูู ุชูุฏูู ุชูุจุคุงุช ูุซู ูุฐุง:

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

ููุง ูู ุงูุฃูุณุงู ุงูุณุงุจูุฉุ ููููู ุงูุนุซูุฑ ุนูู ุงููููุฐุฌ ุงููุนูู ุงูุฐู ุณูููู ุจุชุฏุฑูุจู ูุชุญูููู ุฅูู Hub ุจุงุณุชุฎุฏุงู ุงูููุฏ ุฃุฏูุงู ูุงูุชุญูู ูู ุชูุจุคุงุชู [ููุง](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.).

## ุฅุนุฏุงุฏ ุงูุจูุงูุงุช [[preparing-the-data]]

ูุถุจุท ุฃู ุชุฏุฑูุจ ูููุฐุฌ ุชุฑุฌูุฉ ูู ุงูุตูุฑุ ุณูุญุชุงุฌ ุฅูู ูุฌููุนุฉ ุจูุงูุงุช ููุงุณุจุฉ ูููููุฉ. ููุง ุฐูุฑูุง ุณุงุจููุงุ ุณูุณุชุฎุฏู ูุฌููุนุฉ ุจูุงูุงุช [KDE4](https://huggingface.co/datasets/kde4) ูู ูุฐุง ุงููุณูุ ูููู ููููู ุชูููู ุงูููุฏ ูุงุณุชุฎุฏุงู ุจูุงูุงุชู ุงูุฎุงุตุฉ ุจุณูููุฉุ ุทุงููุง ูุฏูู ุฃุฒูุงุฌ ูู ุงูุฌูู ูู ุงููุบุชูู ุงููุชูู ุชุฑูุฏ ุงูุชุฑุฌูุฉ ููููุง ูุฅููููุง. ุฑุงุฌุน [ุงููุตู 5](/course/chapter5) ุฅุฐุง ููุช ุจุญุงุฌุฉ ุฅูู ุชุฐููุฑ ุจููููุฉ ุชุญููู ุจูุงูุงุชู ุงููุฎุตุตุฉ ูู `Dataset`.

### ูุฌููุนุฉ ุจูุงูุงุช KDE4 [[the-kde4-dataset]]

ููุง ูู ูุนุชุงุฏุ ูููู ุจุชูุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

ุฅุฐุง ููุช ุชุฑูุฏ ุงูุนูู ูุน ุฒูุฌ ูุฎุชูู ูู ุงููุบุงุชุ ููููู ุชุญุฏูุฏูุง ูู ุฎูุงู ุฑููุฒูุง. ููุงู ูุง ูุฌููุนู 92 ูุบุฉ ูุชุงุญุฉ ููุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุชุ ููููู ุงูุงุทูุงุน ุนูููุง ุฌููุนูุง ูู ุฎูุงู ุชูุณูุน ุงูุนูุงูุงุช ุงููุบููุฉ ุนูู [ุจุทุงูุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช](https://huggingface.co/datasets/kde4) ุงูุฎุงุตุฉ ุจูุง.

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

ูุฏููุง 210,173 ุฒูุฌ ูู ุงูุฌููุ ูููู ูู ุชูุณูู ูุงุญุฏ ููุทุ ูุฐูู ุณูุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุฌููุนุฉ ุงูุชุญูู ุงูุฎุงุตุฉ ุจูุง. ููุง ุฑุฃููุง ูู [ุงููุตู 5](/course/chapter5)ุ ุชุญุชูู `Dataset` ุนูู ุทุฑููุฉ `train_test_split()` ุงูุชู ูููู ุฃู ุชุณุงุนุฏูุง. ุณููุฏู ุจุฐุฑุฉ ููุชูุงุณู:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

ูููููุง ุฅุนุงุฏุฉ ุชุณููุฉ ููุชุงุญ "test" ุฅูู "validation" ุนูู ุงููุญู ุงูุชุงูู:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

ุงูุขู ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุนูุตุฑ ูุงุญุฏ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par dรฉfaut, dรฉvelopper les fils de discussion'}
```

ูุญุตู ุนูู ูุงููุณ ุจุฌููุชูู ูู ุฒูุฌ ุงููุบุงุช ุงูุฐู ุทูุจูุงู. ุฅุญุฏู ุฎุตุงุฆุต ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุช ุงููููุฆุฉ ุจูุตุทูุญุงุช ุนููู ุงูููุจููุชุฑ ุงูุชูููุฉ ูู ุฃููุง ูุชุฑุฌูุฉ ุจุงููุงูู ุฅูู ุงููุฑูุณูุฉ. ููุน ุฐููุ ูุชุฑู ุงููููุฏุณูู ุงููุฑูุณููู ูุนุธู ุงููููุงุช ุงูุฎุงุตุฉ ุจุนููู ุงูููุจููุชุฑ ุจุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ุนูุฏูุง ูุชุญุฏุซูู. ููุงุ ุนูู ุณุจูู ุงููุซุงูุ ูุฏ ุชุธูุฑ ูููุฉ "threads" ูู ุฌููุฉ ูุฑูุณูุฉุ ุฎุงุตุฉ ูู ูุญุงุฏุซุฉ ุชูููุฉุ ูููู ูู ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุช ุชู ุชุฑุฌูุชูุง ุฅูู "fils de discussion" ุงูุฃูุซุฑ ุตุญุฉ. ุงููููุฐุฌ ูุณุจู ุงูุชุฏุฑูุจ ุงูุฐู ูุณุชุฎุฏููุ ูุงูุฐู ุชู ุชุฏุฑูุจู ูุณุจููุง ุนูู ูุฌููุนุฉ ุฃูุจุฑ ูู ุงูุฌูู ุงููุฑูุณูุฉ ูุงูุฅูุฌููุฒูุฉุ ูุฃุฎุฐ ุงูุฎูุงุฑ ุงูุฃุณูู ููู ุชุฑู ุงููููุฉ ููุง ูู:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dรฉfaut pour les threads รฉlargis'}]
```

ูููู ุฑุคูุฉ ูุซุงู ุขุฎุฑ ุนูู ูุฐุง ุงูุณููู ูุน ูููุฉ "plugin"ุ ูุงูุชู ููุณุช ูููุฉ ูุฑูุณูุฉ ุฑุณููุฉ ูููู ูุนุธู ุงููุชุญุฏุซูู ุงูุฃุตูููู ุณูููููููุง ููู ูุถุทุฑูุง ุฅูู ุชุฑุฌูุชูุง.
ูู ูุฌููุนุฉ ุจูุงูุงุช KDE4 ุชูุช ุชุฑุฌูุฉ ูุฐู ุงููููุฉ ุฅูู ุงููุฑูุณูุฉ ุฅูู "module d'extension" ุงูุฃูุซุฑ ุฑุณููุฉ:

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```
ููุน ุฐููุ ูุฅู ูููุฐุฌูุง ุงูููุฏุฑุจ ูุณุจููุง ููุชุฒู ุจุงููููุฉ ุงูุฅูุฌููุฒูุฉ ุงููุฃูููุฉ ูุงููุฎุชุตุฑุฉ:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': 'ุชุนุฐุฑ ุงุณุชูุฑุงุฏ %1 ุจุงุณุชุฎุฏุงู ุงููููู ุงูุฅุถุงูู ูุงุณุชูุฑุงุฏ OFX. ูุฐุง ุงูููู ููุณ ุจุงูุชูุณูู ุงูุตุญูุญ.'}]
```

ุณูููู ูู ุงููุซูุฑ ููุงูุชูุงู ุฃู ูุฑู ูุง ุฅุฐุง ูุงู ูููุฐุฌูุง ุงูููุฏุฑุจ ุณูุณุชููุฏ ูู ุชูู ุงูุฎุตุงุฆุต ููุฌููุนุฉ ุงูุจูุงูุงุช (ุชูุจูู: ุณููุนู ุฐูู).

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

โ๏ธ **ุฏูุฑู!** ููุงู ูููุฉ ุฅูุฌููุฒูุฉ ุฃุฎุฑู ุชูุณุชุฎุฏู ุบุงูุจูุง ูู ุงููุบุฉ ุงููุฑูุณูุฉ ููู "email". ุงุจุญุซ ุนู ุฃูู ุนููุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชุฏุฑูุจูุฉ ุงูุชู ุชุณุชุฎุฏู ูุฐู ุงููููุฉ. ููู ุชูุชุฑุฌูุ ูููู ูุชุฑุฌู ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ููุณ ุงูุฌููุฉ ุงูุฅูุฌููุฒูุฉุ

</Tip>

### ูุนุงูุฌุฉ ุงูุจูุงูุงุช[[processing-the-data]]

<Youtube id="XAR8jnZZuUs"/>

ูุฌุจ ุฃู ุชููู ุนูู ุฏุฑุงูุฉ ุจุงูุนูููุฉ ุงูุขู: ุชุญุชุงุฌ ุฌููุน ุงููุตูุต ุฅูู ุชุญููููุง ุฅูู ูุฌููุนุงุช ูู ูุนุฑููุงุช ุงูุฑููุฒ ุญุชู ูุชููู ุงููููุฐุฌ ูู ููููุง. ุจุงููุณุจุฉ ููุฐู ุงููููุฉุ ุณูุญุชุงุฌ ุฅูู ุชูุณูู ูู ูู ุงููุฏุฎูุงุช ูุงูุฃูุฏุงู ุฅูู ุฑููุฒ. ูููุชูุง ุงูุฃููู ูู ุฅูุดุงุก ูุงุฆู `tokenizer`. ููุง ุฐูุฑูุง ุณุงุจููุงุ ุณูุณุชุฎุฏู ูููุฐุฌูุง ููุฏุฑุจูุง ูุณุจููุง ูู Marian ููุชุฑุฌูุฉ ูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงููุฑูุณูุฉ. ุฅุฐุง ููุช ุชุญุงูู ุงุณุชุฎุฏุงู ูุฐุง ุงูููุฏ ูุน ุฒูุฌ ุขุฎุฑ ูู ุงููุบุงุชุ ุชุฃูุฏ ูู ุชูููู ููุทุฉ ุชููู ุงููููุฐุฌ. ุชููุฑ ููุธูุฉ [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) ุฃูุซุฑ ูู ุฃูู ูููุฐุฌ ูู ูุบุงุช ูุชุนุฏุฏุฉ.

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

ููููู ุฃูุถูุง ุงุณุชุจุฏุงู `model_checkpoint` ุจุฃู ูููุฐุฌ ุขุฎุฑ ุชูุถูู ูู [Hub](https://huggingface.co/models)ุ ุฃู ูุฌูุฏ ูุญูู ููุช ุจุญูุธ ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ู tokenizer ููู.

<Tip>

๐ก ุฅุฐุง ููุช ุชุณุชุฎุฏู tokenizer ูุชุนุฏุฏ ุงููุบุงุช ูุซู mBART ุฃู mBART-50 ุฃู M2M100ุ ูุณุชุญุชุงุฌ ุฅูู ุชุนููู ุฑููุฒ ุงููุบุฉ ูููุฏุฎูุงุช ูุงูุฃูุฏุงู ูู tokenizer ูู ุฎูุงู ุชุนููู `tokenizer.src_lang` ู `tokenizer.tgt_lang` ููููู ุงูุตุญูุญุฉ.

</Tip>

ุฅุนุฏุงุฏ ุจูุงูุงุชูุง ูุจุงุดุฑ ููุบุงูุฉ. ููุงู ุดูุก ูุงุญุฏ ููุท ูุฌุจ ุชุฐูุฑูุ ุชุญุชุงุฌ ุฅูู ุงูุชุฃูุฏ ูู ุฃู tokenizer ูุนุงูุฌ ุงูุฃูุฏุงู ูู ูุบุฉ ุงูุฅุฎุฑุงุฌ (ุงููุฑูุณูุฉ ููุง). ููููู ุงูููุงู ุจุฐูู ุนู ุทุฑูู ุชูุฑูุฑ ุงูุฃูุฏุงู ุฅูู ุญุฌุฉ `text_targets` ูุทุฑููุฉ `__call__` ุงูุฎุงุตุฉ ุจู tokenizer.

ููุนุฑูุฉ ููููุฉ ุนูู ุฐููุ ุฏุนูุง ูุนุงูุฌ ุนููุฉ ูุงุญุฏุฉ ูู ูู ูุบุฉ ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

ููุง ูุฑูุ ูุญุชูู ุงูุฅุฎุฑุงุฌ ุนูู ูุนุฑููุงุช ุงููุฏุฎูุงุช ุงููุฑุชุจุทุฉ ุจุงูุฌููุฉ ุงูุฅูุฌููุฒูุฉุ ูู ุญูู ูุชู ุชุฎุฒูู ุงููุนุฑููุงุช ุงููุฑุชุจุทุฉ ุจุงูุฌููุฉ ุงููุฑูุณูุฉ ูู ุญูู `labels`. ุฅุฐุง ูุณูุช ุงูุฅุดุงุฑุฉ ุฅูู ุฃูู ุชููู ุจุชูุณูู ุงูุฑููุฒุ ูุณูุชู ุชูุณูููุง ุจูุงุณุทุฉ tokenizer ุงููุฏุฎูุงุชุ ูุงูุฐู ูู ุญุงูุฉ ูููุฐุฌ Marian ูู ูููู ุฌูุฏูุง ุนูู ุงูุฅุทูุงู:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['โPar', 'โdรฉ', 'f', 'aut', ',', 'โdรฉ', 've', 'lop', 'per', 'โles', 'โfil', 's', 'โde', 'โdiscussion', '</s>']
['โPar', 'โdรฉfaut', ',', 'โdรฉvelopper', 'โles', 'โfils', 'โde', 'โdiscussion', '</s>']
```

ููุง ูุฑูุ ูุคุฏู ุงุณุชุฎุฏุงู tokenizer ุงูุฅูุฌููุฒูุฉ ููุนุงูุฌุฉ ุฌููุฉ ูุฑูุณูุฉ ุฅูู ุงููุฒูุฏ ูู ุงูุฑููุฒุ ุญูุซ ูุง ูุนุฑู tokenizer ุฃู ูููุงุช ูุฑูุณูุฉ (ุจุงุณุชุซูุงุก ุชูู ุงูุชู ุชุธูุฑ ุฃูุถูุง ูู ุงููุบุฉ ุงูุฅูุฌููุฒูุฉุ ูุซู "discussion").

ุจูุง ุฃู `inputs` ุนุจุงุฑุฉ ุนู ูุงููุณ ุจููุงุชูุญูุง ุงููุนุชุงุฏุฉ (ูุนุฑููุงุช ุงููุฏุฎูุงุชุ ููุงุน ุงูุงูุชุจุงูุ ุฅูุฎ)ุ ูุฅู ุงูุฎุทูุฉ ุงูุฃุฎูุฑุฉ ูู ุชุญุฏูุฏ ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุชู ุณูุทุจููุง ุนูู ูุฌููุนุงุช ุงูุจูุงูุงุช:

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

ูุงุญุธ ุฃููุง ูุญุฏุฏ ููุณ ุงูุทูู ุงูุฃูุตู ููุฏุฎูุงุชูุง ููุฎุฑุฌุงุชูุง. ูุธุฑูุง ูุฃู ุงููุตูุต ุงูุชู ูุชุนุงูู ูุนูุง ุชุจุฏู ูุตูุฑุฉ ุฌุฏูุงุ ูุณุชุฎุฏู 128.

<Tip>

๐ก ุฅุฐุง ููุช ุชุณุชุฎุฏู ูููุฐุฌ T5 (ุนูู ูุฌู ุงูุชุญุฏูุฏุ ุฅุญุฏู ููุงุท ุชููู `t5-xxx`)ุ ูุณููุชููุน ุงููููุฐุฌ ุฃู ุชุญุชูู ุงููุฏุฎูุงุช ุงููุตูุฉ ุนูู ุจุงุฏุฆุฉ ุชุดูุฑ ุฅูู ุงููููุฉ ููุฏ ุงูุชูููุฐุ ูุซู `translate: English to French:`.

</Tip>

<Tip warning={true}>

โ๏ธ ูุง ูููู ุงูุชูุงููุง ูููุงุน ุงูุงูุชุจุงู ููุฃูุฏุงูุ ุญูุซ ูู ูุชููุนู ุงููููุฐุฌ. ุจุฏูุงู ูู ุฐููุ ูุฌุจ ุชุนููู ุงูุนูุงูุงุช ุงูููุงุจูุฉ ูุฑููุฒ ุงูุญุดู ุฅูู `-100` ุญุชู ูุชู ุชุฌุงูููุง ูู ุญุณุงุจ ุงูุฎุณุงุฑุฉ. ุณูุชู ุฐูู ุจูุงุณุทุฉ ุฌุงูุน ุงูุจูุงูุงุช ุงูุฎุงุต ุจูุง ูุงุญููุง ุญูุซ ูุทุจู ุงูุญุดู ุงูุฏููุงููููุ ูููู ุฅุฐุง ููุช ุชุณุชุฎุฏู ุงูุญุดู ููุงุ ููุฌุจ ุนููู ุชูููู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ูุชุนููู ุฌููุน ุงูุนูุงูุงุช ุงูุชู ุชูุงุจู ุฑูุฒ ุงูุญุดู ุฅูู `-100`.

</Tip>

ุงูุขู ูููููุง ุชุทุจูู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ูู ุฎุทูุฉ ูุงุญุฏุฉ ุนูู ุฌููุน ุฃูุณุงู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

ุงูุขู ุจุนุฏ ูุนุงูุฌุฉ ุงูุจูุงูุงุชุ ูุญู ูุณุชุนุฏูู ูุชุฏุฑูุจ ูููุฐุฌูุง ุงูููุฏุฑุจ ูุณุจููุง!

{#if fw === 'pt'}

## ุชุฏุฑูุจ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `Trainer`[[fine-tuning-the-model-with-the-trainer-api]]

ุณูููู ุงูููุฏ ุงููุนูู ุจุงุณุชุฎุฏุงู `Trainer` ูู ููุณู ููุง ูุงู ูู ูุจูุ ูุน ุชุบููุฑ ุจุณูุท ูุงุญุฏ: ูุณุชุฎุฏู ููุง [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer)ุ ููู ูุฆุฉ ูุฑุนูุฉ ูู `Trainer` ูุงูุชู ุณุชุณูุญ ููุง ุจุงูุชุนุงูู ุจุดูู ุตุญูุญ ูุน ุงูุชููููุ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `generate()` ููุชูุจุค ุจุงููุฎุฑุฌุงุช ูู ุงููุฏุฎูุงุช. ุณูุบูุต ูู ุฐูู ุจูุฒูุฏ ูู ุงูุชูุตูู ุนูุฏูุง ูุชุญุฏุซ ุนู ุญุณุงุจ ุงููููุงุณ.

ุฃููุงู ููุจู ูู ุดูุกุ ูุญุชุงุฌ ุฅูู ูููุฐุฌ ูุนูู ูุชุฏุฑูุจู. ุณูุณุชุฎุฏู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `AutoModel` ุงููุนุชุงุฏุฉ:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ุชุฏุฑูุจ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู Keras[[fine-tuning-the-model-with-keras]]

ุฃููุงู ููุจู ูู ุดูุกุ ูุญุชุงุฌ ุฅูู ูููุฐุฌ ูุนูู ูุชุฏุฑูุจู. ุณูุณุชุฎุฏู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `AutoModel` ุงููุนุชุงุฏุฉ:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

๐ก ููุทุฉ ุงูุชุญูู 'Helsinki-NLP/opus-mt-en-fr' ุชุญุชูู ููุท ุนูู ุฃูุฒุงู PyTorchุ ูุฐูู ุณุชุญุตู ุนูู ุฎุทุฃ ุฅุฐุง ุญุงููุช ุชุญููู ุงููููุฐุฌ ุจุฏูู ุงุณุชุฎุฏุงู ุงูุญุฌุฉ 'from_pt=True' ูู ุทุฑููุฉ 'from_pretrained()'. ุนูุฏ ุชุญุฏูุฏ 'from_pt=True'ุ ุณูููู ุงูููุชุจุฉ ุชููุงุฆููุง ุจุชูุฒูู ูุชุญููู ุฃูุฒุงู PyTorch ูู. ููุง ุชุฑูุ ูู ุงูุณูู ุฌุฏูุง ุงูุชุจุฏูู ุจูู ุงูุฃุทุฑ ูู ๐ค Transformers!

</Tip>

{/if}

ูุงุญุธ ุฃููุง ูุฐู ุงููุฑุฉ ูุณุชุฎุฏู ูููุฐุฌูุง ุชู ุชุฏุฑูุจู ุนูู ูููุฉ ุชุฑุฌูุฉ ููููู ุงุณุชุฎุฏุงูู ุจุงููุนูุ ูุฐูู ูุง ููุฌุฏ ุชุญุฐูุฑ ุจุดุฃู ุงูุฃูุฒุงู ุงูููููุฏุฉ ุฃู ุชูู ุงูุชู ุชู ุชููุฆุชูุง ุญุฏูุซูุง.

### ุชุฌููุน ุงูุจูุงูุงุช [[data-collation]]

ุณูุญุชุงุฌ ุฅูู ูุฌูุน ุจูุงูุงุช ููุชุนุงูู ูุน ุงูุชูุณูุฏ ููุฏูุนุงุช ุงูุฏููุงููููุฉ. ูุง ูููููุง ุจุจุณุงุทุฉ ุงุณุชุฎุฏุงู 'DataCollatorWithPadding' ููุง ูู [ุงููุตู 3](/course/chapter3) ูู ูุฐู ุงูุญุงูุฉุ ูุฃู ุฐูู ูููู ููุท ุจุชูุณูุฏ ุงููุฏุฎูุงุช (ูุนุฑููุงุช ุงูุฅุฏุฎุงูุ ููุงุน ุงูุงูุชุจุงูุ ููุนุฑููุงุช ููุน ุงูุฑูุฒ). ูุฌุจ ุฃูุถูุง ุชูุณูุฏ ุงูุชุตูููุงุช ุฅูู ุงูุทูู ุงูุฃูุตู ุงูุฐู ุชู ุงูุนุซูุฑ ุนููู ูู ุงูุชุตูููุงุช. ูููุง ุฐูุฑูุง ุณุงุจููุงุ ูุฌุจ ุฃู ุชููู ูููุฉ ุงูุชูุณูุฏ ุงููุณุชุฎุฏูุฉ ูุชูุณูุฏ ุงูุชุตูููุงุช -100 ูููุณ ุฑูุฒ ุงูุชูุณูุฏ ูููุญูู ุงููุบููุ ููุชุฃูุฏ ูู ุชุฌุงูู ุชูู ุงูููู ุงูููุณุฏุฉ ูู ุญุณุงุจ ุงูุฎุณุงุฑุฉ.

ูุชู ุฐูู ููู ุจูุงุณุทุฉ ['DataCollatorForSeq2Seq'](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). ูุซู 'DataCollatorWithPadding'ุ ูุฅูู ูุฃุฎุฐ 'tokenizer' ุงููุณุชุฎุฏู ููุนุงูุฌุฉ ุงููุฏุฎูุงุช ูุณุจููุงุ ููููู ูุฃุฎุฐ ุฃูุถูุง 'model'. ูุฐุง ูุฃู ูุฌูุน ุงูุจูุงูุงุช ูุฐุง ุณูููู ูุณุคููุงู ุฃูุถูุง ุนู ุฅุนุฏุงุฏ ูุนุฑููุงุช ุฅุฏุฎุงู ูู ุงูุชุดููุฑุ ูุงูุชู ูู ุฅุตุฏุงุฑุงุช ููุฒุงุญุฉ ูู ุงูุชุตูููุงุช ูุน ุฑูุฒ ุฎุงุต ูู ุงูุจุฏุงูุฉ. ูุธุฑูุง ูุฃู ูุฐุง ุงูุชุญูู ูุชู ุจุดูู ูุฎุชูู ููููุงู ููููุฏุณุงุช ุงููุนูุงุฑูุฉ ุงููุฎุชููุฉุ ูุญุชุงุฌ 'DataCollatorForSeq2Seq' ุฅูู ูุนุฑูุฉ ูุงุฆู 'model':

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

ูุงุฎุชุจุงุฑ ูุฐุง ุนูู ุจุนุถ ุงูุนููุงุชุ ูุง ุนูููุง ุณูู ุงุณุชุฏุนุงุฆู ุนูู ูุงุฆูุฉ ูู ุงูุฃูุซูุฉ ูู ูุฌููุนุฉ ุจูุงูุงุชูุง ุงููุนููุฉ:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

ูููููุง ุงูุชุญูู ูู ุชูุณูุฏ ุงูุชุตูููุงุช ุฅูู ุงูุทูู ุงูุฃูุตู ููุฏูุนุฉุ ุจุงุณุชุฎุฏุงู -100:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

ููููููุง ุฃูุถูุง ุฅููุงุก ูุธุฑุฉ ุนูู ูุนุฑููุงุช ุฅุฏุฎุงู ูู ุงูุชุดููุฑุ ููุนุฑูุฉ ุฃููุง ุฅุตุฏุงุฑุงุช ููุฒุงุญุฉ ูู ุงูุชุตูููุงุช:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

ูุฐู ูู ุงูุชุตูููุงุช ููุนูุตุฑ ุงูุฃูู ูุงูุซุงูู ูู ูุฌููุนุฉ ุจูุงูุงุชูุง:

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

ุณููุฑุฑ ูุฐุง 'data_collator' ุฅูู 'Seq2SeqTrainer'. ุจุนุฏ ุฐููุ ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุงููููุงุณ.

{:else}

ูููููุง ุงูุขู ุงุณุชุฎุฏุงู ูุฐุง 'data_collator' ูุชุญููู ูู ูุฌููุนุฉ ุจูุงูุงุชูุง ุฅูู 'tf.data.Dataset'ุ ุฌุงูุฒุฉ ููุชุฏุฑูุจ:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### ุงูููุงููุณ [[metrics]]

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

ุชุถูู ููุฒุฉ 'Seq2SeqTrainer' ุฅูู ูุฆุฉ 'Trainer' ุงูุฑุฆูุณูุฉ ุงููุฏุฑุฉ ุนูู ุงุณุชุฎุฏุงู ุทุฑููุฉ 'generate()' ุฃุซูุงุก ุงูุชูููู ุฃู ุงูุชูุจุค. ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุณูุณุชุฎุฏู ุงููููุฐุฌ 'decoder_input_ids' ูุน ููุงุน ุงูุงูุชุจุงู ููุชุฃูุฏ ูู ุฃูู ูุง ูุณุชุฎุฏู ุงูุฑููุฒ ุจุนุฏ ุงูุฑูุฒ ุงูุฐู ูุญุงูู ุงูุชูุจุค ุจูุ ูุชุณุฑูุน ุงูุชุฏุฑูุจ. ุฃุซูุงุก ุงูุงุณุชุฏูุงูุ ูู ูุชููู ูู ุงุณุชุฎุฏุงู ุชูู ุงูุฑููุฒ ูุฃููุง ูู ูููู ุงูุชุตูููุงุชุ ูุฐูู ููู ุงูุฌูุฏ ุชูููู ูููุฐุฌูุง ุจููุณ ุงูุฅุนุฏุงุฏ.

ููุง ุฑุฃููุง ูู [ุงููุตู 1](/course/chapter1/6)ุ ูููู ูู ุงูุชุดููุฑ ุจุงูุงุณุชุฏูุงู ุนู ุทุฑูู ุงูุชูุจุค ุจุงูุฑููุฒ ูุงุญุฏูุง ุชูู ุงูุขุฎุฑ - ููู ุดูุก ูุชู ุชูููุฐู ุฎูู ุงูููุงููุณ ูู ๐ค Transformers ุจูุงุณุทุฉ ุทุฑููุฉ 'generate()'. ุณูุณูุญ ููุง 'Seq2SeqTrainer' ุจุงุณุชุฎุฏุงู ุชูู ุงูุทุฑููุฉ ููุชูููู ุฅุฐุง ูููุง ุจุชุนููู 'predict_with_generate=True'.

{/if}

ุงููููุงุณ ุงูุชูููุฏู ุงููุณุชุฎุฏู ููุชุฑุฌูุฉ ูู [ุฏุฑุฌุฉ BLEU](https://en.wikipedia.org/wiki/BLEU)ุ ุงูุชู ุชู ุชูุฏูููุง ูู [ููุงู ุนุงู 2002](https://aclanthology.org/P02-1040.pdf) ุจูุงุณุทุฉ ููุดูุฑ ุจุงุจููููู ูุขุฎุฑูู. ุชูููู ุฏุฑุฌุฉ BLEU ูุฏู ูุฑุจ ุงูุชุฑุฌูุงุช ูู ุชุตูููุงุชูุง. ูุง ุชููุณ ูุงุจููุฉ ุงูููู ุฃู ุงูุตุญุฉ ุงููุญููุฉ ููููุงุชุฌ ุงููููุฏุฉ ูููููุฐุฌุ ูููููุง ุชุณุชุฎุฏู ููุงุนุฏ ุฅุญุตุงุฆูุฉ ููุชุฃูุฏ ูู ุฃู ุฌููุน ุงููููุงุช ูู ุงูููุงุชุฌ ุงููููุฏุฉ ุชุธูุฑ ุฃูุถูุง ูู ุงูุฃูุฏุงู. ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ููุงู ููุงุนุฏ ุชุนุงูุจ ุงูุชูุฑุงุฑุงุช ูููุณ ุงููููุงุช ุฅุฐุง ูู ุชูู ุฃูุถูุง ูุชูุฑุฑุฉ ูู ุงูุฃูุฏุงู (ูุชุฌูุจ ุฅุฎุฑุงุฌ ุงููููุฐุฌ ูุฌูู ูุซู "the the the the the") ูุงูุฌูู ุงูุฃูุตุฑ ูู ุชูู ุงูููุฌูุฏุฉ ูู ุงูุฃูุฏุงู (ูุชุฌูุจ ุฅุฎุฑุงุฌ ุงููููุฐุฌ ูุฌูู ูุซู "the").

ุฃุญุฏ ุฃูุฌู ุงูุถุนู ูู BLEU ูู ุฃูู ูุชููุน ุฃู ูููู ุงููุต ูุนูููุง ุจุงููุนูุ ููุง ูุฌุนู ูู ุงูุตุนุจ ููุงุฑูุฉ ุงูุฏุฑุฌุงุช ุจูู ุงูููุงุฐุฌ ุงูุชู ุชุณุชุฎุฏู ูุญููุงุช ูุบููุฉ ูุฎุชููุฉ. ูุฐูู ุจุฏูุงู ูู ุฐููุ ูุฅู ุงููููุงุณ ุงูุฃูุซุฑ ุงุณุชุฎุฏุงููุง ุงูููู ูููุงุฑูุฉ ููุงุฐุฌ ุงูุชุฑุฌูุฉ ูู [SacreBLEU](https://github.com/mjpost/sacrebleu)ุ ูุงูุฐู ูุนุงูุฌ ูุฐุง ุงูุถุนู (ูุบูุฑู) ูู ุฎูุงู ุชูุญูุฏ ุฎุทูุฉ ุงููุนุงูุฌุฉ. ูุงุณุชุฎุฏุงู ูุฐุง ุงููููุงุณุ ูุญุชุงุฌ ุฃููุงู ุฅูู ุชุซุจูุช ููุชุจุฉ SacreBLEU:

```py
!pip install sacrebleu
```

ุจุนุฏ ุฐูู ูููููุง ุชุญูููู ุนุจุฑ 'evaluate.load()' ููุง ูุนููุง ูู [ุงููุตู 3](/course/chapter3):

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

ูุฐุง ุงููููุงุณ ุณูุฃุฎุฐ ุงููุตูุต ููุฏุฎูุงุช ูุฃูุฏุงู. ุชู ุชุตูููู ููุจูู ุนุฏุฉ ุฃูุฏุงู ููุจููุฉุ ุญูุซ ุบุงูุจูุง ูุง ุชููู ููุงู ุชุฑุฌูุงุช ููุจููุฉ ูุชุนุฏุฏุฉ ูููุณ ุงูุฌููุฉ - ุชููุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ูุณุชุฎุฏููุง ูุงุญุฏุฉ ููุทุ ูููู ูู ุบูุฑ ุบูุฑ ุงููุนุชุงุฏ ูู NLP ุงูุนุซูุฑ ุนูู ูุฌููุนุงุช ุจูุงูุงุช ุชุนุทู ุนุฏุฉ ุฌูู ูุชุตูููุงุช. ูุฐููุ ูุฌุจ ุฃู ุชููู ุงูุชูุจุคุงุช ูุงุฆูุฉ ูู ุงูุฌููุ ูููู ุงููุฑุงุฌุน ูุฌุจ ุฃู ุชููู ูุงุฆูุฉ ูู ููุงุฆู ุงูุฌูู.

ุฏุนูุง ูุฌุฑุจ ูุซุงู:

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

ูุฐุง ูุญุตู ุนูู ุฏุฑุฌุฉ BLEU ุชุจูุบ 46.75ุ ููู ุฌูุฏ ุฌุฏูุง - ููุฑุฌูุน ุฅูููุ ุญูู ุงููููุฐุฌ ุงูุฃุตูู ูููุญูู ูู ูุฑูุฉ ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) ุฏุฑุฌุฉ BLEU ุชุจูุบ 41.8 ุนูู ูููุฉ ุชุฑุฌูุฉ ููุงุซูุฉ ุจูู ุงูุฅูุฌููุฒูุฉ ูุงููุฑูุณูุฉ! (ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุงูููุงููุณ ุงููุฑุฏูุฉุ ูุซู 'counts' ู 'bp'ุ ุฑุงุฌุน [ูุณุชูุฏุน SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) ูู ูุงุญูุฉ ุฃุฎุฑูุ ุฅุฐุง ุญุงูููุง ูุน ููุนู ุงูุชูุจุคุงุช ุงูุณูุฆูู (ุงููุซูุฑ ูู ุงูุชูุฑุงุฑุงุช ุฃู ุงูุฃูุตุฑ ูู ุงููุงุฒู) ุงูุชู ุบุงูุจูุง ูุง ุชุฎุฑุฌ ูู ููุงุฐุฌ ุงูุชุฑุฌูุฉุ ูุณูุญุตู ุนูู ุฏุฑุฌุงุช BLEU ุณูุฆุฉ ุฌุฏูุง:

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

ูููู ุฃู ูุชุฑุงูุญ ุงูุชูููู ูู 0 ุฅูู 100ุ ููููุง ูุงู ุฃุนูู ูุงู ุฃูุถู.

{#if fw === 'tf'}

ููุญุตูู ุนูู ุงููุตูุต ุงูุชู ูููู ุฃู ุชุณุชุฎุฏููุง ุงููุชุฑุฌูุฉ ูู ูุฎุฑุฌุงุช ุงููููุฐุฌุ ุณูุณุชุฎุฏู ุทุฑููุฉ `tokenizer.batch_decode()`. ูู ูุง ุนูููุง ูุนูู ูู ุชูุธูู ุฌููุน `-100`s ูู ุงูุชุตูููุงุชุ ุณูููู ุงููุญูู ุงูุฑูุฒู ุชููุงุฆููุง ุจููุณ ุงูุดูุก ุจุงููุณุจุฉ ูุฑููุฒ ุงูุญุดู. ุฏุนูุง ูุญุฏุฏ ุฏุงูุฉ ุชุฃุฎุฐ ูููุฐุฌูุง ููุฌููุนุฉ ุจูุงูุงุชูุง ูุชุญุณุจ ุงูููุงููุณ ุนูููุง. ุณูุณุชุฎุฏู ุฃูุถูุง ุฎุฏุนุฉ ุชุฒูุฏ ุงูุฃุฏุงุก ุจุดูู ูุจูุฑ - ุชุฌููุน ููุฏ ุงูุชูููุฏ ุงูุฎุงุต ุจูุง ูุน [XLA](https://www.tensorflow.org/xla)ุ ููู ูุชุฑุฌู ุฌุจุฑ ุฎุทู ูุชุณุงุฑุน ูู TensorFlow. ูุทุจู XLA ุชุญุณููุงุช ูุฎุชููุฉ ุนูู ุฑุณู ุงูุญูุณุจุฉ ูููููุฐุฌุ ููุคุฏู ุฅูู ุชุญุณููุงุช ูุจูุฑุฉ ูู ุงูุณุฑุนุฉ ูุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ. ููุง ูู ููุถุญ ูู ูุฏููุฉ Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate)ุ ูุนูู XLA ุจุดูู ุฃูุถู ุนูุฏูุง ูุง ุชุฎุชูู ุฃุดูุงู ุงูุฅุฏุฎุงู ูุฏููุง ูุซูุฑูุง. ููุชุนุงูู ูุน ูุฐุงุ ุณูููู ุจููุก ุฅุฏุฎุงูุงุชูุง ุฅูู ูุถุงุนูุงุช 128ุ ูุณูููู ุจุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ ูุน ูุฌูุน ุงูุญุดูุ ุซู ุณูุทุจู ุงูุฏูููุฑ `@tf.function(jit_compile=True)` ุนูู ุฏุงูุฉ ุงูุชูููุฏ ุงูุฎุงุตุฉ ุจูุงุ ูุงูุชู ุชุดูุฑ ุฅูู ุงูุฏุงูุฉ ุจุฃููููุง ููุชุฌููุน ูุน XLA.

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )


def compute_metrics():
    all_preds = []
    all_labels = []

    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

ููุญุตูู ุนูู ุงููุตูุต ุงูุชู ูููู ุฃู ุชุณุชุฎุฏููุง ุงููุชุฑุฌูุฉ ูู ูุฎุฑุฌุงุช ุงููููุฐุฌุ ุณูุณุชุฎุฏู ุทุฑููุฉ `tokenizer.batch_decode()`. ูู ูุง ุนูููุง ูุนูู ูู ุชูุธูู ุฌููุน `-100`s ูู ุงูุชุตูููุงุช (ุณูููู ุงููุญูู ุงูุฑูุฒู ุชููุงุฆููุง ุจููุณ ุงูุดูุก ุจุงููุณุจุฉ ูุฑููุฒ ุงูุญุดู):

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # ูู ุญุงูุฉ ุนูุฏุฉ ุงููููุฐุฌ ูุฃูุซุฑ ูู ุงุญุชูุงูุงุช ุงูุชูุจุค
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # ุงุณุชุจุฏุงู -100s ูู ุงูุชุตูููุงุช ุญูุซ ูุง ูููููุง ูู ุชุดููุฑูุง
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ุจุนุถ ุงููุนุงูุฌุฉ ุงูุจุณูุทุฉ
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

ุงูุขู ุจุนุฏ ุฃู ุงูุชูููุง ูู ุฐููุ ูุญู ูุณุชุนุฏูู ูุถุจุท ูููุฐุฌูุง ุงูุฏููู!

### ุถุจุท ุฏูุฉ ุงููููุฐุฌ[[fine-tuning-the-model]]

ุงูุฎุทูุฉ ุงูุฃููู ูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Faceุ ุญุชู ุชุชููู ูู ุชุญููู ูุชุงุฆุฌู ุฅูู Model Hub. ููุงู ุฏุงูุฉ ููุงุฆูุฉ ููุณุงุนุฏุชู ูู ุฐูู ูู ุฏูุชุฑ ุงูููุงุญุธุงุช:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ุณูุชู ุนุฑุถ ุนูุตุฑ ูุงุฌูุฉ ูุณุชุฎุฏู ุญูุซ ููููู ุฅุฏุฎุงู ุจูุงูุงุช ุงุนุชูุงุฏ ุชุณุฌูู ุฏุฎูู Hugging Face ุงูุฎุงุตุฉ ุจู.

ุฅุฐุง ูู ุชูู ุชุนูู ูู ุฏูุชุฑ ููุงุญุธุงุชุ ููุง ุนููู ุณูู ูุชุงุจุฉ ุงูุณุทุฑ ุงูุชุงูู ูู ุทุฑููุชู:

```bash
huggingface-cli login
```

{#if fw === 'tf'}

ูุจู ุฃู ูุจุฏุฃุ ุฏุนูุง ูุฑู ูุง ูู ุงููุชุงุฆุฌ ุงูุชู ูุญุตู ุนูููุง ูู ูููุฐุฌูุง ุจุฏูู ุฃู ุชุฏุฑูุจ:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุฐููุ ูููููุง ุฅุนุฏุงุฏ ูู ูุง ูุญุชุงุฌู ูุชุฌููุน ูุชุฏุฑูุจ ูููุฐุฌูุง. ูุงุญุธ ุงุณุชุฎุฏุงู `tf.keras.mixed_precision.set_global_policy("mixed_float16")` - ุณูุฎุจุฑ ูุฐุง Keras ุจุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู float16ุ ูุงูุฐู ูููู ุฃู ูุนุทู ุชุณุฑูุนูุง ูุจูุฑูุง ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุงูุชู ุชุฏุนูู (Nvidia 20xx/V100 ุฃู ุฃุญุฏุซ).

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ ูู ุนุฏุฏ ุงูุนููุงุช ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ููุณููุฉ ุนูู ุญุฌู ุงูุฏูุนุฉ ุซู ูุถุฑูุจุฉ
# ุจุนุฏุฏ ุงููุชุฑุงุช ุงูุฅุฌูุงููุฉ. ูุงุญุธ ุฃู tf_train_dataset ููุง ูู tf.data.Dataset ุฐู ุฏูุนุงุชุ
# ูููุณ ูุฌููุนุฉ ุจูุงูุงุช Hugging Face ุงูุฃุตููุฉุ ูุฐุง ูุฅู len() ุงูุฎุงุตุฉ ุจู ูู ุจุงููุนู num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ุชุฏุฑูุจ ูู float16 ูุฎุชูุท ุงูุฏูุฉ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ุจุนุฏ ุฐููุ ูุญุฏุฏ `PushToHubCallback` ูุชุญููู ูููุฐุฌูุง ุฅูู Hub ุฃุซูุงุก ุงูุชุฏุฑูุจุ ููุง ุฑุฃููุง ูู [ุงููุณู 2]((/course/chapter7/2))ุ ุซู ูููู ุจุจุณุงุทุฉ ุจุชูุงุณุจ ุงููููุฐุฌ ูุน ูุฐุง ุงูุงุณุชุฏุนุงุก:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

ูุงุญุธ ุฃูู ููููู ุชุญุฏูุฏ ุงุณู ุงููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุฏูุนู ุจุงุณุชุฎุฏุงู ุญุฌุฉ `hub_model_id` (ุนูู ูุฌู ุงูุฎุตูุตุ ุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ูุฐู ุงูุญุฌุฉ ููุฏูุน ุฅูู ููุธูุฉ). ุนูู ุณุจูู ุงููุซุงูุ ุนูุฏูุง ูููุง ุจุฏูุน ุงููููุฐุฌ ุฅูู ููุธูุฉ [`huggingface-course` organization](https://huggingface.co/huggingface-course)ุ ุฃุถููุง `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` ุฅูู `Seq2SeqTrainingArguments`. ุจุดูู ุงูุชุฑุงุถูุ ุณูุชู ุงุณุชุฎุฏุงู ุงููุณุชูุฏุน ุงูููุฌูุฏ ูู ูุณุงุญุฉ ุงูุงุณู ุงูุฎุงุตุฉ ุจู ูุงููุณูู ุจุงุณู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ููุช ุจุชุนููููุ ูุฐุง ุณูููู ููุง `"sgugger/marian-finetuned-kde4-en-to-fr"` (ููู ุงููููุฐุฌ ุงูุฐู ูููุง ุจุฑุจุทู ูู ุจุฏุงูุฉ ูุฐุง ุงููุณู).

<Tip>

๐ก ุฅุฐุง ูุงู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ุชุณุชุฎุฏูู ููุฌูุฏูุง ุจุงููุนูุ ููุฌุจ ุฃู ูููู ูุณุชูุณุฎูุง ูุญูููุง ูููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุฏูุนู ุฅููู. ุฅุฐุง ูู ููู ุงูุฃูุฑ ูุฐููุ ูุณุชุญุตู ุนูู ุฎุทุฃ ุนูุฏ ุงุณุชุฏุนุงุก `model.fit()` ูุณูุชุนูู ุนููู ุชุนููู ุงุณู ุฌุฏูุฏ.

</Tip>

ุฃุฎูุฑูุงุ ุฏุนูุง ูุฑู ููู ุชุจุฏู ููุงููุณูุง ุงูุขู ุจุนุฏ ุงูุชูุงุก ุงูุชุฏุฑูุจ:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

ูู ูุฐู ุงููุฑุญูุฉุ ููููู ุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุงูุงุณุชุฏูุงู ุนูู ููุตุฉ ุงูููุงุฐุฌ ูุงุฎุชุจุงุฑ ูููุฐุฌู ููุดุงุฑูุชู ูุน ุฃุตุฏูุงุฆู. ููุฏ ููุช ุจุถุจุท ูููุฐุฌ ุจูุฌุงุญ ุนูู ูููุฉ ุงูุชุฑุฌูุฉ - ุชูุงูููุง!

{:else}

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุฐููุ ูููููุง ุชุญุฏูุฏ `Seq2SeqTrainingArguments`. ูุซู `Trainer`ุ ูุณุชุฎุฏู ูุฆุฉ ูุฑุนูุฉ ูู `TrainingArguments` ุชุญุชูู ุนูู ุจุนุถ ุงูุญููู ุงูุฅุถุงููุฉ:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

ุจุฎูุงู ุงููุนููุงุช ุงููุนุชุงุฏุฉ (ูุซู ูุนุฏู ุงูุชุนููุ ูุนุฏุฏ ุงูุนุตูุฑุ ูุญุฌู ุงูุฏูุนุฉุ ูุจุนุถ ุงูุชูุงุดู ุงููุฒูู)ุ ููุงู ุจุนุถ ุงูุชุบููุฑุงุช ููุงุฑูุฉ ุจูุง ุฑุฃููุงู ูู ุงูุฃูุณุงู ุงูุณุงุจูุฉ:

- ูุง ูููู ุจุถุจุท ุฃู ุชูููู ููุชุธูุ ุญูุซ ูุณุชุบุฑู ุงูุชูููู ุจุนุถ ุงูููุชุ ุณูููู ููุท ุจุชูููู ูููุฐุฌูุง ูุฑุฉ ูุงุญุฏุฉ ูุจู ุงูุชุฏุฑูุจ ูุจุนุฏู.
- ูุญุฏุฏ `fp16=True`ุ ููุง ูุณุฑุน ุงูุชุฏุฑูุจ ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุงูุญุฏูุซุฉ.
- ูุญุฏุฏ `predict_with_generate=True`ุ ููุง ูุงูุดูุง ุฃุนูุงู.
- ูุณุชุฎุฏู `push_to_hub=True` ูุชุญููู ุงููููุฐุฌ ุฅูู ุงูููุตุฉ ูู ููุงูุฉ ูู ุนุตุฑ.

ูุงุญุธ ุฃูู ููููู ุชุญุฏูุฏ ุงูุงุณู ุงููุงูู ูููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุชุญูููู ุจุงุณุชุฎุฏุงู ุญุฌุฉ `hub_model_id` (ุนูู ูุฌู ุงูุฎุตูุตุ ุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ูุฐู ุงูุญุฌุฉ ููุชุญููู ุฅูู ููุธูุฉ). ุนูู ุณุจูู ุงููุซุงูุ ุนูุฏูุง ูููุง ุจุชุญููู ุงููููุฐุฌ ุฅูู ููุธูุฉ [huggingface-course](https://huggingface.co/huggingface-course)ุ ุฃุถููุง `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` ุฅูู `Seq2SeqTrainingArguments`. ุจุดูู ุงูุชุฑุงุถูุ ุณูุชู ุงุณุชุฎุฏุงู ุงููุณุชูุฏุน ูู ูุณุงุญุฉ ุงุณูู ููุชู ุชุณููุชู ููููุง ูุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ููุช ุจุชุนููููุ ูุฐุง ูู ุญุงูุชูุง ุณูููู `"sgugger/marian-finetuned-kde4-en-to-fr"` (ููู ุงููููุฐุฌ ุงูุฐู ูููุง ุจุฑุจุทู ูู ุจุฏุงูุฉ ูุฐุง ุงููุณู).

<Tip>

๐ก ุฅุฐุง ูุงู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ุชุณุชุฎุฏูู ููุฌูุฏูุง ุจุงููุนูุ ููุฌุจ ุฃู ูููู ูุณุชูุณุฎูุง ูุญูููุง ูููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุชุญูููู. ุฅุฐุง ูู ููู ูุฐููุ ูุณุชุญุตู ุนูู ุฎุทุฃ ุนูุฏ ุชุญุฏูุฏ `Seq2SeqTrainer` ูุณูุชุนูู ุนููู ุชุนููู ุงุณู ุฌุฏูุฏ.

</Tip>

ุฃุฎูุฑูุงุ ูููู ููุท ุจุชูุฑูุฑ ูู ุดูุก ุฅูู `Seq2SeqTrainer`:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

ูุจู ุงูุชุฏุฑูุจุ ุณูููู ูุธุฑุฉ ุฃููุงู ุนูู ุงููุชูุฌุฉ ุงูุชู ูุญุตู ุนูููุง ูููุฐุฌูุงุ ููุชุฃูุฏ ูู ุฃููุง ูุง ูุฌุนู ุงูุฃููุฑ ุฃุณูุฃ ูุน ุงูุถุจุท ุงูุฏููู. ุณุชุณุชุบุฑู ูุฐู ุงูุฃูุงูุฑ ุจุนุถ ุงูููุชุ ูุฐุง ููููู ุชูุงูู ุงููููุฉ ุฃุซูุงุก ุงูุชูููุฐ:

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

ุฏุฑุฌุฉ BLEU 39 ููุณุช ุณูุฆุฉ ููุบุงูุฉุ ููุง ูุนูุณ ุญูููุฉ ุฃู ูููุฐุฌูุง ุฌูุฏ ุจุงููุนู ูู ุชุฑุฌูุฉ ุงูุฌูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงููุฑูุณูุฉ.

ุงูุชุงูู ูู ุงูุชุฏุฑูุจุ ูุงูุฐู ุณูุณุชุบุฑู ุฃูุถูุง ุจุนุถ ุงูููุช:

```python
trainer.train()
```

ูุงุญุธ ุฃูู ุฃุซูุงุก ุญุฏูุซ ุงูุชุฏุฑูุจุ ูููุง ุชู ุญูุธ ุงููููุฐุฌ (ููุงุ ูู ุนุตุฑ) ูุชู ุชุญูููู ุฅูู ุงูููุตุฉ ูู ุงูุฎูููุฉ. ุจูุฐู ุงูุทุฑููุฉุ ุณุชุชููู ูู ุงุณุชุฆูุงู ุชุฏุฑูุจู ุนูู ุขูุฉ ุฃุฎุฑู ุฅุฐุง ูุฒู ุงูุฃูุฑ.

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุงูุชุฏุฑูุจุ ูููู ุจุชูููู ูููุฐุฌูุง ูุฑุฉ ุฃุฎุฑู - ูุฃูู ุฃู ูุฑู ุจุนุถ ุงูุชุญุณู ูู ุฏุฑุฌุฉ BLEU!

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

ูุฐุง ุชุญุณู ุจูุญู 14 ููุทุฉุ ููู ุฃูุฑ ุฑุงุฆุน.

ุฃุฎูุฑูุงุ ูุณุชุฎุฏู ุทุฑููุฉ `push_to_hub()` ููุชุฃูุฏ ูู ุชุญููู ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู ุงููููุฐุฌ. ูููู `Trainer` ุฃูุถูุง ุจุตูุงุบุฉ ุจุทุงูุฉ ูููุฐุฌ ุจุฌููุน ูุชุงุฆุฌ ุงูุชูููู ูุชุญููููุง. ุชุญุชูู ุจุทุงูุฉ ุงููููุฐุฌ ูุฐู ุนูู ุจูุงูุงุช ูุตููุฉ ุชุณุงุนุฏ ููุตุฉ ุงูููุงุฐุฌ ุนูู ุงุฎุชูุงุฑ ุฃุฏุงุฉ ุงูุงุณุชุฏูุงู ูุชุฌุฑุจุฉ ุงูุงุณุชุฏูุงู. ุนุงุฏุฉุ ูุง ุชูุฌุฏ ุญุงุฌุฉ ูููู ุฃู ุดูุก ุญูุซ ููููู ุงุณุชูุชุงุฌ ุงูุฃุฏุงุฉ ุงูุตุญูุญุฉ ูู ูุฆุฉ ุงููููุฐุฌุ ูููู ูู ูุฐู ุงูุญุงูุฉุ ูููู ุงุณุชุฎุฏุงู ููุณ ูุฆุฉ ุงููููุฐุฌ ูุฌููุน ุฃููุงุน ุงููุดููุงุช ุงูุชุณูุณููุฉุ ูุฐุง ูุญุฏุฏ ุฃููุง ูููุฐุฌ ุชุฑุฌูุฉ:

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

ุชุนูุฏ ูุฐู ุงูุฃูุงูุฑ ุนููุงู URL ููุงูุชุฒุงู ุงูุฐู ูุงู ุจู ููุชูุ ุฅุฐุง ููุช ุชุฑูุฏ ูุญุตู:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

ูู ูุฐู ุงููุฑุญูุฉุ ููููู ุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุงูุงุณุชุฏูุงู ุนูู ููุตุฉ ุงูููุงุฐุฌ ูุงุฎุชุจุงุฑ ูููุฐุฌู ููุดุงุฑูุชู ูุน ุฃุตุฏูุงุฆู. ููุฏ ููุช ุจุถุจุท ูููุฐุฌ ุจูุฌุงุญ ุนูู ูููุฉ ุงูุชุฑุฌูุฉ - ุชูุงูููุง!

ุฅุฐุง ููุช ุชุฑูุฏ ุงูุบูุต ุจุดูู ุฃุนูู ููููุงู ูู ุญููุฉ ุงูุชุฏุฑูุจุ ูุณูุฑููู ุงูุขู ููููุฉ ุงูููุงู ุจููุณ ุงูุดูุก ุจุงุณุชุฎุฏุงู ๐ค Accelerate.

{/if}

{#if fw === 'pt'}

## ุญููุฉ ุชุฏุฑูุจ ูุฎุตุตุฉ [[a-custom-training-loop]]

ุฏุนููุง ูููู ูุธุฑุฉ ุงูุขู ุนูู ุญููุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉุ ุจุญูุซ ููููู ุชุฎุตูุต ุงูุฃุฌุฒุงุก ุงูุชู ุชุญุชุงุฌูุง. ุณุชุจุฏู ูุดุงุจูุฉ ููุง ูุนููุงู ูู [ุงููุณู 2](/course/chapter7/2) ู[ุงููุตู 3](/course/chapter3/4).

### ุฅุนุฏุงุฏ ูู ุดูุก ููุชุฏุฑูุจ [[preparing-everything-for-training]]

ููุฏ ุฑุฃูุช ูู ูุฐุง ุนุฏุฉ ูุฑุงุช ุงูุขูุ ูุฐุง ุณููุฑ ุนุจุฑ ุงูููุฏ ุจุณุฑุนุฉ. ุฃููุงู ุณูุจูู `DataLoader`s ูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุงุ ุจุนุฏ ุถุจุท ูุฌููุนุงุช ุงูุจูุงูุงุช ุนูู ุชูุณูู `"torch"` ุญุชู ูุญุตู ุนูู ุชูุณูุฑุงุช PyTorch:

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

ุจุนุฏ ุฐููุ ุณูุนูุฏ ุฅูุดุงุก ูููุฐุฌูุงุ ููุชุฃูุฏ ูู ุฃููุง ูุง ููุงุตู ุงูุถุจุท ุงูุฏููู ูู ูุจู ูููู ูุจุฏุฃ ูู ุงููููุฐุฌ ุงููุณุจู ุงูุชุฏุฑูุจ ูุฑุฉ ุฃุฎุฑู:

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ุซู ุณูุญุชุงุฌ ุฅูู ูุญุณู:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ุจูุฌุฑุฏ ุญุตูููุง ุนูู ูู ูุฐู ุงูุฃุดูุงุกุ ูููููุง ุฅุฑุณุงููุง ุฅูู ุทุฑููุฉ `accelerator.prepare()`. ุชุฐูุฑ ุฃูู ุฅุฐุง ููุช ุชุฑูุฏ ุงูุชุฏุฑูุจ ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช ูู ุฏูุชุฑ ููุงุญุธุงุช Colabุ ูุณูุชุนูู ุนููู ููู ูู ูุฐุง ุงูููุฏ ุฅูู ูุธููุฉ ุชุฏุฑูุจุ ููุง ูุฌุจ ุฃู ุชููุฐ ุฃู ุฎููุฉ ุชูุดุฆ `Accelerator`.

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ุงูุขู ุจุนุฏ ุฃู ุฃุฑุณููุง `train_dataloader` ุฅูู `accelerator.prepare()`ุ ูููููุง ุงุณุชุฎุฏุงู ุทููู ูุญุณุงุจ ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ. ุชุฐูุฑ ุฃูู ูุฌุจ ุนูููุง ุฏุงุฆููุง ุงูููุงู ุจุฐูู ุจุนุฏ ุฅุนุฏุงุฏ ุฃุฏุงุฉ ุงูุชุญูููุ ุญูุซ ุณุชุบูุฑ ูุฐู ุงูุทุฑููุฉ ุทูู `DataLoader`. ูุณุชุฎุฏู ุฌุฏูููุง ุฎุทููุง ููุงุณููููุง ูู ูุนุฏู ุงูุชุนูู ุฅูู 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

ุฃุฎูุฑูุงุ ูุชุญููู ูููุฐุฌูุง ุฅูู ุงูููุตุฉุ ุณูุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุงุฆู `Repository` ูู ูุฌูุฏ ุงูุนูู. ูู ุจุชุณุฌูู ุงูุฏุฎูู ุฅูู ููุตุฉ Hugging Faceุ ุฅุฐุง ูู ุชูู ูุณุฌูุงู ุจุงููุนู. ุณูุญุฏุฏ ุงุณู ุงููุณุชูุฏุน ูู ูุนุฑู ุงููููุฐุฌ ุงูุฐู ูุฑูุฏ ุฅุนุทุงุกู ููููุฐุฌูุง (ูุง ุชุชุฑุฏุฏ ูู ุงุณุชุจุฏุงู `repo_name` ุจุฎูุงุฑู ุงูุฎุงุตุ ูุฌุจ ุฃู ูุญุชูู ููุท ุนูู ุงุณู ุงููุณุชุฎุฏู ุงูุฎุงุต ุจูุ ููู ูุง ุชูุนูู ูุธููุฉ `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

ุซู ูููููุง ุงุณุชูุณุงุฎ ุงููุณุชูุฏุน ูุฐุง ูู ูุฌูุฏ ูุญูู. ุฅุฐุง ูุงู ููุฌูุฏูุง ุจุงููุนูุ ูุฌุจ ุฃู ูููู ูุฐุง ุงููุฌูุฏ ุงููุญูู ูุณุชูุณุฎูุง ูู ุงููุณุชูุฏุน ุงูุฐู ูุนูู ุนููู:

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ุงูุขู ูููููุง ุชุญููู ุฃู ุดูุก ูุญูุธู ูู `output_dir` ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `repo.push_to_hub()`. ุณูุณุงุนุฏูุง ูุฐุง ูู ุชุญููู ุงูููุงุฐุฌ ุงููุณูุทุฉ ูู ููุงูุฉ ูู ุฏูุฑุฉ.

### ุญููุฉ ุงูุชุฏุฑูุจ[[training-loop]]

ูุญู ุงูุขู ูุณุชุนุฏูู ููุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉ. ูุชุจุณูุท ุงูุฌุฒุก ุงูุชูููููุ ูุญุฏุฏ ูุฐู ุงูุฏุงูุฉ `postprocess()` ุงูุชู ุชุฃุฎุฐ ุงูุชูุจุคุงุช ูุงูุนูุงูุงุช ูุชุญูููุง ุฅูู ููุงุฆู ูู ุงูุณูุงุณู ุงููุตูุฉ ุงูุชู ูุชููุนูุง ูุงุฆู `metric`:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # ุงุณุชุจุฏุงู -100 ูู ุงูุนูุงูุงุช ูุฃููุง ูุง ูุณุชุทูุน ูู ุชุดููุฑูุง.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ุจุนุถ ุงููุนุงูุฌุฉ ุงูุจุณูุทุฉ
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

ุชุจุฏู ุญููุฉ ุงูุชุฏุฑูุจ ูุดุงุจูุฉ ุฌุฏูุง ูุชูู ุงูููุฌูุฏุฉ ูู [ุงููุณู 2](/course/chapter7/2) ู[ุงููุตู 3](/course/chapter3)ุ ูุน ุจุนุถ ุงูุงุฎุชูุงูุงุช ูู ุงูุฌุฒุก ุงูุชููููู -- ูุฐุง ุฏุนููุง ูุฑูุฒ ุนูู ุฐูู!

ุฃูู ุดูุก ูุฌุจ ููุงุญุธุชู ูู ุฃููุง ูุณุชุฎุฏู ุทุฑููุฉ `generate()` ูุญุณุงุจ ุงูุชูุจุคุงุชุ ูููู ูุฐู ุงูุทุฑููุฉ ูู ุนูู ูููุฐุฌูุง ุงูุฃุณุงุณูุ ูููุณ ุงููููุฐุฌ ุงูููููู ๐ค Accelerate ุงูุฐู ุชู ุฅูุดุงุคู ูู ุทุฑููุฉ `prepare()`. ููุฐุง ุงูุณุจุจ ูููู ุจุฅูุบุงุก ูู ุงููููุฐุฌ ุฃููุงูุ ุซู ุงุณุชุฏุนุงุก ูุฐู ุงูุทุฑููุฉ.

ุงูุดูุก ุงูุซุงูู ูู ุฃููุ ูุซู [ุชุตููู ุงูุฑููุฒ](/course/chapter7/2)ุ ูุฏ ุชููู ุนูููุชุงู ูุฏ ุฃุถุงูุชุง ุญุดููุง ุฅูู ุงููุฏุฎูุงุช ูุงูุนูุงูุงุช ุจุฃุดูุงู ูุฎุชููุฉุ ูุฐุง ูุณุชุฎุฏู `accelerator.pad_across_processes()` ูุฌุนู ุงูุชูุจุคุงุช ูุงูุนูุงูุงุช ุจููุณ ุงูุดูู ูุจู ุงุณุชุฏุนุงุก ุทุฑููุฉ `gather()`. ุฅุฐุง ูู ููุนู ุฐููุ ูุฅู ุงูุชูููู ุฅูุง ุฃู ูุชุนุทู ุฃู ูุนูู ุฅูู ุงูุฃุจุฏ.

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # ุงูุชุฏุฑูุจ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # ุงูุชูููู
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # ุถุฑูุฑู ูุฅุถุงูุฉ ุญุดู ููุชูุจุคุงุช ูุงูุนูุงูุงุช ูุฌูุนูุง
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # ุงูุญูุธ ูุงูุชุญููู
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุฐููุ ูุฌุจ ุฃู ูููู ูุฏูู ูููุฐุฌ ุฐู ูุชุงุฆุฌ ูุดุงุจูุฉ ุฌุฏูุง ูููููุฐุฌ ุงูุฐู ุชู ุชุฏุฑูุจู ุจุงุณุชุฎุฏุงู `Seq2SeqTrainer`. ููููู ุงูุชุญูู ูู ุงููููุฐุฌ ุงูุฐู ูููุง ุจุชุฏุฑูุจู ุจุงุณุชุฎุฏุงู ูุฐุง ุงูููุฏ ูู [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). ูุฅุฐุง ููุช ุชุฑุบุจ ูู ุงุฎุชุจุงุฑ ุฃู ุชุนุฏููุงุช ุนูู ุญููุฉ ุงูุชุฏุฑูุจุ ููููู ุชูููุฐูุง ูุจุงุดุฑุฉ ุนู ุทุฑูู ุชุนุฏูู ุงูููุฏ ุงูููุถุญ ุฃุนูุงู!

{/if}

## ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงููุฏุฑุจ ุฌูุฏูุง[[using-the-fine-tuned-model]]

ููุฏ ุฃุธูุฑูุง ูู ุจุงููุนู ููู ููููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงูุฐู ูููุง ุจุชุฏุฑูุจู ุฌูุฏูุง ุนูู Model Hub ุจุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุงูุชูููู. ูุงุณุชุฎุฏุงูู ูุญูููุง ูู `pipeline`ุ ูุฌุจ ุนูููุง ููุท ุชุญุฏูุฏ ูุนุฑูู ุงููููุฐุฌ ุงูุตุญูุญ:

```py
from transformers import pipeline

# ุงุณุชุจุฏู ูุฐุง ุจูุนุฑูู ููุทุฉ ุงูุชุญูู ุงูุฎุงุตุฉ ุจู
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dรฉfaut, dรฉvelopper les fils de discussion'}]
```

ููุง ูู ูุชููุนุ ูุงู ูููุฐุฌูุง ุงููุณุจู ุงูุชุฏุฑูุจ ุจุชูููู ูุนุฑูุชู ูุน ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ูููุง ุจุชุฏุฑูุจู ุนูููุงุ ูุจุฏูุงู ูู ุชุฑู ุงููููุฉ ุงูุฅูุฌููุฒูุฉ "threads" ุจููุฑุฏูุงุ ูุฅูู ูุชุฑุฌููุง ุงูุขู ุฅูู ุงููุณุฎุฉ ุงููุฑูุณูุฉ ุงูุฑุณููุฉ. ูููุทุจู ุงูุฃูุฑ ููุณู ุนูู ูููุฉ "plugin":

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

ูุซุงู ุฑุงุฆุน ุขุฎุฑ ุนูู ุชููู ุงููุฌุงู!

<Tip>

โ๏ธ **ุฏูุฑู!** ูุงุฐุง ูุนูุฏ ุงููููุฐุฌ ุนูู ุงูุนููุฉ ุงูุชู ุชุญุชูู ุนูู ุงููููุฉ "email" ุงูุชู ุญุฏุฏุชูุง ุณุงุจููุงุ

</Tip>