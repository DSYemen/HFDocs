ูู ูุชู ุชุฑุฌูุฉ ุงูุฃุฌุฒุงุก ุงูุชู ุชุญุชูู ุนูู ุฃููุงุฏ ุจุฑูุฌูุฉ ุฃู ุฑูุงุจุท ุฃู ุฑููุฒ HTML ู CSSุ ููุง ูู ูุทููุจ. ูููุง ููู ุชุฑุฌูุฉ ุจููุฉ ุงููุต:

# ุงูุชุฑุฌูุฉ

ุฏุนููุง ุงูุขู ูุบูุต ูู ุงูุชุฑุฌูุฉ. ูุฐู ูู ูููุฉ ุฃุฎุฑู [ุชุณูุณู ุฅูู ุชุณูุณู](/course/chapter1/7)ุ ููุฐุง ูุนูู ุฃููุง ูุดููุฉ ูููู ุตูุงุบุชูุง ุนูู ุฃููุง ุงูุงูุชูุงู ูู ุชุณูุณู ุฅูู ุขุฎุฑ. ุจูุฐุง ุงููุนููุ ูุฅู ุงููุดููุฉ ูุฑูุจุฉ ุฌุฏูุง ูู [ุงูุชูุฎูุต](/course/chapter7/6)ุ ูููููู ุชูููู ูุง ุณูุฑุงู ููุง ูุน ูุดููุงุช ุฃุฎุฑู ุชุชุนูู ุจุงูุชุณูุณู ุฅูู ุชุณูุณู ูุซู:

- **ููู ุงูุฃุณููุจ**: ุฅูุดุงุก ูููุฐุฌ *ูุชุฑุฌู* ุงููุตูุต ุงูููุชูุจุฉ ุจุฃุณููุจ ูุนูู ุฅูู ุขุฎุฑ (ุนูู ุณุจูู ุงููุซุงูุ ูู ุงูุฃุณููุจ ุงูุฑุณูู ุฅูู ุบูุฑ ุงูุฑุณูู ุฃู ูู ุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ุงูุดูุณุจูุฑูุฉ ุฅูู ุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ุงูุญุฏูุซุฉ)
- **ุงูุงุณุชุฌุงุจุฉ ููุฃุณุฆูุฉ ุงูุชูููุฏูุฉ**: ุฅูุดุงุก ูููุฐุฌ ูููุฏ ุฅุฌุงุจุงุช ููุฃุณุฆูุฉุ ุจุงููุธุฑ ุฅูู ุณูุงู ูุนูู

ุฅุฐุง ูุงู ูุฏูู ูุฌููุนุฉ ุจูุงูุงุช ูุจูุฑุฉ ุจูุง ูููู ูู ุงููุตูุต ุจูุบุชูู (ุฃู ุฃูุซุฑ)ุ ูููููู ุชุฏุฑูุจ ูููุฐุฌ ุชุฑุฌูุฉ ุฌุฏูุฏ ูู ุงูุตูุฑ ููุง ุณููุนู ูู ุงููุณู ุงูุฎุงุต [ุจููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](/course/chapter7/6). ููุน ุฐููุ ุณูููู ูู ุงูุฃุณุฑุน ุถุจุท ูููุฐุฌ ุชุฑุฌูุฉ ููุฌูุฏุ ุณูุงุก ูุงู ูููุฐุฌูุง ูุชุนุฏุฏ ุงููุบุงุช ูุซู mT5 ุฃู mBART ุงูุฐู ุชุฑูุฏ ุถุจุทู ูุฒูุฌ ูุบูู ูุญุฏุฏุ ุฃู ุญุชู ูููุฐุฌ ูุชุฎุตุต ููุชุฑุฌูุฉ ูู ูุบุฉ ุฅูู ุฃุฎุฑู ูุชุฑูุฏ ุถุจุทู ููุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู.

ูู ูุฐุง ุงููุณูุ ุณูููู ุจุถุจุท ูููุฐุฌ ูุงุฑูุงู ุงููุนูู ูุณุจููุง ููุชุฑุฌูุฉ ูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงููุฑูุณูุฉ (ุญูุซ ูุชุญุฏุซ ุงูุนุฏูุฏ ูู ููุธูู Hugging Face ููุชุง ุงููุบุชูู) ุนูู ูุฌููุนุฉ ุจูุงูุงุช [KDE4](https://huggingface.co/datasets/kde4)ุ ูุงูุชู ูู ูุฌููุนุฉ ุจูุงูุงุช ูููููุงุช ุงูููุถุนูุฉ ูุชุทุจููุงุช [KDE](https://apps.kde.org/). ุชู ุชุนููู ุงููููุฐุฌ ุงูุฐู ุณูุณุชุฎุฏูู ูุณุจููุง ุนูู ูุฌููุนุฉ ูุจูุฑุฉ ูู ุงููุตูุต ุงููุฑูุณูุฉ ูุงูุฅูุฌููุฒูุฉ ุงููุฃุฎูุฐุฉ ูู ูุฌููุนุฉ ุจูุงูุงุช [Opus](https://opus.nlpl.eu/)ุ ูุงูุชู ุชุญุชูู ุจุงููุนู ุนูู ูุฌููุนุฉ ุจูุงูุงุช KDE4. ูููู ุญุชู ุฅุฐุง ุฑุฃู ุงููููุฐุฌ ุงููุนูู ูุณุจููุง ูุฐู ุงูุจูุงูุงุช ุฃุซูุงุก ุชุนููู ุงููุณุจูุ ูุณูู ูุฑู ุฃูู ูููููุง ุงูุญุตูู ุนูู ูุณุฎุฉ ุฃูุถู ููู ุจุนุฏ ุงูุถุจุท ุงูุฏููู.

ุจูุฌุฑุฏ ุงูุงูุชูุงุกุ ุณูุญุตู ุนูู ูููุฐุฌ ูุงุฏุฑ ุนูู ุชูุฏูู ุชูุจุคุงุช ูุซู ูุฐุง:

ุจูุฌุฑุฏ ุงูุงูุชูุงุกุ ุณูุชููู ูู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ูุชูุฏูู ุชูุจุคุงุช ูุซู ุชูู ุงูููุฌูุฏุฉ ูู ุงูุตูุฑุฉ ุฃุฏูุงู:

![ุตูุฑุฉ ูุชูุจุคุงุช ุงููููุฐุฌ](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png)

ููุง ูู ุงูุญุงู ูู ุงูุฃูุณุงู ุงูุณุงุจูุฉุ ููููู ุงูุนุซูุฑ ุนูู ุงููููุฐุฌ ุงููุนูู ุงูุฐู ุณูููู ุจุชุฏุฑูุจู ูุชุญูููู ุฅูู Hub ุจุงุณุชุฎุฏุงู ุงูููุฏ ุฃุฏูุงู ูุงูุชุญูู ุงููุฒุฏูุฌ ูู ุชูุจุคุงุชู [ููุง](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.)

## ุฅุนุฏุงุฏ ุงูุจูุงูุงุช

ูุถุจุท ุฃู ุชุฏุฑูุจ ูููุฐุฌ ุชุฑุฌูุฉ ูู ุงูุตูุฑุ ุณูุญุชุงุฌ ุฅูู ูุฌููุนุฉ ุจูุงูุงุช ููุงุณุจุฉ ูููููุฉ. ููุง ุฐูุฑูุง ุณุงุจููุงุ ุณูุณุชุฎุฏู ูุฌููุนุฉ ุจูุงูุงุช [KDE4](https://huggingface.co/datasets/kde4) ูู ูุฐุง ุงููุณูุ ูููู ููููู ุชูููู ุงูููุฏ ูุงุณุชุฎุฏุงู ุจูุงูุงุชู ุงูุฎุงุตุฉ ุจุณูููุฉุ ุทุงููุง ุฃู ูุฏูู ุฃุฒูุงุฌ ูู ุงูุฌูู ุจูุบุชูู ุชุฑูุฏ ุงูุชุฑุฌูุฉ ููููุง ูุฅููููุง. ุฑุงุฌุน [ุงููุตู 5](/course/chapter5) ุฅุฐุง ููุช ุจุญุงุฌุฉ ุฅูู ุชุฐููุฑ ุจููููุฉ ุชุญููู ุจูุงูุงุชู ุงููุฎุตุตุฉ ูู `ูุฌููุนุฉ ุจูุงูุงุช`.

### ูุฌููุนุฉ ุจูุงูุงุช KDE4

ููุง ูู ูุนุชุงุฏุ ูููู ุจุชูุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ุจุงุณุชุฎุฏุงู ุฏุงูุฉ `load_dataset()` :

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

ุฅุฐุง ููุช ุชุฑูุฏ ุงูุนูู ูุน ุฒูุฌ ูุฎุชูู ูู ุงููุบุงุชุ ูููููู ุชุญุฏูุฏูุง ุจูุงุณุทุฉ ุฑููุฒูุง. ููุงู ูุง ูุฌููุนู 92 ูุบุฉ ูุชุงุญุฉ ููุฌููุนุฉ ุงูุจูุงูุงุช ูุฐูุ ููููู ุฑุคูุชูุง ุฌููุนูุง ุนู ุทุฑูู ุชูุณูุน ุนูุงูุงุช ุงููุบุฉ ุนูู [ุจุทุงูุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช](https://huggingface.co/datasets/kde4) ุงูุฎุงุตุฉ ุจูุง.

![ุงููุบุงุช ุงููุชุงุญุฉ ููุฌููุนุฉ ุจูุงูุงุช KDE4](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png)

ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช:

```py
raw_datasets
```

```python out
DatasetDict({
train: Dataset({
features: ['id', 'translation'],
num_rows: 210173
})
})
```

ูุฏููุง 210,173 ุฒูุฌูุง ูู ุงูุฌููุ ูููู ูู ูุณู ูุงุญุฏุ ูุฐูู ุณูุชุนูู ุนูููุง ุฅูุดุงุก ูุฌููุนุฉ ุงูุชุญูู ุงูุฎุงุตุฉ ุจูุง. ููุง ุฑุฃููุง ูู [ุงููุตู 5](/course/chapter5)ุ ุชุญุชูู `ูุฌููุนุฉ ุงูุจูุงูุงุช` ุนูู ุทุฑููุฉ `train_test_split()` ุงูุชู ูููู ุฃู ุชุณุงุนุฏูุง. ุณููุฏู ุจุฐุฑุฉ ููุชูุฑุงุฑ:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
train: Dataset({
features: ['id', 'translation'],
num_rows: 189155
})
test: Dataset({
features: ['id', 'translation'],
num_rows: 21018
})
})
```

ูููููุง ุฅุนุงุฏุฉ ุชุณููุฉ ููุชุงุญ `"test"` ุฅูู `"validation"` ุจูุฐู ุงูุทุฑููุฉ:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

ุงูุขู ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุฃุญุฏ ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
'fr': 'Par dรฉfaut, dรฉvelopper les fils de discussion'}
```

ูุญุตู ุนูู ูุงููุณ ุจุฌููุชูู ูู ุฒูุฌ ุงููุบุงุช ุงูุฐู ุทูุจูุงู. ุฅุญุฏู ูููุฒุงุช ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุช ุงููููุฆุฉ ุจูุตุทูุญุงุช ุนููู ุงูููุจููุชุฑ ุงููููุฉ ูู ุฃููุง ูุชุฑุฌูุฉ ุจุงููุงูู ุฅูู ุงููุฑูุณูุฉ. ููุน ุฐููุ ูุชุฑู ุงููููุฏุณูู ุงููุฑูุณููู ูุนุธู ุงููููุงุช ุงูุฎุงุตุฉ ุจุนููู ุงูููุจููุชุฑ ุจุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ุนูุฏูุง ูุชุญุฏุซูู. ููุงุ ุนูู ุณุจูู ุงููุซุงูุ ูุฏ ุชุธูุฑ ูููุฉ "threads" ูู ุฌููุฉ ูุฑูุณูุฉุ ุฎุงุตุฉ ูู ูุญุงุฏุซุฉ ูููุฉุ ูููู ูู ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุชุ ุชูุช ุชุฑุฌูุชูุง ุฅูู "fils de discussion" ุงูุฃูุซุฑ ุตุญุฉ. ูุฎุชุงุฑ ุงููููุฐุฌ ุงููุนูู ูุณุจููุงุ ูุงูุฐู ุชู ุชุนูููู ูุณุจููุง ุนูู ูุฌููุนุฉ ุฃูุจุฑ ูู ุงูุฌูู ุงููุฑูุณูุฉ ูุงูุฅูุฌููุฒูุฉุ ุงูุฎูุงุฑ ุงูุฃุณูู ุงููุชูุซู ูู ุชุฑู ุงููููุฉ ููุง ูู:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dรฉfaut pour les threads รฉlargis'}]
```

ูููู ุฑุคูุฉ ูุซุงู ุขุฎุฑ ุนูู ูุฐุง ุงูุณููู ูุน ูููุฉ "plugin"ุ ูุงูุชู ููุณุช ูููุฉ ูุฑูุณูุฉ ุฑุณููุฉ ูููู ูุนุธู ุงููุชุญุฏุซูู ุงูุฃุตูููู ูููููููุง ููู ูููููุง ุฃููุณูู ุนูุงุก ุชุฑุฌูุชูุง.

ูู ูุฌููุนุฉ ุจูุงูุงุช KDE4ุ ุชูุช ุชุฑุฌูุฉ ูุฐู ุงููููุฉ ุฅูู ุงููุฑูุณูุฉ ุฅูู "module d'extension" ุงูุฃูุซุฑ ุฑุณููุฉ:

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

ููุน ุฐููุ ููุชุฒู ูููุฐุฌูุง ุงููุนูู ูุณุจููุง ุจุงููููุฉ ุงูุฅูุฌููุฒูุฉ ุงููุฃูููุฉ ูุงููุฏูุฌุฉ:

```py
translator(
"Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

ุณูููู ูู ุงููุซูุฑ ููุงูุชูุงู ุฃู ูุฑู ุฅุฐุง ูุงู ูููุฐุฌูุง ุงููุถุจูุท ุงูุฏููู ููุชูุท ุชูู ุงูุฎุตุงุฆุต ุงูุฎุงุตุฉ ุจูุฌููุนุฉ ุงูุจูุงูุงุช (ุชูุจูู ุงูููุณุฏ: ุณูู ููุนู).

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

โ๏ธ **ุฌุฑุจ ุจููุณู!** ูููุฉ ุฅูุฌููุฒูุฉ ุฃุฎุฑู ุชุณุชุฎุฏู ุบุงูุจูุง ูู ุงููุฑูุณูุฉ ูู "email". ุงุจุญุซ ุนู ุฃูู ุนููุฉ ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงูุชู ุชุณุชุฎุฏู ูุฐู ุงููููุฉ. ููู ูุชู ุชุฑุฌูุชูุงุ ููู ูุชุฑุฌู ุงููููุฐุฌ ุงููุนูู ูุณุจููุง ููุณ ุงูุฌููุฉ ุงูุฅูุฌููุฒูุฉุ

</Tip>
### ูุนุงูุฌุฉ ุงูุจูุงูุงุช

ูุฌุจ ุนููู ุงูุขู ุฃู ุชููู ุนูู ุฏุฑุงูุฉ ุจุงูุฑูุชูู: ูุฌุจ ุชุญููู ุฌููุน ุงููุตูุต ุฅูู ูุฌููุนุงุช ูู ุฑููุฒ ุงูุชุนุฑูู ุญุชู ูุชููู ุงููููุฐุฌ ูู ููููุง. ุจุงููุณุจุฉ ููุฐู ุงููููุฉุ ุณูููู ุจุชููููุฒ ุงููุฏุฎูุงุช ูุงูุฃูุฏุงู. ุชุชูุซู ูููุชูุง ุงูุฃููู ูู ุฅูุดุงุก ูุงุฆู tokenizer. ููุง ุฐูุฑูุง ุณุงุจููุงุ ุณูุณุชุฎุฏู ูููุฐุฌูุง ูุณุจู ุงูุชุฏุฑูุจ ูู Marian English ุฅูู French. ุฅุฐุง ููุช ุชุญุงูู ุงุณุชุฎุฏุงู ูุฐู ุงูุดูุฑุฉ ูุน ุฒูุฌ ุขุฎุฑ ูู ุงููุบุงุชุ ูุชุฃูุฏ ูู ุชูููู ููุทุฉ ุชูุชูุด ุงููููุฐุฌ. ุชููุฑ ููุธูุฉ [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) ุฃูุซุฑ ูู ุฃูู ูููุฐุฌ ุจุนุฏุฉ ูุบุงุช.

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

ููููู ุฃูุถูุง ุงุณุชุจุฏุงู `model_checkpoint` ุจุฃู ูููุฐุฌ ุขุฎุฑ ุชูุถูู ูู [Hub](https://huggingface.co/models)ุ ุฃู ูุฌูุฏ ูุญูู ููุช ุจุญูุธ ูููุฐุฌ ูุณุจู ุงูุชุฏุฑูุจ ููุญูู ููู.

<Tip>
๐ก ุฅุฐุง ููุช ุชุณุชุฎุฏู ูุญูู ูุบุฉ ูุชุนุฏุฏ ุงููุบุงุช ูุซู mBART ุฃู mBART-50 ุฃู M2M100ุ ูุณุชุญุชุงุฌ ุฅูู ุชุนููู ุฑููุฒ ูุบุฉ ุงููุฏุฎูุงุช ูุงูุฃูุฏุงู ูู ุงููุญูู ูู ุฎูุงู ุชุนููู `tokenizer.src_lang` ู`tokenizer.tgt_lang` ุฅูู ุงูููู ุงูุตุญูุญุฉ.
</Tip>

ุฅู ุฅุนุฏุงุฏ ุจูุงูุงุชูุง ูุจุงุดุฑ ุฅูู ุญุฏ ูุง. ููุงู ุดูุก ูุงุญุฏ ููุท ูุฌุจ ุชุฐูุฑูุ ุชุญุชุงุฌ ุฅูู ุงูุชุฃูุฏ ูู ุฃู ุงููุญูู ูุฑูุฒ ุงูุฃูุฏุงู ูู ูุบุฉ ุงูุฅุฎุฑุงุฌ (ุงููุฑูุณูุฉ ููุง). ููููู ุงูููุงู ุจุฐูู ุนู ุทุฑูู ุชูุฑูุฑ ุงูุฃูุฏุงู ุฅูู ูุณูุท `text_targets` ูุฃุณููุจ `__call__` ูููุญูู.

ููุนุฑูุฉ ููููุฉ ุนูู ุฐููุ ุฏุนูุง ูุนุงูุฌ ุนููุฉ ูุงุญุฏุฉ ูู ูู ูุบุฉ ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

ููุง ูุฑูุ ูุญุชูู ุงูุฅุฎุฑุงุฌ ุนูู ุฑููุฒ ุชุนุฑูู ุงูุฅุฏุฎุงู ุงููุฑุชุจุทุฉ ุจุงูุฌููุฉ ุงูุฅูุฌููุฒูุฉุ ุจูููุง ูุชู ุชุฎุฒูู ุฑููุฒ ุงูุชุนุฑูู ุงููุฑุชุจุทุฉ ุจุงูุฌููุฉ ุงููุฑูุณูุฉ ูู ุญูู `labels`. ุฅุฐุง ูุณูุช ุงูุฅุดุงุฑุฉ ุฅูู ุฃูู ุชููู ุจุชููููุฒ ุงูุชุตูููุงุชุ ูุณูุชู ุชููููุฒูุง ุจูุงุณุทุฉ ูุญูู ุงูุฅุฏุฎุงูุ ูุงูุฐู ูู ุญุงูุฉ ูููุฐุฌ Marian ูู ููุฌุญ ุนูู ุงูุฅุทูุงู:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['โPar', 'โdรฉ', 'f', 'aut', ',', 'โdรฉ', 've', 'lop', 'per', 'โles', 'โfil', 's', 'โde', 'โdiscussion', '</s>']
['โPar', 'โdรฉfaut', ',', 'โdรฉvelopper', 'โles', 'โfils', 'โde', 'โdiscussion', '</s>']
```

ููุง ูุฑูุ ูุคุฏู ุงุณุชุฎุฏุงู ูุญูู ุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ููุนุงูุฌุฉ ุฌููุฉ ูุฑูุณูุฉ ุฅูู ุฒูุงุฏุฉ ุนุฏุฏ ุงูุฑููุฒุ ุญูุซ ูุง ูุนุฑู ุงููุญูู ุฃู ูููุงุช ูุฑูุณูุฉ (ุจุงุณุชุซูุงุก ุชูู ุงูุชู ุชุธูุฑ ุฃูุถูุง ูู ุงููุบุฉ ุงูุฅูุฌููุฒูุฉุ ูุซู "discussion").

ูุธุฑูุง ูุฃู `inputs` ุนุจุงุฑุฉ ุนู ูุงููุณ ุจููุงุชูุญูุง ุงููุนุชุงุฏุฉ (ุฑููุฒ ุชุนุฑูู ุงูุฅุฏุฎุงูุ ููุงุน ุงูุงูุชูุงูุ ุฅูุฎ)ุ ูุฅู ุงูุฎุทูุฉ ุงูุฃุฎูุฑุฉ ูู ุชุญุฏูุฏ ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุชู ุณูุทุจููุง ุนูู ูุฌููุนุงุช ุงูุจูุงูุงุช:

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

ูุงุญุธ ุฃููุง ุญุฏุฏูุง ููุณ ุงูุทูู ุงูุฃูุตู ููุฏุฎูุงุชูุง ููุฎุฑุฌุงุชูุง. ูุธุฑูุง ูุฃู ุงููุตูุต ุงูุชู ูุชุนุงูู ูุนูุง ุชุจุฏู ูุตูุฑุฉ ุฌุฏูุงุ ูุณุชุฎุฏู 128.

<Tip>
๐ก ุฅุฐุง ููุช ุชุณุชุฎุฏู ูููุฐุฌ T5 (ูุจุดูู ุฃูุซุฑ ุชุญุฏูุฏูุงุ ุฅุญุฏู ููุงุท ุชูุชูุด `t5-xxx`)ุ ูุณููุชููุน ูู ุงููููุฐุฌ ุฃู ูููู ูููุฏุฎูุงุช ุงููุตูุฉ ุจุงุฏุฆุฉ ุชุดูุฑ ุฅูู ุงููููุฉ ููุฏ ุงูุชูููุฐุ ูุซู `translate: English to French:`.
</Tip>

<Tip warning={true}>
โ๏ธ ูุง ููุชู ุจููุงุน ุงูุชูุงู ุงูุฃูุฏุงูุ ุญูุซ ูู ูุชููุนูุง ุงููููุฐุฌ. ุจุฏูุงู ูู ุฐููุ ูุฌุจ ุชุนููู ุงูุชุตูููุงุช ุงูููุงุจูุฉ ูุฑููุฒ ุงูุชุนุจุฆุฉ ุฅูู -100 ุญุชู ูุชู ุชุฌุงูููุง ูู ุญุณุงุจ ุงูุฎุณุงุฑุฉ. ุณูุชู ุฐูู ุจูุงุณุทุฉ ุฌุงูุน ุงูุจูุงูุงุช ุงูุฎุงุต ุจูุง ูุงุญููุง ูุธุฑูุง ูุฃููุง ูุทุจู ุงูุชุนุจุฆุฉ ุงูุฏููุงููููุฉุ ูููู ุฅุฐุง ููุช ุชุณุชุฎุฏู ุงูุชุนุจุฆุฉ ููุงุ ููุฌุจ ุนููู ุชูููู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ูุชุนููู ุฌููุน ุงูุชุตูููุงุช ุงูุชู ุชูุงุจู ุฑูุฒ ุงูุชุนุจุฆุฉ ุฅูู -100.
</Tip>

ูููููุง ุงูุขู ุชุทุจูู ูุฐู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ูู ุฎุทูุฉ ูุงุญุฏุฉ ุนูู ุฌููุน ุชูุณููุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

ุงูุขู ุจุนุฏ ูุนุงูุฌุฉ ุงูุจูุงูุงุชุ ูุญู ูุณุชุนุฏูู ูุถุจุท ูููุฐุฌูุง ุงููุณุจู ุงูุชุฏุฑูุจ!

{#if fw === 'pt'}
## ุถุจุท ุฏููู ูููููุฐุฌ ุจุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช Trainer

ุณุชููู ุงูุดูุฑุฉ ุงููุนููุฉ ุงูุชู ุชุณุชุฎุฏู `Trainer` ูู ููุณูุง ููุง ูุงูุช ูู ูุจูุ ูุน ุชุบููุฑ ูุงุญุฏ ููุท: ูุณุชุฎุฏู [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) ููุงุ ููู ูุฆุฉ ูุฑุนูุฉ ูู `Trainer` ุณุชุณูุญ ููุง ุจุงูุชุนุงูู ุจุดูู ุตุญูุญ ูุน ุงูุชููููุ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `generate()` ููุชูุจุค ุจุงููุฎุฑุฌุงุช ูู ุงููุฏุฎูุงุช. ุณูุบูุต ูู ุฐูู ุจูุฒูุฏ ูู ุงูุชูุงุตูู ุนูุฏ ุญุฏูุซูุง ุนู ุญุณุงุจ ุงููููุงุณ.

ุฃูููุงุ ูุญุชุงุฌ ุฅูู ูููุฐุฌ ูุนูู ูุถุจุทู ุจุฏูุฉ. ุณูุณุชุฎุฏู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช `AutoModel` ุงููุนุชุงุฏุฉ:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}
## ุถุจุท ุฏููู ูููููุฐุฌ ุจุงุณุชุฎุฏุงู Keras

ุฃูููุงุ ูุญุชุงุฌ ุฅูู ูููุฐุฌ ูุนูู ูุถุจุทู ุจุฏูุฉ. ุณูุณุชุฎุฏู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช `AutoModel` ุงููุนุชุงุฏุฉ:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>
๐ก ุชุญุชูู ููุทุฉ ุชูุชูุด `Helsinki-NLP/opus-mt-en-fr` ุนูู ุฃูุฒุงู PyTorch ููุทุ ูุฐุง ูุณุชุญุตู ุนูู ุฎุทุฃ ุฅุฐุง ุญุงููุช ุชุญููู ุงููููุฐุฌ ุฏูู ุงุณุชุฎุฏุงู ูุณูุท `from_pt=True` ูู ุทุฑููุฉ `from_pretrained()`. ุนูุฏูุง ุชุญุฏุฏ `from_pt=True`ุ ุณูููู ุงูููุชุจุฉ ุชููุงุฆููุง ุจุชูุฒูู ุงูุฃูุฒุงู ูุชุญููููุง ูู PyTorch ูู ุฃุฌูู. ููุง ุชุฑูุ ูู ุงูุณูู ุฌุฏูุง ุงูุชุจุฏูู ุจูู ุงูุฃุทุฑ ูู ๐ค Transformers!
</Tip>

{/if}

ูุงุญุธ ุฃููุง ูุฐู ุงููุฑุฉ ูุณุชุฎุฏู ูููุฐุฌูุง ุชู ุชุฏุฑูุจู ุนูู ูููุฉ ุงูุชุฑุฌูุฉ ููููู ุงุณุชุฎุฏุงูู ุจุงููุนูุ ูุฐุง ูุง ููุฌุฏ ุชุญุฐูุฑ ุจุดุฃู ุงูุฃูุฒุงู ุงูููููุฏุฉ ุฃู ุงูุชู ุชู ุชููุฆุชูุง ุญุฏูุซูุง.

### ุชุฌููุน ุงูุจูุงูุงุช

ุณูุญุชุงุฌ ุฅูู ุฌุงูุน ุจูุงูุงุช ููุชุนุงูู ูุน ุงูุชุนุจุฆุฉ ููุชุบุฐูุฉ ุงูุฏูุนูุฉ ุงูุฏููุงููููุฉ. ูุง ูููููุง ุงุณุชุฎุฏุงู `DataCollatorWithPadding` ููุง ูู ุงูุญุงู ูู [ุงููุตู 3](/course/chapter3) ูู ูุฐู ุงูุญุงูุฉุ ูุฃู ุฐูู ูููู ููุท ุจุชุนุจุฆุฉ ุงููุฏุฎูุงุช (ุฑููุฒ ุชุนุฑูู ุงูุฅุฏุฎุงูุ ูููุงุน ุงูุงูุชูุงูุ ูุฃููุงุน ุฑููุฒ ุงูุชุนุฑูู). ูุฌุจ ุฃูุถูุง ุชุนุจุฆุฉ ุงูุชุตูููุงุช ูุฏููุง ุฅูู ุงูุทูู ุงูุฃูุตู ุงูุฐู ุชูุช ููุงุฌูุชู ูู ุงูุชุตูููุงุช. ูููุง ุฐูุฑูุง ุณุงุจููุงุ ูุฌุจ ุฃู ุชููู ูููุฉ ุงูุชุนุจุฆุฉ ุงููุณุชุฎุฏูุฉ ูุชุนุจุฆุฉ ุงูุชุตูููุงุช `-100` ูููุณ ุฑูุฒ ุงูุชุนุจุฆุฉ ูููุญููุ ููุชุฃูุฏ ูู ุชุฌุงูู ูุฐู ุงูููู ุงููุจุงุฏุฉ ูู ุญุณุงุจ ุงูุฎุณุงุฑุฉ.

ูุชู ุฐูู ููู ุจูุงุณุทุฉ [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). ูุซู `DataCollatorWithPadding`ุ ูุฅูู ูุฃุฎุฐ `tokenizer` ุงููุณุชุฎุฏู ููุนุงูุฌุฉ ุงููุฏุฎูุงุชุ ููููู ูุฃุฎุฐ ุฃูุถูุง `model`. ููุฑุฌุน ุฐูู ุฅูู ุฃู ุฌุงูุน ุงูุจูุงูุงุช ูุฐุง ุณูููู ูุณุคููุงู ุฃูุถูุง ุนู ุฅุนุฏุงุฏ ุฑููุฒ ุชุนุฑูู ุงูุฅุฏุฎุงู ูู ุงูุชุดููุฑุ ูุงูุชู ูู ุฅุตุฏุงุฑุงุช ููุฒุงุญุฉ ูู ุงูุชุตูููุงุช ูุน ุฑูุฒ ุฎุงุต ูู ุงูุจุฏุงูุฉ. ูุธุฑูุง ูุฃู ูุฐุง ุงูุชุญูู ูุชู ุจุดูู ูุฎุชูู ููููุงู ููุฎุชูู ุงูุจููุ ูุฅู `DataCollatorForSeq2Seq` ุจุญุงุฌุฉ ุฅูู ูุนุฑูุฉ ูุงุฆู `model`:

{#if fw === 'pt'}
```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```
{:else}
```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```
{/if}

ูุงุฎุชุจุงุฑ ุฐูู ุนูู ุจุถุน ุนููุงุชุ ูุง ุนูููุง ุณูู ุงุณุชุฏุนุงุฆู ุนูู ูุงุฆูุฉ ูู ุงูุฃูุซูุฉ ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงููุนููุฉ ุงูุฎุงุตุฉ ุจูุง:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

ูููููุง ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ุชุตูููุงุชูุง ูุฏ ุชู ุชุนุจุฆุชูุง ุฅูู ุงูุทูู ุงูุฃูุตู ููุฏูุนุฉุ ุจุงุณุชุฎุฏุงู `-100`:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
-100,  -100,  -100,  -100,  -100,  -100],
[ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
550,  7032,  5821,  7907, 12649,     0]])
```

ููููููุง ุฃูุถูุง ุฅููุงุก ูุธุฑุฉ ุนูู ุฑููุฒ ุชุนุฑูู ูู ุงูุชุดููุฑุ ููุนุฑูุฉ ุฃููุง ุฅุตุฏุงุฑุงุช ููุฒุงุญุฉ ูู ุงูุชุตูููุงุช:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
59513, 59513, 59513, 59513, 59513, 59513],
[59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
817,   550,  7032,  5821,  7907, 12649]])
```

ูููุง ููู ุงูุชุตูููุงุช ููุนูุตุฑ ุงูุฃูู ูุงูุซุงูู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง:

```py
for i in range(1, 3):
print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}
ุณููุฑุฑ ูุฐุง `data_collator` ุฅูู `Seq2SeqTrainer`. ุจุนุฏ ุฐููุ ุฏุนูุง ูููู ูุธุฑุฉ ุนูู ุงููููุงุณ.
{:else}
ูููููุง ุงูุขู ุงุณุชุฎุฏุงู ูุฐุง `data_collator` ูุชุญููู ูู ูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ุฅูู `tf.data.Dataset`ุ ุฌุงูุฒ ููุชุฏุฑูุจ:

```python
tf_train_dataset = model.prepare_tf_dataset(
tokenized_datasets["train"],
collate_fn=data_collator,
shuffle=True,
batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
tokenized_datasets["validation"],
collate_fn=data_collator,
shuffle=False,
batch_size=16,
)
```

{/if}
### ุงูููุงููุณ

<Youtube id="M05L1DhFqcw"/>

ุชุชูุซู ุงูููุฒุฉ ุงูุชู ุชุถูููุง `Seq2SeqTrainer` ุฅูู ูุฆุฉ `Trainer` ุงูุฃุณุงุณูุฉ ูู ุงููุฏุฑุฉ ุนูู ุงุณุชุฎุฏุงู ุทุฑููุฉ `generate()` ุฃุซูุงุก ุงูุชูููู ุฃู ุงูุชูุจุค. ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุณูุณุชุฎุฏู ุงููููุฐุฌ `decoder_input_ids` ูุน ููุงุน ุงูุชูุงู ูุถูู ุนุฏู ุงุณุชุฎุฏุงู ุงูุฑููุฒ ุจุนุฏ ุงูุฑูุฒ ุงูุฐู ูุญุงูู ุงูุชูุจุค ุจูุ ูุชุณุฑูุน ุงูุชุฏุฑูุจ. ุฃุซูุงุก ุงูุงุณุชุฏูุงูุ ูู ูุชููู ูู ุงุณุชุฎุฏุงู ุชูู ุงูุนูุงูุงุช ูุฃููุง ูู ูููู ุชุณููุงุชุ ูุฐุง ููู ุงูุฌูุฏ ุชูููู ูููุฐุฌูุง ุจููุณ ุงูุฅุนุฏุงุฏ.

ููุง ุฑุฃููุง ูู [ุงููุตู 1](/course/chapter1/6)ุ ูููู ูู ุงูุชุดููุฑ ุจุงูุงุณุชุฏูุงู ุนู ุทุฑูู ุงูุชูุจุค ุจุงูุฑููุฒ ูุงุญุฏูุง ุชูู ุงูุขุฎุฑ - ููู ูุง ูุชู ุชูููุฐู ุฎูู ุงูููุงููุณ ูู ููุชุจุฉ ๐ค Transformers ุจูุงุณุทุฉ ุทุฑููุฉ `generate()`. ุณุชุณูุญ ููุง `Seq2SeqTrainer` ุจุงุณุชุฎุฏุงู ูุฐู ุงูุทุฑููุฉ ููุชูููู ุฅุฐุง ูููุง ุจุชุนููู `predict_with_generate=True`.

ุฅู ุงููููุงุณ ุงูุชูููุฏู ุงููุณุชุฎุฏู ููุชุฑุฌูุฉ ูู [ุฏุฑุฌุฉ BLEU](https://en.wikipedia.org/wiki/BLEU)ุ ุงูุชู ุชู ุชูุฏูููุง ูู [ููุงูุฉ ูู ุนุงู 2002](https://aclanthology.org/P02-1040.pdf) ุจูุงุณุทุฉ ููุดูุฑ ุจุงุจููููู ูุขุฎุฑูู. ุชููู ุฏุฑุฌุฉ BLEU ูุฏู ูุฑุจ ุงูุชุฑุฌูุงุช ูู ุชุณููุงุชูุง. ููุง ูููุณ ูุงุจููุฉ ููู ุฃู ุตุญุฉ ูุงุชุฌ ุงููููุฐุฌ ูู ุงููุงุญูุฉ ุงููุญููุฉุ ููููู ูุณุชุฎุฏู ููุงุนุฏ ุฅุญุตุงุฆูุฉ ููุชุฃูุฏ ูู ุธููุฑ ุฌููุน ุงููููุงุช ูู ุงูููุงุชุฌ ุงููููุฏุฉ ุฃูุถูุง ูู ุงูุฃูุฏุงู. ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ููุงู ููุงุนุฏ ุชุนุงูุจ ุชูุฑุงุฑ ุงููููุงุช ููุณูุง ุฅุฐุง ูู ูุชู ุชูุฑุงุฑูุง ุฃูุถูุง ูู ุงูุฃูุฏุงู (ูุชุฌูุจ ุฅุฎุฑุงุฌ ุงููููุฐุฌ ูุฌูู ูุซู "the the the the the") ูุฌูู ุงูุฅุฎุฑุงุฌ ุฃูุตุฑ ูู ุชูู ุงูููุฌูุฏุฉ ูู ุงูุฃูุฏุงู (ูุชุฌูุจ ููุงู ุงููููุฐุฌ ุจุฅุฎุฑุงุฌ ุฌูู ูุซู "the").

ุฃุญุฏ ุฃูุฌู ุงููุตูุฑ ูู BLEU ูู ุฃูู ูุชููุน ุฃู ูููู ุงููุต ููุตูููุง ูุณุจููุงุ ููุง ูุฌุนู ูู ุงูุตุนุจ ููุงุฑูุฉ ุงููุชุงุฆุฌ ุจูู ุงูููุงุฐุฌ ุงูุชู ุชุณุชุฎุฏู ุจุฑุงูุฌ ุชุญููู ูุฎุชููุฉ. ูุฐููุ ูุฅู ุงููููุงุณ ุงูุฃูุซุฑ ุงุณุชุฎุฏุงููุง ูููุงุฑูุฉ ููุงุฐุฌ ุงูุชุฑุฌูุฉ ุงูููู ูู [SacreBLEU](https://github.com/mjpost/sacrebleu)ุ ูุงูุฐู ูุนุงูุฌ ูุฐุง ุงููุตูุฑ (ูุบูุฑู) ูู ุฎูุงู ุชูุญูุฏ ุฎุทูุฉ ุงูุชุญููู. ูุงุณุชุฎุฏุงู ูุฐุง ุงููููุงุณุ ูุญุชุงุฌ ุฃููุงู ุฅูู ุชุซุจูุช ููุชุจุฉ SacreBLEU:

```py
!pip install sacrebleu
```

ุจุนุฏ ุฐููุ ูููููุง ุชุญูููู ุนุจุฑ `evaluate.load()` ููุง ูุนููุง ูู [ุงููุตู 3](/course/chapter3):

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

ูุฃุฎุฐ ูุฐุง ุงููููุงุณ ุงููุตูุต ููุฏุฎูุงุช ูุฃูุฏุงู. ุชู ุชุตูููู ููุจูู ุนุฏุฉ ุฃูุฏุงู ููุจููุฉุ ุญูุซ ุชูุฌุฏ ุบุงูุจูุง ุชุฑุฌูุงุช ููุจููุฉ ูุชุนุฏุฏุฉ ููุฌููุฉ ููุณูุง - ุชููุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ูุณุชุฎุฏููุง ูุงุญุฏุฉ ููุทุ ูููู ูู ุงูุดุงุฆุน ูู NLP ุงูุนุซูุฑ ุนูู ูุฌููุนุงุช ุจูุงูุงุช ุชูุฏู ุนุฏุฉ ุฌูู ูุนูุงูุงุช ุชุตููู. ูุฐููุ ูุฌุจ ุฃู ุชููู ุงูุชูุจุคุงุช ูุงุฆูุฉ ูู ุงูุฌููุ ูููู ูุฌุจ ุฃู ุชููู ุงููุฑุงุฌุน ูุงุฆูุฉ ูู ููุงุฆู ุงูุฌูู.

ุฏุนููุง ูุฌุฑุจ ูุซุงูุงู:

```py
predictions = [
"This plugin lets you translate web pages between several languages automatically."
]
references = [
[
"This plugin allows you to automatically translate web pages between several languages."
]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
'counts': [11, 6, 4, 3],
'totals': [12, 11, 10, 9],
'precisions': [91.67, 54.54, 40.0, 33.33],
'bp': 0.9200444146293233,
'sys_len': 12,
'ref_len': 13}
```

ูุญุตู ูุฐุง ุนูู ุฏุฑุฌุฉ BLEU ุชุจูุบ 46.75ุ ููู ุฃูุฑ ุฌูุฏ ุฌุฏูุง - ููุฑุฌูุนุ ุญูู ุงููููุฐุฌ ุงูุฃุตูู Transformer ูู ูุฑูุฉ ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) ุฏุฑุฌุฉ BLEU ุชุจูุบ 41.8 ูู ูููุฉ ุชุฑุฌูุฉ ููุงุซูุฉ ุจูู ุงููุบุฉ ุงูุฅูุฌููุฒูุฉ ูุงููุฑูุณูุฉ! (ููุญุตูู ุนูู ูุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุงูููุงููุณ ุงููุฑุฏูุฉุ ูุซู `counts` ู`bp`ุ ุฑุงุฌุน [ูุณุชูุฏุน SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) ูู ูุงุญูุฉ ุฃุฎุฑูุ ุฅุฐุง ุญุงูููุง ุจุงุณุชุฎุฏุงู ููุนูู ูู ุงูุชูุจุคุงุช ุงูุณูุฆุฉ (ุงููุซูุฑ ูู ุงูุชูุฑุงุฑุงุช ุฃู ุฃูุตุฑ ูู ุงููุงุฒู) ุงูุชู ุบุงูุจูุง ูุง ุชุฎุฑุฌ ูู ููุงุฐุฌ ุงูุชุฑุฌูุฉุ ูุณูุญุตู ุนูู ุฏุฑุฌุงุช BLEU ุณูุฆุฉ ููุบุงูุฉ:

```py
predictions = ["This This This This"]
references = [
[
"This plugin allows you to automatically translate web pages between several languages."
]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
'counts': [1, 0, 0, 0],
'totals': [4, 3, 2, 1],
'precisions': [25.0, 16.67, 12.5, 12.5],
'bp': 0.10539922456186433,
'sys_len': 4,
'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
[
"This plugin allows you to automatically translate web pages between several languages."
]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
'counts': [2, 1, 0, 0],
'totals': [2, 1, 0, 0],
'precisions': [100.0, 100.0, 0.0, 0.0],
'bp': 0.004086771438464067,
'sys_len': 2,
'ref_len': 13}
```

ูููู ุฃู ูุชุฑุงูุญ ุงูุฏุฑุฌุงุช ูู 0 ุฅูู 100ุ ูุงูุฃุนูู ุฃูุถู.

ููุญุตูู ุนูู ุงููุตูุต ุงูุชู ูููู ุฃู ูุณุชุฎุฏููุง ุงููููุงุณุ ุณูุณุชุฎุฏู ุทุฑููุฉ `tokenizer.batch_decode()`ุ ูู ูุง ุนูููุง ูุนูู ูู ุชูุธูู ุฌููุน ุนูุงูุงุช `-100` ูู ุงูุชุณููุงุชุ ูุณูููู ุงููุญูู ุงูุจุฑูุฌู ุชููุงุฆููุง ุจููุณ ุงูุฃูุฑ ุจุงููุณุจุฉ ูุฑููุฒ ุงูุชุนุจุฆุฉ. ุฏุนููุง ูุญุฏุฏ ูุธููุฉ ุชุฃุฎุฐ ูููุฐุฌูุง ููุฌููุนุฉ ุจูุงูุงุชูุง ูุชุญุณุจ ุงูููุงููุณ ุนูููุง. ุณูุณุชุฎุฏู ุฃูุถูุง ุฎุฏุนุฉ ุชุญุณู ุงูุฃุฏุงุก ุจุดูู ูุจูุฑ - ูู ุจุชุฌููุน ููุฏ ุงูุชูููุฏ ุงูุฎุงุต ุจูุง ุจุงุณุชุฎุฏุงู [XLA](https://www.tensorflow.org/xla)ุ ููู ูุฌูุน ุงูุฌุจุฑ ุงูุฎุทู ุงููุนุฌู ูู TensorFlow. ุชุทุจู XLA ุชุญุณููุงุช ูุฎุชููุฉ ุนูู ูุฎุทุท ุงูุญุณุงุจ ุงูุฎุงุต ุจุงููููุฐุฌุ ููุง ูุคุฏู ุฅูู ุชุญุณููุงุช ูุจูุฑุฉ ูู ุงูุณุฑุนุฉ ูุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ. ููุง ูู ููุถุญ ูู ูุฏููุฉ Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate)ุ ุชุนูู XLA ุจุดูู ุฃูุถู ุนูุฏูุง ูุง ุชุฎุชูู ุฃุดูุงู ุงูุฅุฏุฎุงู ูุฏููุง ูุซูุฑูุง. ููุชุนุงูู ูุน ูุฐุงุ ุณูููู ุจุชุจุทูู ุฅุฏุฎุงูุงุชูุง ุฅูู ูุถุงุนูุงุช 128ุ ูุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ ูุน ุฃุฏุงุฉ ุงูุชุจุทููุ ุซู ุณูุทุจู ุงูุฏูููุฑ `@tf.function(jit_compile=True)` ุนูู ูุธููุฉ ุงูุชูููุฏ ุงูุฎุงุตุฉ ุจูุงุ ูุงูุชู ุชุดูุฑ ุฅูู ุงูุฏุงูุฉ ุจุฃููููุง ููุชุฌููุน ุจุงุณุชุฎุฏุงู XLA.

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
tokenized_datasets["validation"],
collate_fn=generation_data_collator,
shuffle=False,
batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
return model.generate(
input_ids=batch["input_ids"],
attention_mask=batch["attention_mask"],
max_new_tokens=128,
)


def compute_metrics():
all_preds = []
all_labels = []

for batch, labels in tqdm(tf_generate_dataset):
predictions = generate_with_xla(batch)
decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
labels = labels.numpy()
labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
decoded_preds = [pred.strip() for pred in decoded_preds]
decoded_labels = [[label.strip()] for label in decoded_labels]
all_preds.extend(decoded_preds)
all_labels.extend(decoded_labels)

result = metric.compute(predictions=all_preds, references=all_labels)
return {"bleu": result["score"]}
```

{:else}

ููุญุตูู ุนูู ุงููุตูุต ุงูุชู ูููู ุฃู ูุณุชุฎุฏููุง ุงููููุงุณุ ุณูุณุชุฎุฏู ุทุฑููุฉ `tokenizer.batch_decode()`ุ ูู ูุง ุนูููุง ูุนูู ูู ุชูุธูู ุฌููุน ุนูุงูุงุช `-100` ูู ุงูุชุณููุงุชุ ูุณูููู ุงููุญูู ุงูุจุฑูุฌู ุชููุงุฆููุง ุจุงูุดูุก ููุณู ุจุงููุณุจุฉ ูุฑููุฒ ุงูุชุนุจุฆุฉ:

```py
import numpy as np


def compute_metrics(eval_preds):
preds, labels = eval_preds
# In case the model returns more than the prediction logits
if isinstance(preds, tuple):
preds = preds[0]

decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

# Replace -100s in the labels as we can't decode them
labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

# Some simple post-processing
decoded_preds = [pred.strip() for pred in decoded_preds]
decoded_labels = [[label.strip()] for label in decoded_labels]

result = metric.compute(predictions=decoded_preds, references=decoded_labels)
return {"bleu": result["score"]}
```

{/if}

ุงูุขู ุจุนุฏ ุฃู ุงูุชูููุง ูู ุฐููุ ูุญู ูุณุชุนุฏูู ูุถุจุท ูููุฐุฌูุง ุจุฏูุฉ!
## ุชููุฆุฉ ุงููููุฐุฌ

ุงูุฎุทูุฉ ุงูุฃููู ูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Faceุ ุญุชู ุชุชููู ูู ุชุญููู ูุชุงุฆุฌู ุฅูู Model Hub. ููุงู ุฏุงูุฉ ููุงุฆูุฉ ููุณุงุนุฏุชู ูู ุฐูู ูู ุฏูุชุฑ ุงูููุงุญุธุงุช:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ุณูุชู ุนุฑุถ ุฃุฏุงุฉ ููููู ูู ุฎูุงููุง ุฅุฏุฎุงู ุจูุงูุงุช ุงุนุชูุงุฏ ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face.

ุฅุฐุง ููุช ูุง ุชุนูู ูู ุฏูุชุฑ ุงูููุงุญุธุงุชุ ููุง ุนููู ุณูู ูุชุงุจุฉ ุงูุณุทุฑ ุงูุชุงูู ูู ุงููุญุทุฉ ุงูุทุฑููุฉ ุงูุฎุงุตุฉ ุจู:

```bash
huggingface-cli login
```

ูุจู ุฃู ูุจุฏุฃุ ุฏุนููุง ูุฑู ูุง ูู ุงููุชุงุฆุฌ ุงูุชู ูุญุตู ุนูููุง ูู ูููุฐุฌูุง ุจุฏูู ุฃู ุชุฏุฑูุจ:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุฐููุ ูููููุง ุฅุนุฏุงุฏ ูู ูุง ูุญุชุงุฌู ูุชุฌููุน ูุชุฏุฑูุจ ูููุฐุฌูุง. ูุงุญุธ ุงุณุชุฎุฏุงู `tf.keras.mixed_precision.set_global_policy("mixed_float16")` - ุณูุฎุจุฑ ูุฐุง Keras ุจุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู float16ุ ูุงูุฐู ูููู ุฃู ูููุฑ ุชุณุฑูุนูุง ูุจูุฑูุง ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPUs) ุงูุชู ุชุฏุนูู (Nvidia 20xx/V100 ุฃู ุฃุญุฏุซ).

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ ูู ุนุฏุฏ ุงูุนููุงุช ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ููุณูููุง ุนูู ุญุฌู ุงูุฏูุนุฉ ุซู ูุชู ุถุฑุจู
# ุจุนุฏุฏ ุงูุนุตูุฑ. ูุงุญุธ ุฃู tf_train_dataset ููุง ูู tf.data.Dataset ูุฌูุนุ
# ูููุณ ูุฌููุนุฉ ุจูุงูุงุช Hugging Face ุงูุฃุตููุฉุ ูุฐูู len() ูู ุจุงููุนู num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
init_lr=5e-5,
num_warmup_steps=0,
num_train_steps=num_train_steps,
weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ุชุฏุฑูุจ ูู ุฏูุฉ ุงูููุทุฉ ุงูุนุงุฆูุฉ ุงููุฎุชูุทุฉ 16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ุจุนุฏ ุฐููุ ูููู ุจุชุนุฑูู `PushToHubCallback` ูุชุญููู ูููุฐุฌูุง ุฅูู Hub ุฃุซูุงุก ุงูุชุฏุฑูุจุ ููุง ุฑุฃููุง ูู [ุงููุณู 2]((/course/chapter7/2))ุ ุซู ูููู ุจุจุณุงุทุฉ ุจุชุฌููุฒ ุงููููุฐุฌ ูุน ูุฐุง ุงูุงุณุชุฏุนุงุก:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
tf_train_dataset,
validation_data=tf_eval_dataset,
callbacks=[callback],
epochs=num_epochs,
)
```

ูุงุญุธ ุฃูู ููููู ุชุญุฏูุฏ ุงุณู ุงููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุงูุฏูุน ุฅููู ุจุงุณุชุฎุฏุงู ูุณูุท `hub_model_id` (ุนูู ูุฌู ุงูุฎุตูุตุ ุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ูุณูุท `hub_model_id` ููุฏูุน ุฅูู ููุธูุฉ). ุนูู ุณุจูู ุงููุซุงูุ ุนูุฏูุง ูููุง ุจุงูุฏูุน ุจุงููููุฐุฌ ุฅูู ููุธูุฉ [`huggingface-course`](https://huggingface.co/huggingface-course)ุ ุฃุถููุง `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` ุฅูู `Seq2SeqTrainingArguments`. ุจุดูู ุงูุชุฑุงุถูุ ุณูุชู ุงุณุชุฎุฏุงู ุงููุณุชูุฏุน ุงูููุฌูุฏ ูู ูุณุงุญุฉ ุงูุงุณู ุงูุฎุงุตุฉ ุจู ููุชู ุชุณููุชู ุจุงุณู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ููุช ุจุชุนููููุ ูุฐุง ููุง ุณูููู `"sgugger/marian-finetuned-kde4-en-to-fr"` (ููู ุงููููุฐุฌ ุงูุฐู ุงุฑุชุจุทูุง ุจู ูู ุจุฏุงูุฉ ูุฐุง ุงููุณู).

<Tip>
๐ก ุฅุฐุง ูุงู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ุชุณุชุฎุฏูู ููุฌูุฏูุง ุจุงููุนูุ ููุฌุจ ุฃู ูููู ูุณุชูุณุฎูุง ูุญูููุง ูููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุงูุฏูุน ุฅููู. ุฅุฐุง ูู ููู ูุฐููุ ูุณุชุญุตู ุนูู ุฎุทุฃ ุนูุฏ ุงุณุชุฏุนุงุก `model.fit()` ูุณูุชุนูู ุนููู ุชุนููู ุงุณู ุฌุฏูุฏ.
</Tip>

ุฃุฎูุฑูุงุ ุฏุนููุง ูุฑู ููู ุชุจุฏู ููุงููุณูุง ุงูุขู ุจุนุฏ ุงูุชูุงุก ุงูุชุฏุฑูุจ:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

ูู ูุฐู ุงููุฑุญูุฉุ ููููู ุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุงูุงุณุชุฏูุงู ุนูู Model Hub ูุงุฎุชุจุงุฑ ูููุฐุฌู ููุดุงุฑูุชู ูุน ุฃุตุฏูุงุฆู. ููุฏ ููุช ุจูุฌุงุญ ุจุถุจุท ูููุฐุฌ ููููุฉ ุงูุชุฑุฌูุฉ - ุชูุงูููุง!

{:else}

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุฐููุ ูููููุง ุชุนุฑูู `Seq2SeqTrainingArguments` ุงูุฎุงุตุฉ ุจูุง. ูุซู ุงููุฏุฑุจุ ูุณุชุฎุฏู ูุฆุฉ ูุฑุนูุฉ ูู `TrainingArguments` ุชุญุชูู ุนูู ุจุนุถ ุงูุญููู ุงูุฅุถุงููุฉ:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
f"marian-finetuned-kde4-en-to-fr",
evaluation_strategy="no"ุ
save_strategy="epoch"ุ
learning_rate=2e-5ุ
per_device_train_batch_size=32ุ
per_device_eval_batch_size=64ุ
weight_decay=0.01ุ
save_total_limit=3ุ
num_train_epochs=3ุ
predict_with_generate=Trueุ
fp16=Trueุ
push_to_hub=Trueุ
)
```

ุจุตุฑู ุงููุธุฑ ุนู ูุฑุท ุงููุนููุงุช ุงููุนุชุงุฏุฉ (ูุซู ูุนุฏู ุงูุชุนูู ูุนุฏุฏ ุงูุนุตูุฑ ูุญุฌู ุงูุฏูุนุฉ ูุจุนุถ ุชุฏููุฑ ุงููุฒู)ุ ูููุง ููู ุจุนุถ ุงูุชุบููุฑุงุช ููุงุฑูุฉ ุจูุง ุฑุฃููุงู ูู ุงูุฃูุณุงู ุงูุณุงุจูุฉ:

- ูุง ูููู ุจุชุนููู ุฃู ุชูููู ููุชุธูุ ุญูุซ ูุณุชุบุฑู ุงูุชูููู ุจุนุถ ุงูููุชุ ุณูููู ููุท ุจุชูููู ูููุฐุฌูุง ูุฑุฉ ูุงุญุฏุฉ ูุจู ุงูุชุฏุฑูุจ ูุจุนุฏู.
- ูุญุฏุฏ `fp16=True`ุ ูุงูุฐู ูุณุฑุน ุงูุชุฏุฑูุจ ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ุงูุญุฏูุซุฉ.
- ูุญุฏุฏ `predict_with_generate=True`ุ ููุง ูุงูุดูุง ุฃุนูุงู.
- ูุณุชุฎุฏู `push_to_hub=True` ูุชุญููู ุงููููุฐุฌ ุฅูู Hub ูู ููุงูุฉ ูู ุนุตุฑ.

ูุงุญุธ ุฃูู ููููู ุชุญุฏูุฏ ุงูุงุณู ุงููุงูู ูููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุงูุฏูุน ุฅููู ุจุงุณุชุฎุฏุงู ูุณูุท `hub_model_id` (ุนูู ูุฌู ุงูุฎุตูุตุ ุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ูุณูุท `hub_model_id` ููุฏูุน ุฅูู ููุธูุฉ). ุนูู ุณุจูู ุงููุซุงูุ ุนูุฏูุง ูููุง ุจุงูุฏูุน ุจุงููููุฐุฌ ุฅูู ููุธูุฉ [`huggingface-course`](https://huggingface.co/huggingface-course)ุ ุฃุถููุง `hub_model_Multiplier="huggingface-course/marian-finetuned-kde4-en-to-fr"` ุฅูู `Seq2SeqTrainingArguments`. ุจุดูู ุงูุชุฑุงุถูุ ุณูุชู ุงุณุชุฎุฏุงู ุงููุณุชูุฏุน ุงูููุฌูุฏ ูู ูุณุงุญุฉ ุงูุงุณู ุงูุฎุงุตุฉ ุจู ููุชู ุชุณููุชู ุจุงุณู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ููุช ุจุชุนููููุ ูุฐุง ูู ุญุงูุชูุง ุณูููู `"sgugger/marian-finetuned-kde4-en-to-fr"` (ููู ุงููููุฐุฌ ุงูุฐู ุงุฑุชุจุทูุง ุจู ูู ุจุฏุงูุฉ ูุฐุง ุงููุณู).

<Tip>
๐ก ุฅุฐุง ูุงู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ุชุณุชุฎุฏูู ููุฌูุฏูุง ุจุงููุนูุ ููุฌุจ ุฃู ูููู ูุณุชูุณุฎูุง ูุญูููุง ูููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุงูุฏูุน ุฅููู. ุฅุฐุง ูู ููู ูุฐููุ ูุณุชุญุตู ุนูู ุฎุทุฃ ุนูุฏ ุชุนุฑูู `Seq2SeqTrainer` ุงูุฎุงุต ุจู ูุณูุชุนูู ุนููู ุชุนููู ุงุณู ุฌุฏูุฏ.
</Tip>

ุฃุฎูุฑูุงุ ูููู ููุท ุจุชูุฑูุฑ ูู ุดูุก ุฅูู `Seq2SeqTrainer`:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
modelุ
argsุ
train_dataset=tokenized_datasets["train"]ุ
eval_dataset=tokenized_datasets["validation"]ุ
data_collator=data_collatorุ
tokenizer=tokenizerุ
compute_metrics=compute_metricsุ
)
```

ูุจู ุงูุชุฏุฑูุจุ ุณูููู ุฃููุงู ูุธุฑุฉ ุนูู ุงููุชูุฌุฉ ุงูุชู ูุญุตู ุนูููุง ูููุฐุฌูุงุ ููุชุฃูุฏ ูู ุฃููุง ูุง ูุฌุนู ุงูุฃููุฑ ุฃุณูุฃ ูุน ุงูุถุจุท ุงูุฏููู ูุฏููุง. ุณูุณุชุบุฑู ูุฐุง ุงูุฃูุฑ ุจุนุถ ุงูููุชุ ูุฐุง ููููู ุงุญุชุณุงุก ุงููููุฉ ุฃุซูุงุก ุชูููุฐู:

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981ุ
'eval_bleu': 39.26865061007616ุ
'eval_runtime': 965.8884ุ
'eval_samples_per_second': 21.76ุ
'eval_steps_per_second': 0.341}
```

ุฏุฑุฌุฉ BLEU ุงูุจุงูุบุฉ 39 ููุณุช ุณูุฆุฉ ููุบุงูุฉุ ููุง ูุนูุณ ุญูููุฉ ุฃู ูููุฐุฌูุง ุฌูุฏ ุจุงููุนู ูู ุชุฑุฌูุฉ ุงูุฌูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงููุฑูุณูุฉ.

ุงูุชุงูู ูู ุงูุชุฏุฑูุจุ ูุงูุฐู ุณูุณุชุบุฑู ุฃูุถูุง ุจุนุถ ุงูููุช:

```python
trainer.train()
```

ูุงุญุธ ุฃูู ุฃุซูุงุก ุญุฏูุซ ุงูุชุฏุฑูุจุ ูู ูู ูุฑุฉ ูุชู ูููุง ุญูุธ ุงููููุฐุฌ (ููุงุ ูู ุนุตุฑ) ูุชู ุชุญูููู ูู ุงูุฎูููุฉ ุฅูู Hub. ุจูุฐู ุงูุทุฑููุฉุ ุณุชุชููู ูู ุงุณุชุฆูุงู ุชุฏุฑูุจู ุนูู ุขูุฉ ุฃุฎุฑู ุฅุฐุง ูุฒู ุงูุฃูุฑ.

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุงูุชุฏุฑูุจุ ูููู ุจุชูููู ูููุฐุฌูุง ูุฑุฉ ุฃุฎุฑู - ูุฃูู ุฃู ูุฑู ุจุนุถ ุงูุชุญุณู ูู ุฏุฑุฌุฉ BLEU!

```ุจู
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312ุ
'eval_bleu': 52.94161337775576ุ
'eval_runtime': 714.2576ุ
'eval_samples_per_second': 29.426ุ
'eval_steps_per_second': 0.461ุ
'epoch': 3.0}
```

ูุฐุง ุชุญุณู ูุจูุฑ ูุจูุบ 14 ููุทุฉุ ููู ุฃูุฑ ุฑุงุฆุน.

ุฃุฎูุฑูุงุ ูุณุชุฎุฏู ุทุฑููุฉ `push_to_hub()` ููุชุฃูุฏ ูู ุชุญููู ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู ุงููููุฐุฌ. ููุง ูููู ุงููุฏุฑุจ ุจุตูุงุบุฉ ุจุทุงูุฉ ูููุฐุฌ ุจุฌููุน ูุชุงุฆุฌ ุงูุชูููู ูุชุญููููุง. ุชุญุชูู ุจุทุงูุฉ ุงููููุฐุฌ ูุฐู ุนูู ุจูุงูุงุช ูุตููุฉ ุชุณุงุนุฏ Model Hub ุนูู ุงุฎุชูุงุฑ ุงูุฃุฏุงุฉ ุงููุณุงุนุฏุฉ ูุนุฑุถ ุงูุงุณุชุฏูุงู. ุนุงุฏุฉุ ูุง ุชูุฌุฏ ุญุงุฌุฉ ูููู ุฃู ุดูุก ูุฃูู ููููู ุงุณุชูุชุงุฌ ุงูุฃุฏุงุฉ ุงูุตุญูุญุฉ ูู ูุฆุฉ ุงููููุฐุฌุ ูููู ูู ูุฐู ุงูุญุงูุฉุ ูููู ุงุณุชุฎุฏุงู ููุณ ูุฆุฉ ุงููููุฐุฌ ูุฌููุน ุฃููุงุน ุงููุดููุงุช ุชุณูุณู ุฅูู ุชุณูุณูุ ูุฐูู ูุญุฏุฏ ุฃููุง ูููุฐุฌ ุชุฑุฌูุฉ:

```ุจู
trainer.push_to_hub(tags="translation"ุ commit_message="Training complete")
```

ุชุนูุฏ ูุฐู ุงูุฃูุงูุฑ ุนููุงู URL ููุงูุชุฒุงู ุงูุฐู ูุงู ุจู ููุชูุ ุฅุฐุง ููุช ุชุฑูุฏ ูุญุตู:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

ูู ูุฐู ุงููุฑุญูุฉุ ููููู ุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุงูุงุณุชุฏูุงู ุนูู Model Hub ูุงุฎุชุจุงุฑ ูููุฐุฌู ููุดุงุฑูุชู ูุน ุฃุตุฏูุงุฆู. ููุฏ ููุช ุจูุฌุงุญ ุจุถุจุท ูููุฐุฌ ููููุฉ ุงูุชุฑุฌูุฉ - ุชูุงูููุง!

ุฅุฐุง ููุช ุชุฑุบุจ ูู ุงูุบูุต ุจุดูู ุฃุนูู ููููุงู ูู ุญููุฉ ุงูุชุฏุฑูุจุ ูุณูููู ุงูุขู ุจุฅุธูุงุฑ ููููุฉ ุงูููุงู ุจููุณ ุงูุดูุก ุจุงุณุชุฎุฏุงู ๐ค Accelerate.

{/if}

## ุญููุฉ ุชุฏุฑูุจ ูุฎุตุตุฉ

ุฏุนููุง ุงูุขู ูููู ูุธุฑุฉ ุนูู ุญููุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉุ ุญุชู ุชุชููู ูู ุชุฎุตูุต ุงูุฃุฌุฒุงุก ุงูุชู ุชุญุชุงุฌูุง. ุณูุดุจู ุฐูู ุฅูู ุญุฏ ูุจูุฑ ูุง ูุนููุงู ูู [ุงููุณู 2](/course/chapter7/2) ู [ุงููุตู 3](/course/chapter3/4).

### ุฅุนุฏุงุฏ ูู ุดูุก ููุชุฏุฑูุจ

ููุฏ ุฑุฃูุช ูู ูุฐุง ุนุฏุฉ ูุฑุงุช ุงูุขูุ ูุฐุง ุณููุฑ ุนุจุฑ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุจุณุฑุนุฉ ูุจูุฑุฉ. ุฃููุงูุ ุณูููู ุจุจูุงุก `DataLoader`s ูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุงุ ุจุนุฏ ุชุนููู ุชูุณูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุฅูู `"torch"` ุญุชู ูุญุตู ุนูู ุชูุณูุฑุงุช PyTorch:

```ุจู
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
tokenized_datasets["train"]ุ
shuffle=Trueุ
collate_fn=data_collatorุ
batch_size=8ุ
)
eval_dataloader = DataLoader(
tokenized_datasets["validation"]ุ collate_fn=data_collatorุ batch_size=8
)
```

ุจุนุฏ ุฐููุ ุณูุนูุฏ ุฅูุดุงุก ูุซูู ูููุฐุฌูุงุ ููุชุฃูุฏ ูู ุฃููุง ูุง ููุงุตู ุงูุถุจุท ุงูุฏููู ูู ูุจูุ ูููููุง ูุจุฏุฃ ูู ุงููููุฐุฌ ุงููุณุจู ุงูุชุฏุฑูุจ ูุฑุฉ ุฃุฎุฑู:

```ุจู
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ุจุนุฏ ุฐููุ ุณุชุญุชุงุฌ ุฅูู ูุญุณู:

```ุจู
from transformers import AdamW

optimizer = AdamW(model.parameters()ุ lr=2e-5)
```

ุจูุฌุฑุฏ ุฃู ูุญุตู ุนูู ูู ูุฐู ุงูุฃุดูุงุกุ ูููููุง ุฅุฑุณุงููุง ุฅูู ุทุฑููุฉ `accelerator.prepare()`. ุชุฐูุฑ ุฃูู ุฅุฐุง ููุช ุชุฑูุฏ ุงูุชุฏุฑูุจ ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ูู ุฏูุชุฑ ููุงุญุธุงุช Colabุ ูุณูุชุนูู ุนููู ููู ูู ูุฐุง ุงูุฑูุฒ ุฅูู ุฏุงูุฉ ุชุฏุฑูุจุ ููุง ูุฌุจ ุชูููุฐ ุฃู ุฎููุฉ ุชููู ุจุชูููุฐ `Accelerator`.

```ุจู
from accelerate import Accelerator

accelerator = Accelerator()
modelุ optimizerุ train_dataloaderุ eval_dataloader = accelerator.prepare(
modelุ optimizerุ train_dataloaderุ eval_dataloader
)
```

ุงูุขู ุจุนุฏ ุฃู ุฃุฑุณููุง `train_dataloader` ุฅูู `accelerator.prepare()`ุ ูููููุง ุงุณุชุฎุฏุงู ุทููู ูุญุณุงุจ ุนุฏุฏ ุฎุทูุงุช ุงูุชุฏุฑูุจ. ุชุฐูุฑ ุฃูู ูุฌุจ ุนูููุง ุฏุงุฆููุง ุงูููุงู ุจุฐูู ุจุนุฏ ุฅุนุฏุงุฏ ุจุฑูุงูุฌ ุงูุชุบุฐูุฉ ุงูุชููุงุฆูุ ุญูุซ ุณุชุบูุฑ ุทุฑููุฉ ุงูุฅุนุฏุงุฏ ุทูู `DataLoader`. ูุณุชุฎุฏู ุฌุฏูููุง ุฎุทููุง ููุงุณููููุง ูู ูุนุฏู ุงูุชุนูู ุฅูู 0:

```ุจู
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
"linear"ุ
optimizer=optimizerุ
num_warmup_steps=0ุ
num_training_steps=num_training_stepsุ
)
```

ุฃุฎูุฑูุงุ ูุฏูุน ูููุฐุฌูุง ุฅูู Hubุ ุณูุชุนูู ุนูููุง ุฅูุดุงุก ูุงุฆู `Repository` ูู ูุฌูุฏ ุนูู. ูู ุจุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face Hubุ ุฅุฐุง ูู ุชูู ูุฏ ููุช ุจุชุณุฌูู ุงูุฏุฎูู ุจุงููุนู. ุณูุญุฏุฏ ุงุณู ุงููุณุชูุฏุน ูู ูุนุฑู ุงููููุฐุฌ ุงูุฐู ูุฑูุฏ ููุญู ููููุฐุฌูุง (ูุง ุชุชุฑุฏุฏ ูู ุงุณุชุจุฏุงู `repo_name` ุจุฎูุงุฑู ุงูุฎุงุตุ ูุฌุจ ุฃู ูุญุชูู ููุท ุนูู ุงุณู ุงููุณุชุฎุฏู ุงูุฎุงุต ุจูุ ููู ูุง ุชูุนูู ูุธููุฉ `get_full_repo_name()`):

```ุจู
from huggingface_hub import Repositoryุ get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

ุจุนุฏ ุฐููุ ูููููุง ุงุณุชูุณุงุฎ ูุฐุง ุงููุณุชูุฏุน ูู ูุฌูุฏ ูุญูู. ุฅุฐุง ูุงู ููุฌูุฏูุง ุจุงููุนูุ ููุฌุจ ุฃู ูููู ูุฐุง ุงููุฌูุฏ ุงููุญูู ูุณุชูุณุฎูุง ูู ุงููุณุชูุฏุน ุงูุฐู ูุนูู ุนููู:

```ุจู
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dirุ clone_from=repo_name)
```

ุงูุขู ูููููุง ุชุญููู ุฃู ุดูุก ูููู ุจุญูุธู ูู `output_dir` ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `repo.push_to_hub()`. ุณูุณุงุนุฏูุง ูุฐุง ูู ุชุญููู ุงูููุงุฐุฌ ุงููุชูุณุทุฉ ูู ููุงูุฉ ูู ุนุตุฑ.
## ุญููุฉ ุงูุชุฏุฑูุจ

ุงูุขู ูุญู ูุณุชุนุฏูู ููุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ุงููุงููุฉ. ูุชุจุณูุท ุงูุฌุฒุก ุงูุฎุงุต ุจุงูุชููููุ ูููู ุจุชุนุฑูู ุฏุงูุฉ `postprocess()` ุงูุชู ุชุฃุฎุฐ ุงูุชููุนุงุช ูุงูุนูุงูุงุช ูุชุญูููุง ุฅูู ููุงุฆู ูู ุงูุณูุงุณู ุงููุตูุฉ ุงูุชู ูุชููุนูุง ูุงุฆู `metric`:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # ุงุณุชุจุฏู -100 ูู ุงูุนูุงูุงุช ูุฃููุง ูุง ูุณุชุทูุน ูู ุชุดููุฑูุง.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ุจุนุถ ุงููุนุงูุฌุฉ ุงูุจุณูุทุฉ ุจุนุฏ ูู ุงูุชุฑููุฒ
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

ุชุจุฏู ุญููุฉ ุงูุชุฏุฑูุจ ูุดุงุจูุฉ ูุชูู ุงูููุฌูุฏุฉ ูู [ุงููุณู 2](/course/chapter7/2) ู[ุงููุตู 3](/course/chapter3)ุ ูุน ุจุนุถ ุงูุงุฎุชูุงูุงุช ูู ุฌุฒุก ุงูุชูููู - ูุฐูู ุฏุนููุง ูุฑูุฒ ุนูู ุฐูู!

ุฃูู ุดูุก ูุฌุจ ููุงุญุธุชู ูู ุฃููุง ูุณุชุฎุฏู ุทุฑููุฉ `generate()` ูุญุณุงุจ ุงูุชููุนุงุชุ ูููู ูุฐู ุทุฑููุฉ ูู ูููุฐุฌูุง ุงูุฃุณุงุณูุ ูููุณ ุงููููุฐุฌ ุงูููููู ๐ค Accelerate ุงูุฐู ุชู ุฅูุดุงุคู ูู ุทุฑููุฉ `prepare()`. ูููุฐุง ููู ูู ุงููููุฐุฌ ุฃููุงูุ ุซู ูุณุชุฏุนู ูุฐู ุงูุทุฑููุฉ.

ูุงูุดูุก ุงูุซุงูู ูู ุฃููุ ูุซููุง ูู ุงูุญุงู ูุน [ุชุตููู ุงูุฑููุฒ](/course/chapter7/2)ุ ูุฏ ุชููู ุนูููุชุงู ูุฏ ูุณูุทุชุง ุงููุฏุฎูุงุช ูุงูุนูุงูุงุช ุฅูู ุฃุดูุงู ูุฎุชููุฉุ ูุฐูู ูุณุชุฎุฏู `accelerator.pad_across_processes()` ูุฌุนู ุงูุชููุนุงุช ูุงูุนูุงูุงุช ุจููุณ ุงูุดูู ูุจู ุงุณุชุฏุนุงุก ุทุฑููุฉ `gather()`. ุฅุฐุง ูู ููุนู ุฐููุ ูุฅู ุงูุชูููู ุฅูุง ุฃู ูุฎุทุฆ ุฃู ูุนูู ุฅูู ุงูุฃุจุฏ.

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # ุงูุชุฏุฑูุจ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # ุงูุชูููู
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
            labels = batch["labels"]

            # ุถุฑูุฑู ูููุก ุงูุชููุนุงุช ูุงูุนูุงูุงุช ูุจู ุฌูุนูุง
            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

            predictions_gathered = accelerator.gather(generated_tokens)
            labels_gathered = accelerator.gather(labels)

            decoded_preds, decoded_labels = postprocess(
                predictions_gathered, labels_gathered
            )
            metric.add_batch(predictions=decoded_preds, references=decoded_labels)

        results = metric.compute()
        print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # ุงูุญูุธ ูุงูุฑูุน
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุฐููุ ูุฌุจ ุฃู ูููู ูุฏูู ูููุฐุฌ ุฐู ูุชุงุฆุฌ ูุดุงุจูุฉ ุฌุฏูุง ูููููุฐุฌ ุงูุฐู ุชู ุชุฏุฑูุจู ุจุงุณุชุฎุฏุงู `Seq2SeqTrainer`. ููููู ุงูุชุญูู ูู ุงููููุฐุฌ ุงูุฐู ุชุฏุฑุจูุง ุนููู ุจุงุณุชุฎุฏุงู ูุฐุง ุงูุฑูุฒ ูู [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). ูุฅุฐุง ููุช ุชุฑุบุจ ูู ุชุฌุฑุจุฉ ุฃู ุชุนุฏููุงุช ุนูู ุญููุฉ ุงูุชุฏุฑูุจุ ูููููู ุชูููุฐูุง ูุจุงุดุฑุฉ ูู ุฎูุงู ุชุนุฏูู ุงูููุฏ ุงูููุถุญ ุฃุนูุงู!

## ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงูุฏููู ุงูุถุจุท

ููุฏ ุฃุธูุฑูุง ูู ุจุงููุนู ููู ููููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงูุฐู ูููุง ุจุถุจุทู ุจุฏูุฉ ุนูู Model Hub ุจุงุณุชุฎุฏุงู ุฃุฏุงุฉ ุงูุชูุจุค ุงูุชูุงุนููุฉ. ูุงุณุชุฎุฏุงูู ูุญูููุง ูู `pipeline`ุ ุนูููุง ููุท ุชุญุฏูุฏ ูุนุฑู ุงููููุฐุฌ ุงูุตุญูุญ:

```py
from transformers import pipeline

# ุงุณุชุจุฏู ูุฐุง ุจูุนุฑู ููุทุฉ ุงูุชูุชูุด ุงูุฎุงุตุฉ ุจู
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dรฉfaut, dรฉvelopper les fils de discussion'}]
```

ููุง ูู ูุชููุนุ ูุงู ูููุฐุฌูุง ุงูููุฏุฑุจ ูุณุจููุง ุจุชูููู ูุนุฑูุชู ูุน ุงููุฌููุนุฉ ุงูุชู ูููุง ุจุถุจุทูุง ุจุฏูุฉุ ูุจุฏูุงู ูู ุชุฑู ุงููููุฉ ุงูุฅูุฌููุฒูุฉ "threads" ุจููุฑุฏูุงุ ูุฅูู ูุชุฑุฌููุง ุงูุขู ุฅูู ุงููุณุฎุฉ ุงููุฑูุณูุฉ ุงูุฑุณููุฉ. ูููุทุจู ุงูุดูุก ููุณู ุนูู ูููุฉ "plugin":

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

ูุซุงู ุขุฎุฑ ุฑุงุฆุน ุนูู ุชููู ุงููุฌุงู!

<Tip>

โ๏ธ **ุฌุฑุจ ุจููุณู!** ูุงุฐุง ูุนูุฏ ุงููููุฐุฌ ุนูู ุงูุนููุฉ ุงูุชู ุชุญุชูู ุนูู ูููุฉ "email" ุงูุชู ุญุฏุฏุชูุง ุณุงุจููุงุ

</Tip>